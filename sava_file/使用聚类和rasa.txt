WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 0): env://
| distributed init (rank 1): env://
Namespace(batch_size=13, checkpoint='./data/ALBEF/ALBEF.pth', config='configs/PS_cuhk_pedes.yaml', device='cuda', dist_backend='nccl', dist_url='env://', distributed=True, embed_dim=577, eval_mAP=True, evaluate=False, gpu=0, output_dir='output/cuhk-pedes/train', rank=0, resume=False, seed=42, text_encoder='bert-base-uncased', world_size=2)
{'train_file': ['./re_id/CUHK-PEDES/processed_data/train.json'], 'val_file': './re_id/CUHK-PEDES/processed_data/val.json', 'test_file': './re_id/CUHK-PEDES/processed_data/test.json', 'train_image_root': './re_id/CUHK-PEDES/imgs', 'val_image_root': './re_id/CUHK-PEDES/imgs', 'test_image_root': './re_id/CUHK-PEDES/imgs', 'bert_config': 'configs/config_bert.json', 'max_words': 50, 'image_res': 384, 'batch_size_train': 13, 'batch_size_test': 64, 'mlm_probability': 0.15, 'weak_pos_pair_probability': 0.1, 'mrtd_mask_probability': 0.3, 'queue_size': 65536, 'momentum': 0.995, 'vision_width': 768, 'embed_dim': 256, 'temp': 0.07, 'k_test': 128, 'alpha': 0.4, 'warm_up': True, 'optimizer': {'opt': 'adamW', 'lr': 1e-05, 'weight_decay': 0.02, 'lr_custm': 0.0001}, 'schedular': {'sched': 'cosine', 'lr': 1e-05, 'epochs': 30, 'min_lr': 1e-06, 'decay_rate': 1, 'warmup_lr': 1e-05, 'warmup_epochs': 1, 'cooldown_epochs': 0}, 'eval_epoch': 24, 'weights': [0.5, 1, 1, 0.5, 0.5]}
args.distributed True
Creating retrieval dataset
args.text_encoder bert-base-uncased
Creating model
/home/cxd/storage/envs/irra/lib/python3.8/site-packages/transformers/modeling_utils.py:1211: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(resolved_archive_file, map_location="cpu")
/home/cxd/storage/envs/irra/lib/python3.8/site-packages/transformers/modeling_utils.py:1211: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(resolved_archive_file, map_location="cpu")
Retrieval.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.checkpoint, map_location='cpu')
Retrieval.py:470: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(args.checkpoint, map_location='cpu')
reshape position embedding from 256 to 576
reshape position embedding from 256 to 576
load checkpoint from ./data/ALBEF/ALBEF.pth
_IncompatibleKeys(missing_keys=['idx_queue', 'prd_head.weight', 'prd_head.bias', 'mrtd_head.weight', 'mrtd_head.bias'], unexpected_keys=[])
Start training
Train Epoch的聚类: [0]  [   0/5241]  eta: 2:04:53    time: 1.4299  data: 0.7350  max mem: 3140
Train Epoch的聚类: [0]  [5240/5241]  eta: 0:00:00    time: 0.2071  data: 0.0002  max mem: 3162
Train Epoch的聚类: [0] Total time: 0:14:58 (0.1714 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 5555
-1 (未归入任何簇) 的数量: 21209

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [0]  [   0/1804]  eta: 20:15:09  lr: 0.000010  loss_cl: 11.4133  loss_pitm: 0.5693  loss_mlm: 1.9211  loss_prd: 0.6286  loss_mrtd: 0.5931  time: 40.4155  data: 0.4829  max mem: 14828
Train Epoch: [0]  [  50/1804]  eta: 0:59:34  lr: 0.000010  loss_cl: 4.4046  loss_pitm: 0.1832  loss_mlm: 2.2033  loss_prd: 0.1119  loss_mrtd: 0.5821  time: 1.2844  data: 0.0001  max mem: 17546
Train Epoch: [0]  [ 100/1804]  eta: 0:47:24  lr: 0.000010  loss_cl: 6.2310  loss_pitm: 0.1722  loss_mlm: 0.9263  loss_prd: 0.6334  loss_mrtd: 0.3940  time: 1.2912  data: 0.0002  max mem: 17546
Train Epoch: [0]  [ 150/1804]  eta: 0:42:38  lr: 0.000010  loss_cl: 7.6176  loss_pitm: 0.2550  loss_mlm: 1.6288  loss_prd: 0.1008  loss_mrtd: 0.4093  time: 1.3010  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 200/1804]  eta: 0:39:42  lr: 0.000010  loss_cl: 6.0864  loss_pitm: 0.1750  loss_mlm: 0.8480  loss_prd: 0.2293  loss_mrtd: 0.3187  time: 1.3043  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 250/1804]  eta: 0:37:31  lr: 0.000010  loss_cl: 6.4378  loss_pitm: 0.2983  loss_mlm: 1.4619  loss_prd: 0.2754  loss_mrtd: 0.3555  time: 1.2924  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 300/1804]  eta: 0:35:45  lr: 0.000010  loss_cl: 8.4819  loss_pitm: 0.2739  loss_mlm: 0.7496  loss_prd: 0.0907  loss_mrtd: 0.4423  time: 1.3219  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 350/1804]  eta: 0:34:09  lr: 0.000010  loss_cl: 6.7854  loss_pitm: 0.3232  loss_mlm: 1.7358  loss_prd: 0.4158  loss_mrtd: 0.4962  time: 1.3102  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 400/1804]  eta: 0:32:39  lr: 0.000010  loss_cl: 7.1199  loss_pitm: 0.1919  loss_mlm: 1.1134  loss_prd: 0.0906  loss_mrtd: 0.3915  time: 1.2955  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 450/1804]  eta: 0:31:16  lr: 0.000010  loss_cl: 7.5241  loss_pitm: 0.2188  loss_mlm: 0.9020  loss_prd: 0.6353  loss_mrtd: 0.3008  time: 1.3120  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 500/1804]  eta: 0:29:56  lr: 0.000010  loss_cl: 7.5889  loss_pitm: 0.3410  loss_mlm: 0.8824  loss_prd: 0.2503  loss_mrtd: 0.3859  time: 1.3007  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 550/1804]  eta: 0:28:39  lr: 0.000010  loss_cl: 8.1142  loss_pitm: 0.0943  loss_mlm: 1.2544  loss_prd: 0.0695  loss_mrtd: 0.3580  time: 1.2928  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 600/1804]  eta: 0:27:23  lr: 0.000010  loss_cl: 7.7056  loss_pitm: 0.1015  loss_mlm: 1.2429  loss_prd: 0.4633  loss_mrtd: 0.3329  time: 1.3003  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 650/1804]  eta: 0:26:09  lr: 0.000010  loss_cl: 8.0642  loss_pitm: 0.3468  loss_mlm: 0.6922  loss_prd: 0.4397  loss_mrtd: 0.3778  time: 1.2951  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 700/1804]  eta: 0:24:56  lr: 0.000010  loss_cl: 7.1977  loss_pitm: 0.1840  loss_mlm: 0.9110  loss_prd: 0.0992  loss_mrtd: 0.3321  time: 1.3053  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 750/1804]  eta: 0:23:44  lr: 0.000010  loss_cl: 7.1167  loss_pitm: 0.2114  loss_mlm: 1.2540  loss_prd: 0.4612  loss_mrtd: 0.3727  time: 1.3013  data: 0.0001  max mem: 17660
Train Epoch: [0]  [ 800/1804]  eta: 0:22:33  lr: 0.000010  loss_cl: 7.7253  loss_pitm: 0.1511  loss_mlm: 0.6735  loss_prd: 0.2856  loss_mrtd: 0.3263  time: 1.2984  data: 0.0001  max mem: 17660
Train Epoch: [0]  [ 850/1804]  eta: 0:21:25  lr: 0.000010  loss_cl: 8.0606  loss_pitm: 0.1226  loss_mlm: 0.8200  loss_prd: 0.4568  loss_mrtd: 0.3161  time: 1.3244  data: 0.0002  max mem: 17660
Train Epoch: [0]  [ 900/1804]  eta: 0:20:15  lr: 0.000010  loss_cl: 7.2178  loss_pitm: 0.1298  loss_mlm: 0.9848  loss_prd: 0.3778  loss_mrtd: 0.3164  time: 1.3037  data: 0.0001  max mem: 17660
Train Epoch: [0]  [ 950/1804]  eta: 0:19:06  lr: 0.000010  loss_cl: 8.4787  loss_pitm: 0.2705  loss_mlm: 0.6243  loss_prd: 0.4529  loss_mrtd: 0.2906  time: 1.3059  data: 0.0002  max mem: 17660
Train Epoch: [0]  [1000/1804]  eta: 0:17:58  lr: 0.000010  loss_cl: 7.5291  loss_pitm: 0.2167  loss_mlm: 0.9652  loss_prd: 0.0888  loss_mrtd: 0.3240  time: 1.3049  data: 0.0002  max mem: 17660
Train Epoch: [0]  [1050/1804]  eta: 0:16:49  lr: 0.000010  loss_cl: 8.2959  loss_pitm: 0.2761  loss_mlm: 1.3135  loss_prd: 0.2756  loss_mrtd: 0.3741  time: 1.3039  data: 0.0001  max mem: 17660
Train Epoch: [0]  [1100/1804]  eta: 0:15:41  lr: 0.000010  loss_cl: 6.7853  loss_pitm: 0.2554  loss_mlm: 1.4586  loss_prd: 0.3824  loss_mrtd: 0.4098  time: 1.3116  data: 0.0001  max mem: 17660
Train Epoch: [0]  [1150/1804]  eta: 0:14:34  lr: 0.000010  loss_cl: 7.6061  loss_pitm: 0.1019  loss_mlm: 0.5983  loss_prd: 0.4570  loss_mrtd: 0.3710  time: 1.3094  data: 0.0002  max mem: 17660
Train Epoch: [0]  [1200/1804]  eta: 0:13:26  lr: 0.000010  loss_cl: 7.8928  loss_pitm: 0.2571  loss_mlm: 0.4939  loss_prd: 0.2244  loss_mrtd: 0.2375  time: 1.3054  data: 0.0002  max mem: 17660
Train Epoch: [0]  [1250/1804]  eta: 0:12:19  lr: 0.000010  loss_cl: 7.5735  loss_pitm: 0.2392  loss_mlm: 0.4968  loss_prd: 0.2924  loss_mrtd: 0.3549  time: 1.3078  data: 0.0001  max mem: 17660
Train Epoch: [0]  [1300/1804]  eta: 0:11:12  lr: 0.000010  loss_cl: 8.5743  loss_pitm: 0.5600  loss_mlm: 0.5634  loss_prd: 0.4940  loss_mrtd: 0.3289  time: 1.3136  data: 0.0001  max mem: 17660
Train Epoch: [0]  [1350/1804]  eta: 0:10:05  lr: 0.000010  loss_cl: 8.5214  loss_pitm: 0.1842  loss_mlm: 1.0388  loss_prd: 0.2797  loss_mrtd: 0.3536  time: 1.3118  data: 0.0002  max mem: 17660
Train Epoch: [0]  [1400/1804]  eta: 0:08:58  lr: 0.000010  loss_cl: 8.5383  loss_pitm: 0.1178  loss_mlm: 0.6266  loss_prd: 0.0564  loss_mrtd: 0.2889  time: 1.3127  data: 0.0002  max mem: 17660
Train Epoch: [0]  [1450/1804]  eta: 0:07:51  lr: 0.000010  loss_cl: 7.9307  loss_pitm: 0.0491  loss_mlm: 1.3250  loss_prd: 0.2819  loss_mrtd: 0.2786  time: 1.2987  data: 0.0002  max mem: 17660
Train Epoch: [0]  [1500/1804]  eta: 0:06:44  lr: 0.000010  loss_cl: 8.3104  loss_pitm: 0.0893  loss_mlm: 0.9856  loss_prd: 0.2101  loss_mrtd: 0.2951  time: 1.3009  data: 0.0002  max mem: 17660
Train Epoch: [0]  [1550/1804]  eta: 0:05:37  lr: 0.000010  loss_cl: 8.5602  loss_pitm: 0.1187  loss_mlm: 0.4194  loss_prd: 0.2487  loss_mrtd: 0.2674  time: 1.3025  data: 0.0002  max mem: 17721
Train Epoch: [0]  [1600/1804]  eta: 0:04:30  lr: 0.000010  loss_cl: 8.2029  loss_pitm: 0.1093  loss_mlm: 0.5728  loss_prd: 0.5123  loss_mrtd: 0.2899  time: 1.3035  data: 0.0001  max mem: 17721
Train Epoch: [0]  [1650/1804]  eta: 0:03:24  lr: 0.000010  loss_cl: 8.2045  loss_pitm: 0.0831  loss_mlm: 0.8341  loss_prd: 0.2197  loss_mrtd: 0.2665  time: 1.2986  data: 0.0002  max mem: 17721
Train Epoch: [0]  [1700/1804]  eta: 0:02:17  lr: 0.000010  loss_cl: 8.5181  loss_pitm: 0.1355  loss_mlm: 0.8896  loss_prd: 0.0924  loss_mrtd: 0.3401  time: 1.3062  data: 0.0002  max mem: 17721
Train Epoch: [0]  [1750/1804]  eta: 0:01:11  lr: 0.000010  loss_cl: 8.3695  loss_pitm: 0.0381  loss_mlm: 0.6951  loss_prd: 0.0710  loss_mrtd: 0.2957  time: 1.2950  data: 0.0002  max mem: 17721
Train Epoch: [0]  [1800/1804]  eta: 0:00:05  lr: 0.000010  loss_cl: 8.2079  loss_pitm: 0.2780  loss_mlm: 1.1789  loss_prd: 0.3098  loss_mrtd: 0.3026  time: 1.3077  data: 0.0002  max mem: 17721
Train Epoch: [0]  [1803/1804]  eta: 0:00:01  lr: 0.000010  loss_cl: 8.1735  loss_pitm: 0.0473  loss_mlm: 0.5354  loss_prd: 0.0619  loss_mrtd: 0.3375  time: 1.3095  data: 0.0002  max mem: 17721
Train Epoch: [0] Total time: 0:39:51 (1.3259 s / it)
Averaged stats: lr: 0.0000  loss_cl: 7.5898  loss_pitm: 0.1890  loss_mlm: 1.0105  loss_prd: 0.2830  loss_mrtd: 0.3520
Train Epoch的聚类: [1]  [   0/5241]  eta: 0:50:44    time: 0.5810  data: 0.5574  max mem: 17721
Train Epoch的聚类: [1]  [5240/5241]  eta: 0:00:00    time: 0.1717  data: 0.0002  max mem: 17721
Train Epoch的聚类: [1] Total time: 0:15:00 (0.1718 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 6058
-1 (未归入任何簇) 的数量: 11461

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [1]  [   0/2179]  eta: 18:43:19  lr: 0.000010  loss_cl: 10.1283  loss_pitm: 0.1545  loss_mlm: 0.7994  loss_prd: 0.3009  loss_mrtd: 0.2933  time: 30.9315  data: 0.6756  max mem: 17721
Train Epoch: [1]  [  50/2179]  eta: 1:05:40  lr: 0.000010  loss_cl: 9.3315  loss_pitm: 0.0983  loss_mlm: 0.9595  loss_prd: 0.2633  loss_mrtd: 0.3258  time: 1.2795  data: 0.0002  max mem: 17721
Train Epoch: [1]  [ 100/2179]  eta: 0:54:32  lr: 0.000010  loss_cl: 10.0160  loss_pitm: 0.2309  loss_mlm: 0.4107  loss_prd: 0.2936  loss_mrtd: 0.1879  time: 1.3017  data: 0.0002  max mem: 17734
Train Epoch: [1]  [ 150/2179]  eta: 0:50:18  lr: 0.000010  loss_cl: 9.6779  loss_pitm: 0.0908  loss_mlm: 0.7616  loss_prd: 0.3204  loss_mrtd: 0.2784  time: 1.3166  data: 0.0002  max mem: 17734
Train Epoch: [1]  [ 200/2179]  eta: 0:47:33  lr: 0.000010  loss_cl: 9.8290  loss_pitm: 0.1819  loss_mlm: 0.6071  loss_prd: 0.2323  loss_mrtd: 0.3238  time: 1.3135  data: 0.0002  max mem: 17734
Train Epoch: [1]  [ 250/2179]  eta: 0:45:25  lr: 0.000010  loss_cl: 9.6600  loss_pitm: 0.0227  loss_mlm: 0.7333  loss_prd: 0.0893  loss_mrtd: 0.2379  time: 1.2891  data: 0.0002  max mem: 17734
Train Epoch: [1]  [ 300/2179]  eta: 0:43:38  lr: 0.000010  loss_cl: 9.7058  loss_pitm: 0.0711  loss_mlm: 0.7382  loss_prd: 0.0911  loss_mrtd: 0.3139  time: 1.3024  data: 0.0002  max mem: 17734
Train Epoch: [1]  [ 350/2179]  eta: 0:42:06  lr: 0.000010  loss_cl: 10.1911  loss_pitm: 0.0372  loss_mlm: 0.6217  loss_prd: 0.4541  loss_mrtd: 0.3233  time: 1.2980  data: 0.0002  max mem: 17734
Train Epoch: [1]  [ 400/2179]  eta: 0:40:40  lr: 0.000010  loss_cl: 9.7990  loss_pitm: 0.2567  loss_mlm: 0.4267  loss_prd: 0.2272  loss_mrtd: 0.2786  time: 1.3179  data: 0.0002  max mem: 17734
Train Epoch: [1]  [ 450/2179]  eta: 0:39:20  lr: 0.000010  loss_cl: 9.2620  loss_pitm: 0.0673  loss_mlm: 0.8840  loss_prd: 0.2645  loss_mrtd: 0.3037  time: 1.3126  data: 0.0003  max mem: 17734
Train Epoch: [1]  [ 500/2179]  eta: 0:38:01  lr: 0.000010  loss_cl: 9.5524  loss_pitm: 0.0167  loss_mlm: 0.8329  loss_prd: 0.4733  loss_mrtd: 0.3566  time: 1.2996  data: 0.0001  max mem: 17734
Train Epoch: [1]  [ 550/2179]  eta: 0:36:44  lr: 0.000010  loss_cl: 9.6897  loss_pitm: 0.0973  loss_mlm: 0.4217  loss_prd: 0.1074  loss_mrtd: 0.2574  time: 1.2973  data: 0.0001  max mem: 17734
Train Epoch: [1]  [ 600/2179]  eta: 0:35:29  lr: 0.000010  loss_cl: 9.1523  loss_pitm: 0.0747  loss_mlm: 0.5537  loss_prd: 0.2644  loss_mrtd: 0.3507  time: 1.3030  data: 0.0001  max mem: 17734
Train Epoch: [1]  [ 650/2179]  eta: 0:34:16  lr: 0.000010  loss_cl: 9.4991  loss_pitm: 0.0730  loss_mlm: 0.3851  loss_prd: 0.4486  loss_mrtd: 0.2460  time: 1.3006  data: 0.0001  max mem: 17734
Train Epoch: [1]  [ 700/2179]  eta: 0:33:04  lr: 0.000010  loss_cl: 9.6985  loss_pitm: 0.0363  loss_mlm: 0.6414  loss_prd: 0.1024  loss_mrtd: 0.2738  time: 1.2950  data: 0.0001  max mem: 17734
Train Epoch: [1]  [ 750/2179]  eta: 0:31:53  lr: 0.000010  loss_cl: 9.5857  loss_pitm: 0.0444  loss_mlm: 0.6853  loss_prd: 0.3075  loss_mrtd: 0.3583  time: 1.2966  data: 0.0001  max mem: 17734
Train Epoch: [1]  [ 800/2179]  eta: 0:30:42  lr: 0.000010  loss_cl: 9.7828  loss_pitm: 0.1532  loss_mlm: 0.6900  loss_prd: 0.0999  loss_mrtd: 0.2351  time: 1.3008  data: 0.0001  max mem: 17734
Train Epoch: [1]  [ 850/2179]  eta: 0:29:32  lr: 0.000010  loss_cl: 9.5784  loss_pitm: 0.0465  loss_mlm: 0.5297  loss_prd: 0.4971  loss_mrtd: 0.2633  time: 1.2892  data: 0.0001  max mem: 17734
Train Epoch: [1]  [ 900/2179]  eta: 0:28:24  lr: 0.000010  loss_cl: 9.4500  loss_pitm: 0.0949  loss_mlm: 0.7226  loss_prd: 0.0858  loss_mrtd: 0.3916  time: 1.3068  data: 0.0001  max mem: 17734
Train Epoch: [1]  [ 950/2179]  eta: 0:27:16  lr: 0.000010  loss_cl: 9.2569  loss_pitm: 0.0419  loss_mlm: 0.6256  loss_prd: 0.4795  loss_mrtd: 0.3196  time: 1.3117  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1000/2179]  eta: 0:26:07  lr: 0.000010  loss_cl: 9.3803  loss_pitm: 0.0563  loss_mlm: 0.6599  loss_prd: 0.2703  loss_mrtd: 0.2763  time: 1.3097  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1050/2179]  eta: 0:25:00  lr: 0.000010  loss_cl: 9.4921  loss_pitm: 0.0649  loss_mlm: 0.8798  loss_prd: 0.0897  loss_mrtd: 0.3276  time: 1.3126  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1100/2179]  eta: 0:23:53  lr: 0.000010  loss_cl: 9.6641  loss_pitm: 0.1045  loss_mlm: 0.8988  loss_prd: 0.1121  loss_mrtd: 0.3071  time: 1.3119  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1150/2179]  eta: 0:22:45  lr: 0.000010  loss_cl: 9.3762  loss_pitm: 0.2261  loss_mlm: 0.8527  loss_prd: 0.0678  loss_mrtd: 0.3278  time: 1.3093  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1200/2179]  eta: 0:21:38  lr: 0.000010  loss_cl: 9.4648  loss_pitm: 0.0600  loss_mlm: 0.7124  loss_prd: 0.2401  loss_mrtd: 0.1961  time: 1.2993  data: 0.0002  max mem: 17734
Train Epoch: [1]  [1250/2179]  eta: 0:20:31  lr: 0.000010  loss_cl: 9.6769  loss_pitm: 0.1127  loss_mlm: 0.6657  loss_prd: 0.0824  loss_mrtd: 0.2100  time: 1.3137  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1300/2179]  eta: 0:19:25  lr: 0.000010  loss_cl: 9.5131  loss_pitm: 0.2278  loss_mlm: 0.7085  loss_prd: 0.5716  loss_mrtd: 0.2693  time: 1.3157  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1350/2179]  eta: 0:18:18  lr: 0.000010  loss_cl: 9.3112  loss_pitm: 0.1176  loss_mlm: 0.7713  loss_prd: 0.0969  loss_mrtd: 0.2937  time: 1.3141  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1400/2179]  eta: 0:17:11  lr: 0.000010  loss_cl: 8.9942  loss_pitm: 0.0536  loss_mlm: 0.4518  loss_prd: 0.1763  loss_mrtd: 0.2218  time: 1.3147  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1450/2179]  eta: 0:16:04  lr: 0.000010  loss_cl: 8.8308  loss_pitm: 0.0769  loss_mlm: 0.5431  loss_prd: 0.2492  loss_mrtd: 0.2647  time: 1.2968  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1500/2179]  eta: 0:14:58  lr: 0.000010  loss_cl: 8.8841  loss_pitm: 0.1563  loss_mlm: 0.6331  loss_prd: 0.3323  loss_mrtd: 0.3448  time: 1.3016  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1550/2179]  eta: 0:13:51  lr: 0.000010  loss_cl: 9.0025  loss_pitm: 0.0436  loss_mlm: 0.7533  loss_prd: 0.2885  loss_mrtd: 0.3076  time: 1.2917  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1600/2179]  eta: 0:12:44  lr: 0.000010  loss_cl: 9.1676  loss_pitm: 0.3022  loss_mlm: 0.6126  loss_prd: 0.5011  loss_mrtd: 0.3611  time: 1.3010  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1650/2179]  eta: 0:11:38  lr: 0.000010  loss_cl: 9.0216  loss_pitm: 0.1169  loss_mlm: 0.6629  loss_prd: 0.1878  loss_mrtd: 0.2584  time: 1.3205  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1700/2179]  eta: 0:10:32  lr: 0.000010  loss_cl: 8.9492  loss_pitm: 0.0898  loss_mlm: 0.5183  loss_prd: 0.2941  loss_mrtd: 0.2309  time: 1.2983  data: 0.0001  max mem: 17734
Train Epoch: [1]  [1750/2179]  eta: 0:09:26  lr: 0.000010  loss_cl: 8.8530  loss_pitm: 0.1347  loss_mlm: 0.4332  loss_prd: 0.0660  loss_mrtd: 0.2767  time: 1.3047  data: 0.0002  max mem: 17734
Train Epoch: [1]  [1800/2179]  eta: 0:08:20  lr: 0.000010  loss_cl: 8.6053  loss_pitm: 0.1263  loss_mlm: 0.5674  loss_prd: 0.3226  loss_mrtd: 0.3093  time: 1.3132  data: 0.0002  max mem: 17734
Train Epoch: [1]  [1850/2179]  eta: 0:07:13  lr: 0.000010  loss_cl: 8.8797  loss_pitm: 0.0394  loss_mlm: 0.6065  loss_prd: 0.2787  loss_mrtd: 0.2442  time: 1.2982  data: 0.0002  max mem: 17734
Train Epoch: [1]  [1900/2179]  eta: 0:06:07  lr: 0.000010  loss_cl: 8.5128  loss_pitm: 0.0915  loss_mlm: 0.5278  loss_prd: 0.2273  loss_mrtd: 0.3518  time: 1.3019  data: 0.0002  max mem: 17734
Train Epoch: [1]  [1950/2179]  eta: 0:05:01  lr: 0.000010  loss_cl: 8.3878  loss_pitm: 0.1357  loss_mlm: 0.8409  loss_prd: 0.0947  loss_mrtd: 0.3041  time: 1.3048  data: 0.0002  max mem: 17734
Train Epoch: [1]  [2000/2179]  eta: 0:03:55  lr: 0.000010  loss_cl: 9.1084  loss_pitm: 0.2916  loss_mlm: 0.7211  loss_prd: 0.4318  loss_mrtd: 0.2994  time: 1.3071  data: 0.0002  max mem: 17734
Train Epoch: [1]  [2050/2179]  eta: 0:02:49  lr: 0.000010  loss_cl: 8.3285  loss_pitm: 0.3417  loss_mlm: 0.8172  loss_prd: 0.3254  loss_mrtd: 0.3145  time: 1.3080  data: 0.0002  max mem: 17734
Train Epoch: [1]  [2100/2179]  eta: 0:01:44  lr: 0.000010  loss_cl: 8.3869  loss_pitm: 0.1328  loss_mlm: 0.6768  loss_prd: 0.0964  loss_mrtd: 0.2860  time: 1.3077  data: 0.0002  max mem: 17734
Train Epoch: [1]  [2150/2179]  eta: 0:00:38  lr: 0.000010  loss_cl: 8.5462  loss_pitm: 0.1671  loss_mlm: 0.6020  loss_prd: 0.1352  loss_mrtd: 0.2794  time: 1.3049  data: 0.0002  max mem: 17734
Train Epoch: [1]  [2178/2179]  eta: 0:00:01  lr: 0.000010  loss_cl: 8.4947  loss_pitm: 0.1131  loss_mlm: 0.4315  loss_prd: 0.1358  loss_mrtd: 0.2316  time: 1.2865  data: 0.0002  max mem: 17734
Train Epoch: [1] Total time: 0:47:49 (1.3168 s / it)
Averaged stats: lr: 0.0000  loss_cl: 9.2938  loss_pitm: 0.1239  loss_mlm: 0.6589  loss_prd: 0.2799  loss_mrtd: 0.2950
Train Epoch的聚类: [2]  [   0/5241]  eta: 0:47:03    time: 0.5388  data: 0.5206  max mem: 17734
Train Epoch的聚类: [2]  [5240/5241]  eta: 0:00:00    time: 0.1717  data: 0.0002  max mem: 17734
Train Epoch的聚类: [2] Total time: 0:14:59 (0.1716 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 5931
-1 (未归入任何簇) 的数量: 6512

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [2]  [   0/2369]  eta: 19:01:07  lr: 0.000010  loss_cl: 10.0416  loss_pitm: 0.1013  loss_mlm: 0.8428  loss_prd: 0.3708  loss_mrtd: 0.3624  time: 28.9014  data: 0.5491  max mem: 17734
Train Epoch: [2]  [  50/2369]  eta: 1:09:40  lr: 0.000010  loss_cl: 9.7239  loss_pitm: 0.0365  loss_mlm: 0.5071  loss_prd: 0.4303  loss_mrtd: 0.2203  time: 1.2678  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 100/2369]  eta: 0:58:33  lr: 0.000010  loss_cl: 9.8826  loss_pitm: 0.0948  loss_mlm: 0.3290  loss_prd: 0.1597  loss_mrtd: 0.3136  time: 1.2920  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 150/2369]  eta: 0:54:11  lr: 0.000010  loss_cl: 9.9650  loss_pitm: 0.0645  loss_mlm: 0.7413  loss_prd: 0.0692  loss_mrtd: 0.2588  time: 1.2988  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 200/2369]  eta: 0:51:29  lr: 0.000010  loss_cl: 9.8467  loss_pitm: 0.0225  loss_mlm: 0.6403  loss_prd: 0.0535  loss_mrtd: 0.3130  time: 1.2917  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 250/2369]  eta: 0:49:29  lr: 0.000010  loss_cl: 10.2180  loss_pitm: 0.0538  loss_mlm: 0.5266  loss_prd: 0.4525  loss_mrtd: 0.2064  time: 1.3228  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 300/2369]  eta: 0:47:48  lr: 0.000010  loss_cl: 9.9219  loss_pitm: 0.0432  loss_mlm: 0.5991  loss_prd: 0.0737  loss_mrtd: 0.3302  time: 1.3191  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 350/2369]  eta: 0:46:16  lr: 0.000010  loss_cl: 10.0067  loss_pitm: 0.0193  loss_mlm: 0.8456  loss_prd: 0.0740  loss_mrtd: 0.3308  time: 1.3120  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 400/2369]  eta: 0:44:52  lr: 0.000010  loss_cl: 10.6037  loss_pitm: 0.0727  loss_mlm: 0.4604  loss_prd: 0.5983  loss_mrtd: 0.2821  time: 1.3026  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 450/2369]  eta: 0:43:30  lr: 0.000010  loss_cl: 10.4853  loss_pitm: 0.0474  loss_mlm: 0.8704  loss_prd: 0.6102  loss_mrtd: 0.3115  time: 1.3018  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 500/2369]  eta: 0:42:10  lr: 0.000010  loss_cl: 10.1140  loss_pitm: 0.1775  loss_mlm: 0.3908  loss_prd: 0.2279  loss_mrtd: 0.2396  time: 1.2917  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 550/2369]  eta: 0:40:53  lr: 0.000010  loss_cl: 9.9506  loss_pitm: 0.0306  loss_mlm: 0.6012  loss_prd: 0.0803  loss_mrtd: 0.2643  time: 1.2955  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 600/2369]  eta: 0:39:40  lr: 0.000010  loss_cl: 10.4745  loss_pitm: 0.0956  loss_mlm: 0.5182  loss_prd: 0.1183  loss_mrtd: 0.3591  time: 1.3120  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 650/2369]  eta: 0:38:28  lr: 0.000010  loss_cl: 10.0313  loss_pitm: 0.0507  loss_mlm: 0.4993  loss_prd: 0.2847  loss_mrtd: 0.2949  time: 1.3073  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 700/2369]  eta: 0:37:16  lr: 0.000010  loss_cl: 9.8770  loss_pitm: 0.0459  loss_mlm: 0.6219  loss_prd: 0.2115  loss_mrtd: 0.2936  time: 1.3070  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 750/2369]  eta: 0:36:05  lr: 0.000010  loss_cl: 9.9035  loss_pitm: 0.2471  loss_mlm: 0.4115  loss_prd: 0.2809  loss_mrtd: 0.2839  time: 1.2970  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 800/2369]  eta: 0:34:55  lr: 0.000010  loss_cl: 9.9114  loss_pitm: 0.0254  loss_mlm: 0.7018  loss_prd: 0.0678  loss_mrtd: 0.2988  time: 1.3057  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 850/2369]  eta: 0:33:45  lr: 0.000010  loss_cl: 9.5700  loss_pitm: 0.2373  loss_mlm: 0.3642  loss_prd: 0.0769  loss_mrtd: 0.2199  time: 1.2964  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 900/2369]  eta: 0:32:36  lr: 0.000010  loss_cl: 9.7715  loss_pitm: 0.2144  loss_mlm: 0.4069  loss_prd: 0.0664  loss_mrtd: 0.3025  time: 1.3132  data: 0.0001  max mem: 17734
Train Epoch: [2]  [ 950/2369]  eta: 0:31:28  lr: 0.000010  loss_cl: 10.0566  loss_pitm: 0.1682  loss_mlm: 0.2860  loss_prd: 0.1761  loss_mrtd: 0.1585  time: 1.3000  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1000/2369]  eta: 0:30:19  lr: 0.000010  loss_cl: 9.8776  loss_pitm: 0.0539  loss_mlm: 0.4280  loss_prd: 0.6799  loss_mrtd: 0.2615  time: 1.2985  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1050/2369]  eta: 0:29:11  lr: 0.000010  loss_cl: 10.1230  loss_pitm: 0.2288  loss_mlm: 0.6673  loss_prd: 0.2768  loss_mrtd: 0.2321  time: 1.2999  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1100/2369]  eta: 0:28:03  lr: 0.000010  loss_cl: 9.8837  loss_pitm: 0.0784  loss_mlm: 0.7336  loss_prd: 0.2711  loss_mrtd: 0.3731  time: 1.3174  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1150/2369]  eta: 0:26:56  lr: 0.000010  loss_cl: 9.6740  loss_pitm: 0.0623  loss_mlm: 0.9210  loss_prd: 0.3102  loss_mrtd: 0.2978  time: 1.2940  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1200/2369]  eta: 0:25:49  lr: 0.000010  loss_cl: 9.3525  loss_pitm: 0.1869  loss_mlm: 0.6117  loss_prd: 0.1080  loss_mrtd: 0.3057  time: 1.3121  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1250/2369]  eta: 0:24:42  lr: 0.000010  loss_cl: 9.3731  loss_pitm: 0.0558  loss_mlm: 0.7335  loss_prd: 0.4232  loss_mrtd: 0.3496  time: 1.3188  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1300/2369]  eta: 0:23:36  lr: 0.000010  loss_cl: 9.3672  loss_pitm: 0.1599  loss_mlm: 0.6022  loss_prd: 0.4317  loss_mrtd: 0.3070  time: 1.3190  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1350/2369]  eta: 0:22:29  lr: 0.000010  loss_cl: 9.2683  loss_pitm: 0.0708  loss_mlm: 0.6342  loss_prd: 0.0529  loss_mrtd: 0.3856  time: 1.3128  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1400/2369]  eta: 0:21:22  lr: 0.000010  loss_cl: 9.3128  loss_pitm: 0.0618  loss_mlm: 0.7116  loss_prd: 0.3607  loss_mrtd: 0.3160  time: 1.3111  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1450/2369]  eta: 0:20:15  lr: 0.000010  loss_cl: 9.1667  loss_pitm: 0.0846  loss_mlm: 0.7429  loss_prd: 0.2343  loss_mrtd: 0.2114  time: 1.3182  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1500/2369]  eta: 0:19:09  lr: 0.000010  loss_cl: 9.2222  loss_pitm: 0.0391  loss_mlm: 0.6165  loss_prd: 0.0772  loss_mrtd: 0.2524  time: 1.3034  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1550/2369]  eta: 0:18:02  lr: 0.000010  loss_cl: 9.3632  loss_pitm: 0.0379  loss_mlm: 0.4336  loss_prd: 0.2455  loss_mrtd: 0.2317  time: 1.3056  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1600/2369]  eta: 0:16:56  lr: 0.000010  loss_cl: 8.3246  loss_pitm: 0.3649  loss_mlm: 1.0078  loss_prd: 0.2712  loss_mrtd: 0.2673  time: 1.3085  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1650/2369]  eta: 0:15:49  lr: 0.000010  loss_cl: 8.8650  loss_pitm: 0.3957  loss_mlm: 0.9362  loss_prd: 0.4590  loss_mrtd: 0.2669  time: 1.3191  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1700/2369]  eta: 0:14:43  lr: 0.000010  loss_cl: 8.9719  loss_pitm: 0.0603  loss_mlm: 0.4423  loss_prd: 0.4593  loss_mrtd: 0.2646  time: 1.3006  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1750/2369]  eta: 0:13:37  lr: 0.000010  loss_cl: 8.4248  loss_pitm: 0.1429  loss_mlm: 0.7098  loss_prd: 0.2180  loss_mrtd: 0.3265  time: 1.3145  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1800/2369]  eta: 0:12:31  lr: 0.000010  loss_cl: 8.9170  loss_pitm: 0.2380  loss_mlm: 0.5782  loss_prd: 0.5068  loss_mrtd: 0.3347  time: 1.3091  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1850/2369]  eta: 0:11:24  lr: 0.000010  loss_cl: 8.4868  loss_pitm: 0.1248  loss_mlm: 0.4072  loss_prd: 0.3267  loss_mrtd: 0.2399  time: 1.2999  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1900/2369]  eta: 0:10:18  lr: 0.000010  loss_cl: 8.5044  loss_pitm: 0.1709  loss_mlm: 0.6713  loss_prd: 0.2394  loss_mrtd: 0.3088  time: 1.3022  data: 0.0001  max mem: 17734
Train Epoch: [2]  [1950/2369]  eta: 0:09:12  lr: 0.000010  loss_cl: 8.1904  loss_pitm: 0.0511  loss_mlm: 0.4061  loss_prd: 0.0692  loss_mrtd: 0.3225  time: 1.3057  data: 0.0001  max mem: 17734
Train Epoch: [2]  [2000/2369]  eta: 0:08:06  lr: 0.000010  loss_cl: 8.5216  loss_pitm: 0.0384  loss_mlm: 0.5295  loss_prd: 0.4237  loss_mrtd: 0.2346  time: 1.3129  data: 0.0001  max mem: 17734
Train Epoch: [2]  [2050/2369]  eta: 0:07:00  lr: 0.000010  loss_cl: 8.6450  loss_pitm: 0.1092  loss_mlm: 0.5057  loss_prd: 0.0820  loss_mrtd: 0.3154  time: 1.2970  data: 0.0001  max mem: 17734
Train Epoch: [2]  [2100/2369]  eta: 0:05:54  lr: 0.000010  loss_cl: 8.0277  loss_pitm: 0.0590  loss_mlm: 0.3667  loss_prd: 0.0809  loss_mrtd: 0.2461  time: 1.3042  data: 0.0001  max mem: 17734
Train Epoch: [2]  [2150/2369]  eta: 0:04:48  lr: 0.000010  loss_cl: 8.6582  loss_pitm: 0.1463  loss_mlm: 0.6783  loss_prd: 0.1167  loss_mrtd: 0.3261  time: 1.3044  data: 0.0001  max mem: 17734
Train Epoch: [2]  [2200/2369]  eta: 0:03:42  lr: 0.000010  loss_cl: 8.0865  loss_pitm: 0.4336  loss_mlm: 0.5066  loss_prd: 0.5788  loss_mrtd: 0.2492  time: 1.3056  data: 0.0001  max mem: 17734
Train Epoch: [2]  [2250/2369]  eta: 0:02:36  lr: 0.000010  loss_cl: 8.0229  loss_pitm: 0.0631  loss_mlm: 0.3441  loss_prd: 0.2767  loss_mrtd: 0.2614  time: 1.3152  data: 0.0001  max mem: 17734
Train Epoch: [2]  [2300/2369]  eta: 0:01:30  lr: 0.000010  loss_cl: 7.9581  loss_pitm: 0.0314  loss_mlm: 0.8391  loss_prd: 0.0719  loss_mrtd: 0.2834  time: 1.3117  data: 0.0001  max mem: 17734
Train Epoch: [2]  [2350/2369]  eta: 0:00:25  lr: 0.000010  loss_cl: 7.7070  loss_pitm: 0.1015  loss_mlm: 0.8044  loss_prd: 0.2506  loss_mrtd: 0.2597  time: 1.3062  data: 0.0001  max mem: 17734
Train Epoch: [2]  [2368/2369]  eta: 0:00:01  lr: 0.000010  loss_cl: 7.9111  loss_pitm: 0.3767  loss_mlm: 0.3758  loss_prd: 0.3604  loss_mrtd: 0.2552  time: 1.3065  data: 0.0002  max mem: 17734
Train Epoch: [2] Total time: 0:51:59 (1.3167 s / it)
Averaged stats: lr: 0.0000  loss_cl: 9.3754  loss_pitm: 0.1152  loss_mlm: 0.6061  loss_prd: 0.2681  loss_mrtd: 0.2781
Train Epoch的聚类: [3]  [   0/5241]  eta: 0:59:31    time: 0.6814  data: 0.6612  max mem: 17734
Train Epoch的聚类: [3]  [5240/5241]  eta: 0:00:00    time: 0.1716  data: 0.0002  max mem: 17734
Train Epoch的聚类: [3] Total time: 0:14:59 (0.1716 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 5599
-1 (未归入任何簇) 的数量: 5099

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [3]  [   0/2424]  eta: 1 day, 3:58:00  lr: 0.000010  loss_cl: 10.3938  loss_pitm: 0.0995  loss_mlm: 0.9114  loss_prd: 0.2443  loss_mrtd: 0.2326  time: 41.5348  data: 0.5612  max mem: 17734
Train Epoch: [3]  [  50/2424]  eta: 1:21:23  lr: 0.000010  loss_cl: 10.0995  loss_pitm: 0.2041  loss_mlm: 0.4953  loss_prd: 0.1434  loss_mrtd: 0.2487  time: 1.2804  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 100/2424]  eta: 1:05:07  lr: 0.000010  loss_cl: 10.1405  loss_pitm: 0.1036  loss_mlm: 0.6872  loss_prd: 0.6532  loss_mrtd: 0.2944  time: 1.3084  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 150/2424]  eta: 0:58:53  lr: 0.000010  loss_cl: 10.0671  loss_pitm: 0.0482  loss_mlm: 0.4732  loss_prd: 0.3999  loss_mrtd: 0.2481  time: 1.2981  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 200/2424]  eta: 0:55:18  lr: 0.000010  loss_cl: 10.0666  loss_pitm: 0.0147  loss_mlm: 0.5951  loss_prd: 0.0652  loss_mrtd: 0.3260  time: 1.2982  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 250/2424]  eta: 0:52:39  lr: 0.000010  loss_cl: 10.2946  loss_pitm: 0.0717  loss_mlm: 0.3518  loss_prd: 0.1058  loss_mrtd: 0.2407  time: 1.2992  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 300/2424]  eta: 0:50:31  lr: 0.000010  loss_cl: 10.0109  loss_pitm: 0.0133  loss_mlm: 0.5355  loss_prd: 0.0647  loss_mrtd: 0.2436  time: 1.2992  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 350/2424]  eta: 0:48:42  lr: 0.000010  loss_cl: 10.0819  loss_pitm: 0.1323  loss_mlm: 0.6686  loss_prd: 0.1909  loss_mrtd: 0.1898  time: 1.3036  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 400/2424]  eta: 0:47:07  lr: 0.000010  loss_cl: 9.9424  loss_pitm: 0.0920  loss_mlm: 0.3581  loss_prd: 0.1181  loss_mrtd: 0.2479  time: 1.3098  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 450/2424]  eta: 0:45:36  lr: 0.000010  loss_cl: 9.9808  loss_pitm: 0.0135  loss_mlm: 0.5600  loss_prd: 0.4395  loss_mrtd: 0.2745  time: 1.2970  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 500/2424]  eta: 0:44:10  lr: 0.000010  loss_cl: 9.8661  loss_pitm: 0.2541  loss_mlm: 0.4344  loss_prd: 0.1641  loss_mrtd: 0.2579  time: 1.2963  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 550/2424]  eta: 0:42:52  lr: 0.000010  loss_cl: 10.1994  loss_pitm: 0.0657  loss_mlm: 0.5678  loss_prd: 0.0691  loss_mrtd: 0.2065  time: 1.3217  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 600/2424]  eta: 0:41:34  lr: 0.000010  loss_cl: 9.9914  loss_pitm: 0.0871  loss_mlm: 0.4336  loss_prd: 0.2711  loss_mrtd: 0.3014  time: 1.3160  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 650/2424]  eta: 0:40:18  lr: 0.000010  loss_cl: 9.7510  loss_pitm: 0.0209  loss_mlm: 0.5903  loss_prd: 0.3070  loss_mrtd: 0.3190  time: 1.3031  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 700/2424]  eta: 0:39:03  lr: 0.000010  loss_cl: 10.0224  loss_pitm: 0.0246  loss_mlm: 0.5361  loss_prd: 0.2634  loss_mrtd: 0.2563  time: 1.3173  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 750/2424]  eta: 0:37:51  lr: 0.000010  loss_cl: 10.0137  loss_pitm: 0.0279  loss_mlm: 0.7691  loss_prd: 0.2546  loss_mrtd: 0.2746  time: 1.3232  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 800/2424]  eta: 0:36:38  lr: 0.000010  loss_cl: 9.7901  loss_pitm: 0.1634  loss_mlm: 0.6927  loss_prd: 0.4196  loss_mrtd: 0.2288  time: 1.3038  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 850/2424]  eta: 0:35:24  lr: 0.000010  loss_cl: 9.9640  loss_pitm: 0.1437  loss_mlm: 0.9955  loss_prd: 0.2190  loss_mrtd: 0.2092  time: 1.2889  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 900/2424]  eta: 0:34:12  lr: 0.000010  loss_cl: 9.8683  loss_pitm: 0.0797  loss_mlm: 0.5287  loss_prd: 0.5390  loss_mrtd: 0.2342  time: 1.3034  data: 0.0001  max mem: 17734
Train Epoch: [3]  [ 950/2424]  eta: 0:33:01  lr: 0.000010  loss_cl: 9.5792  loss_pitm: 0.1102  loss_mlm: 0.5467  loss_prd: 0.4200  loss_mrtd: 0.2513  time: 1.3031  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1000/2424]  eta: 0:31:50  lr: 0.000010  loss_cl: 9.7637  loss_pitm: 0.1874  loss_mlm: 0.6285  loss_prd: 0.0878  loss_mrtd: 0.2067  time: 1.2984  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1050/2424]  eta: 0:30:40  lr: 0.000010  loss_cl: 9.7161  loss_pitm: 0.0234  loss_mlm: 0.5292  loss_prd: 0.3228  loss_mrtd: 0.2762  time: 1.2883  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1100/2424]  eta: 0:29:31  lr: 0.000010  loss_cl: 9.9246  loss_pitm: 0.0760  loss_mlm: 0.3494  loss_prd: 0.3420  loss_mrtd: 0.3048  time: 1.3153  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1150/2424]  eta: 0:28:22  lr: 0.000010  loss_cl: 9.7157  loss_pitm: 0.0616  loss_mlm: 0.5880  loss_prd: 0.6860  loss_mrtd: 0.2745  time: 1.2981  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1200/2424]  eta: 0:27:14  lr: 0.000010  loss_cl: 9.3224  loss_pitm: 0.1108  loss_mlm: 0.4894  loss_prd: 0.1813  loss_mrtd: 0.2495  time: 1.3114  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1250/2424]  eta: 0:26:06  lr: 0.000010  loss_cl: 9.1764  loss_pitm: 0.0395  loss_mlm: 1.0252  loss_prd: 0.0775  loss_mrtd: 0.3007  time: 1.3051  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1300/2424]  eta: 0:24:58  lr: 0.000010  loss_cl: 9.2267  loss_pitm: 0.0318  loss_mlm: 0.7908  loss_prd: 0.0983  loss_mrtd: 0.2956  time: 1.2908  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1350/2424]  eta: 0:23:50  lr: 0.000010  loss_cl: 9.1441  loss_pitm: 0.0094  loss_mlm: 0.3584  loss_prd: 0.0756  loss_mrtd: 0.2931  time: 1.3141  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1400/2424]  eta: 0:22:43  lr: 0.000010  loss_cl: 9.3573  loss_pitm: 0.0516  loss_mlm: 0.8278  loss_prd: 0.7100  loss_mrtd: 0.2570  time: 1.3081  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1450/2424]  eta: 0:21:36  lr: 0.000010  loss_cl: 8.9498  loss_pitm: 0.1595  loss_mlm: 0.3478  loss_prd: 0.4671  loss_mrtd: 0.2917  time: 1.3184  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1500/2424]  eta: 0:20:28  lr: 0.000010  loss_cl: 8.6825  loss_pitm: 0.0161  loss_mlm: 0.4995  loss_prd: 0.0525  loss_mrtd: 0.3659  time: 1.3010  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1550/2424]  eta: 0:19:21  lr: 0.000010  loss_cl: 9.1466  loss_pitm: 0.2486  loss_mlm: 0.3608  loss_prd: 0.2300  loss_mrtd: 0.2192  time: 1.2980  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1600/2424]  eta: 0:18:14  lr: 0.000010  loss_cl: 8.9074  loss_pitm: 0.0128  loss_mlm: 0.4787  loss_prd: 0.0511  loss_mrtd: 0.2707  time: 1.3035  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1650/2424]  eta: 0:17:07  lr: 0.000010  loss_cl: 8.9633  loss_pitm: 0.1129  loss_mlm: 0.4600  loss_prd: 0.0602  loss_mrtd: 0.1996  time: 1.3077  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1700/2424]  eta: 0:16:00  lr: 0.000010  loss_cl: 8.5917  loss_pitm: 0.0753  loss_mlm: 0.6673  loss_prd: 0.0552  loss_mrtd: 0.2638  time: 1.2933  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1750/2424]  eta: 0:14:53  lr: 0.000010  loss_cl: 8.7779  loss_pitm: 0.0502  loss_mlm: 0.3701  loss_prd: 0.0922  loss_mrtd: 0.2148  time: 1.3073  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1800/2424]  eta: 0:13:46  lr: 0.000010  loss_cl: 9.0075  loss_pitm: 0.0857  loss_mlm: 0.3129  loss_prd: 0.4446  loss_mrtd: 0.2287  time: 1.3046  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1850/2424]  eta: 0:12:40  lr: 0.000010  loss_cl: 8.7001  loss_pitm: 0.5522  loss_mlm: 0.4825  loss_prd: 0.3613  loss_mrtd: 0.2753  time: 1.2929  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1900/2424]  eta: 0:11:33  lr: 0.000010  loss_cl: 8.5511  loss_pitm: 0.0423  loss_mlm: 0.7508  loss_prd: 0.3016  loss_mrtd: 0.2776  time: 1.2906  data: 0.0001  max mem: 17734
Train Epoch: [3]  [1950/2424]  eta: 0:10:27  lr: 0.000010  loss_cl: 8.5272  loss_pitm: 0.0115  loss_mlm: 0.7730  loss_prd: 0.0576  loss_mrtd: 0.3188  time: 1.3021  data: 0.0001  max mem: 17734
Train Epoch: [3]  [2000/2424]  eta: 0:09:20  lr: 0.000010  loss_cl: 8.2934  loss_pitm: 0.0343  loss_mlm: 0.7122  loss_prd: 0.4640  loss_mrtd: 0.2880  time: 1.3233  data: 0.0001  max mem: 17734
Train Epoch: [3]  [2050/2424]  eta: 0:08:14  lr: 0.000010  loss_cl: 8.0405  loss_pitm: 0.0348  loss_mlm: 0.3322  loss_prd: 0.2755  loss_mrtd: 0.2664  time: 1.3048  data: 0.0001  max mem: 17734
Train Epoch: [3]  [2100/2424]  eta: 0:07:08  lr: 0.000010  loss_cl: 8.2656  loss_pitm: 0.1342  loss_mlm: 0.7753  loss_prd: 0.3377  loss_mrtd: 0.2162  time: 1.3031  data: 0.0001  max mem: 17734
Train Epoch: [3]  [2150/2424]  eta: 0:06:02  lr: 0.000010  loss_cl: 8.0705  loss_pitm: 0.0434  loss_mlm: 0.6176  loss_prd: 0.0732  loss_mrtd: 0.3243  time: 1.3056  data: 0.0001  max mem: 17734
Train Epoch: [3]  [2200/2424]  eta: 0:04:55  lr: 0.000010  loss_cl: 8.1475  loss_pitm: 0.0181  loss_mlm: 0.6728  loss_prd: 0.4747  loss_mrtd: 0.2938  time: 1.2817  data: 0.0001  max mem: 17734
Train Epoch: [3]  [2250/2424]  eta: 0:03:49  lr: 0.000010  loss_cl: 7.8662  loss_pitm: 0.0693  loss_mlm: 0.8971  loss_prd: 0.0744  loss_mrtd: 0.3440  time: 1.3050  data: 0.0001  max mem: 17734
Train Epoch: [3]  [2300/2424]  eta: 0:02:43  lr: 0.000010  loss_cl: 7.9893  loss_pitm: 0.0669  loss_mlm: 0.8726  loss_prd: 0.3205  loss_mrtd: 0.3143  time: 1.2928  data: 0.0001  max mem: 17738
Train Epoch: [3]  [2350/2424]  eta: 0:01:37  lr: 0.000010  loss_cl: 7.7003  loss_pitm: 0.1787  loss_mlm: 0.4978  loss_prd: 0.4700  loss_mrtd: 0.2856  time: 1.3124  data: 0.0001  max mem: 17738
Train Epoch: [3]  [2400/2424]  eta: 0:00:31  lr: 0.000010  loss_cl: 7.4778  loss_pitm: 0.0699  loss_mlm: 0.4713  loss_prd: 0.5150  loss_mrtd: 0.2216  time: 1.3097  data: 0.0002  max mem: 17738
Train Epoch: [3]  [2423/2424]  eta: 0:00:01  lr: 0.000010  loss_cl: 7.5959  loss_pitm: 0.0243  loss_mlm: 0.6574  loss_prd: 0.6605  loss_mrtd: 0.2754  time: 1.3135  data: 0.0002  max mem: 17738
Train Epoch: [3] Total time: 0:53:20 (1.3202 s / it)
Averaged stats: lr: 0.0000  loss_cl: 9.2838  loss_pitm: 0.1012  loss_mlm: 0.5726  loss_prd: 0.2664  loss_mrtd: 0.2664
Train Epoch的聚类: [4]  [   0/5241]  eta: 0:51:51    time: 0.5936  data: 0.5682  max mem: 17738
Train Epoch的聚类: [4]  [5240/5241]  eta: 0:00:00    time: 0.1716  data: 0.0002  max mem: 17738
Train Epoch的聚类: [4] Total time: 0:14:59 (0.1716 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 5334
-1 (未归入任何簇) 的数量: 4139

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [4]  [   0/2461]  eta: 20:12:40  lr: 0.000010  loss_cl: 10.4910  loss_pitm: 0.1703  loss_mlm: 0.5735  loss_prd: 0.1442  loss_mrtd: 0.2257  time: 29.5654  data: 1.5270  max mem: 17738
Train Epoch: [4]  [  50/2461]  eta: 1:13:52  lr: 0.000010  loss_cl: 10.2874  loss_pitm: 0.0817  loss_mlm: 0.7340  loss_prd: 0.1101  loss_mrtd: 0.2950  time: 1.2765  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 100/2461]  eta: 1:01:50  lr: 0.000010  loss_cl: 9.9373  loss_pitm: 0.0959  loss_mlm: 0.2911  loss_prd: 0.2809  loss_mrtd: 0.2599  time: 1.3078  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 150/2461]  eta: 0:57:15  lr: 0.000010  loss_cl: 9.8065  loss_pitm: 0.0280  loss_mlm: 0.3992  loss_prd: 0.5342  loss_mrtd: 0.2811  time: 1.3297  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 200/2461]  eta: 0:54:32  lr: 0.000010  loss_cl: 9.9060  loss_pitm: 0.0365  loss_mlm: 0.5618  loss_prd: 0.4800  loss_mrtd: 0.2301  time: 1.3282  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 250/2461]  eta: 0:52:33  lr: 0.000010  loss_cl: 10.1571  loss_pitm: 0.0137  loss_mlm: 0.5729  loss_prd: 0.5110  loss_mrtd: 0.3140  time: 1.3534  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 300/2461]  eta: 0:50:48  lr: 0.000010  loss_cl: 10.2362  loss_pitm: 0.0662  loss_mlm: 0.4836  loss_prd: 0.3104  loss_mrtd: 0.2542  time: 1.3393  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 350/2461]  eta: 0:49:13  lr: 0.000010  loss_cl: 9.7642  loss_pitm: 0.0588  loss_mlm: 0.5870  loss_prd: 0.3898  loss_mrtd: 0.2605  time: 1.3337  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 400/2461]  eta: 0:47:49  lr: 0.000010  loss_cl: 9.6620  loss_pitm: 0.1030  loss_mlm: 0.3859  loss_prd: 0.0675  loss_mrtd: 0.2396  time: 1.3552  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 450/2461]  eta: 0:46:26  lr: 0.000010  loss_cl: 9.7497  loss_pitm: 0.0320  loss_mlm: 0.3743  loss_prd: 0.9506  loss_mrtd: 0.2039  time: 1.3376  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 500/2461]  eta: 0:45:09  lr: 0.000010  loss_cl: 9.9692  loss_pitm: 0.1643  loss_mlm: 0.5531  loss_prd: 0.4539  loss_mrtd: 0.2640  time: 1.3352  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 550/2461]  eta: 0:43:51  lr: 0.000010  loss_cl: 10.2590  loss_pitm: 0.0351  loss_mlm: 0.6868  loss_prd: 0.5733  loss_mrtd: 0.3156  time: 1.3343  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 600/2461]  eta: 0:42:35  lr: 0.000010  loss_cl: 10.1039  loss_pitm: 0.4405  loss_mlm: 0.5795  loss_prd: 0.3013  loss_mrtd: 0.2504  time: 1.3263  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 650/2461]  eta: 0:41:19  lr: 0.000010  loss_cl: 10.0490  loss_pitm: 0.0287  loss_mlm: 0.8237  loss_prd: 0.1606  loss_mrtd: 0.2274  time: 1.3122  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 700/2461]  eta: 0:40:06  lr: 0.000010  loss_cl: 10.3975  loss_pitm: 0.0028  loss_mlm: 0.4384  loss_prd: 0.4869  loss_mrtd: 0.2196  time: 1.3394  data: 0.0003  max mem: 17738
Train Epoch: [4]  [ 750/2461]  eta: 0:38:54  lr: 0.000010  loss_cl: 9.9919  loss_pitm: 0.0106  loss_mlm: 0.3351  loss_prd: 0.0696  loss_mrtd: 0.2662  time: 1.3187  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 800/2461]  eta: 0:37:41  lr: 0.000010  loss_cl: 9.5888  loss_pitm: 0.0799  loss_mlm: 0.9245  loss_prd: 0.5226  loss_mrtd: 0.2247  time: 1.3339  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 850/2461]  eta: 0:36:30  lr: 0.000010  loss_cl: 9.5885  loss_pitm: 0.1037  loss_mlm: 1.0185  loss_prd: 0.1236  loss_mrtd: 0.2649  time: 1.3270  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 900/2461]  eta: 0:35:20  lr: 0.000010  loss_cl: 9.5340  loss_pitm: 0.0591  loss_mlm: 1.0306  loss_prd: 0.2052  loss_mrtd: 0.3074  time: 1.3434  data: 0.0002  max mem: 17738
Train Epoch: [4]  [ 950/2461]  eta: 0:34:10  lr: 0.000010  loss_cl: 9.9402  loss_pitm: 0.0301  loss_mlm: 0.4445  loss_prd: 0.2781  loss_mrtd: 0.3366  time: 1.3496  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1000/2461]  eta: 0:33:00  lr: 0.000010  loss_cl: 9.6706  loss_pitm: 0.1409  loss_mlm: 0.3762  loss_prd: 0.0705  loss_mrtd: 0.2412  time: 1.3175  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1050/2461]  eta: 0:31:51  lr: 0.000010  loss_cl: 9.6616  loss_pitm: 0.1686  loss_mlm: 0.4498  loss_prd: 0.3474  loss_mrtd: 0.1778  time: 1.3324  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1100/2461]  eta: 0:30:42  lr: 0.000010  loss_cl: 9.7902  loss_pitm: 0.1250  loss_mlm: 0.3949  loss_prd: 0.2559  loss_mrtd: 0.3120  time: 1.3381  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1150/2461]  eta: 0:29:33  lr: 0.000010  loss_cl: 9.2294  loss_pitm: 0.1102  loss_mlm: 0.7226  loss_prd: 0.5848  loss_mrtd: 0.2085  time: 1.3150  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1200/2461]  eta: 0:28:24  lr: 0.000010  loss_cl: 9.3286  loss_pitm: 0.0953  loss_mlm: 0.2822  loss_prd: 0.3686  loss_mrtd: 0.2383  time: 1.3399  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1250/2461]  eta: 0:27:16  lr: 0.000010  loss_cl: 9.2991  loss_pitm: 0.0190  loss_mlm: 0.4279  loss_prd: 0.0703  loss_mrtd: 0.2008  time: 1.3235  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1300/2461]  eta: 0:26:07  lr: 0.000010  loss_cl: 9.1320  loss_pitm: 0.0821  loss_mlm: 0.3637  loss_prd: 0.0901  loss_mrtd: 0.2659  time: 1.3235  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1350/2461]  eta: 0:24:58  lr: 0.000010  loss_cl: 9.0495  loss_pitm: 0.1050  loss_mlm: 0.7761  loss_prd: 0.5006  loss_mrtd: 0.3000  time: 1.3134  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1400/2461]  eta: 0:23:50  lr: 0.000010  loss_cl: 9.1490  loss_pitm: 0.0550  loss_mlm: 0.3617  loss_prd: 0.3231  loss_mrtd: 0.2692  time: 1.3317  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1450/2461]  eta: 0:22:42  lr: 0.000010  loss_cl: 9.1011  loss_pitm: 0.1031  loss_mlm: 0.4747  loss_prd: 0.0811  loss_mrtd: 0.2131  time: 1.3360  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1500/2461]  eta: 0:21:35  lr: 0.000010  loss_cl: 9.0016  loss_pitm: 0.0458  loss_mlm: 0.2964  loss_prd: 0.0499  loss_mrtd: 0.2148  time: 1.3470  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1550/2461]  eta: 0:20:27  lr: 0.000010  loss_cl: 9.2597  loss_pitm: 0.1200  loss_mlm: 1.0629  loss_prd: 0.3629  loss_mrtd: 0.2676  time: 1.3364  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1600/2461]  eta: 0:19:19  lr: 0.000010  loss_cl: 8.6602  loss_pitm: 0.1038  loss_mlm: 0.2883  loss_prd: 0.5361  loss_mrtd: 0.2787  time: 1.3347  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1650/2461]  eta: 0:18:12  lr: 0.000010  loss_cl: 8.7381  loss_pitm: 0.0839  loss_mlm: 0.3880  loss_prd: 0.5471  loss_mrtd: 0.2769  time: 1.3372  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1700/2461]  eta: 0:17:04  lr: 0.000010  loss_cl: 8.8785  loss_pitm: 0.1090  loss_mlm: 0.6107  loss_prd: 0.5518  loss_mrtd: 0.3199  time: 1.3383  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1750/2461]  eta: 0:15:57  lr: 0.000010  loss_cl: 8.5865  loss_pitm: 0.0087  loss_mlm: 0.7943  loss_prd: 0.0469  loss_mrtd: 0.2603  time: 1.3321  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1800/2461]  eta: 0:14:49  lr: 0.000010  loss_cl: 8.6560  loss_pitm: 0.2273  loss_mlm: 0.4809  loss_prd: 0.1213  loss_mrtd: 0.3028  time: 1.3270  data: 0.0003  max mem: 17738
Train Epoch: [4]  [1850/2461]  eta: 0:13:41  lr: 0.000010  loss_cl: 8.7103  loss_pitm: 0.1120  loss_mlm: 0.6389  loss_prd: 0.1778  loss_mrtd: 0.2797  time: 1.3155  data: 0.0003  max mem: 17738
Train Epoch: [4]  [1900/2461]  eta: 0:12:34  lr: 0.000010  loss_cl: 8.2519  loss_pitm: 0.1070  loss_mlm: 0.4843  loss_prd: 0.1450  loss_mrtd: 0.2605  time: 1.3264  data: 0.0002  max mem: 17738
Train Epoch: [4]  [1950/2461]  eta: 0:11:26  lr: 0.000010  loss_cl: 8.7692  loss_pitm: 0.0241  loss_mlm: 1.0420  loss_prd: 0.1734  loss_mrtd: 0.3029  time: 1.3221  data: 0.0002  max mem: 17738
Train Epoch: [4]  [2000/2461]  eta: 0:10:19  lr: 0.000010  loss_cl: 7.9521  loss_pitm: 0.1149  loss_mlm: 0.6432  loss_prd: 0.6586  loss_mrtd: 0.2506  time: 1.3255  data: 0.0002  max mem: 17738
Train Epoch: [4]  [2050/2461]  eta: 0:09:12  lr: 0.000010  loss_cl: 8.2785  loss_pitm: 0.1067  loss_mlm: 0.4252  loss_prd: 0.5020  loss_mrtd: 0.2386  time: 1.3335  data: 0.0002  max mem: 17738
Train Epoch: [4]  [2100/2461]  eta: 0:08:04  lr: 0.000010  loss_cl: 8.4472  loss_pitm: 0.0521  loss_mlm: 0.6086  loss_prd: 0.2902  loss_mrtd: 0.2740  time: 1.3484  data: 0.0002  max mem: 17738
Train Epoch: [4]  [2150/2461]  eta: 0:06:57  lr: 0.000010  loss_cl: 7.9754  loss_pitm: 0.0940  loss_mlm: 0.4489  loss_prd: 0.2958  loss_mrtd: 0.2547  time: 1.3181  data: 0.0002  max mem: 17738
Train Epoch: [4]  [2200/2461]  eta: 0:05:50  lr: 0.000010  loss_cl: 8.2577  loss_pitm: 0.2052  loss_mlm: 0.7468  loss_prd: 0.0667  loss_mrtd: 0.3007  time: 1.3386  data: 0.0002  max mem: 17738
Train Epoch: [4]  [2250/2461]  eta: 0:04:43  lr: 0.000010  loss_cl: 7.6226  loss_pitm: 0.0671  loss_mlm: 0.3974  loss_prd: 0.3661  loss_mrtd: 0.2529  time: 1.3222  data: 0.0002  max mem: 17738
Train Epoch: [4]  [2300/2461]  eta: 0:03:36  lr: 0.000010  loss_cl: 7.9265  loss_pitm: 0.1215  loss_mlm: 0.6179  loss_prd: 0.2502  loss_mrtd: 0.2274  time: 1.3278  data: 0.0002  max mem: 17738
Train Epoch: [4]  [2350/2461]  eta: 0:02:29  lr: 0.000010  loss_cl: 7.7176  loss_pitm: 0.2928  loss_mlm: 0.5706  loss_prd: 0.5635  loss_mrtd: 0.2275  time: 1.3225  data: 0.0002  max mem: 17738
Train Epoch: [4]  [2400/2461]  eta: 0:01:21  lr: 0.000010  loss_cl: 7.6097  loss_pitm: 0.1258  loss_mlm: 0.4495  loss_prd: 0.9605  loss_mrtd: 0.2593  time: 1.3350  data: 0.0002  max mem: 17738
Train Epoch: [4]  [2450/2461]  eta: 0:00:14  lr: 0.000010  loss_cl: 7.4033  loss_pitm: 0.0204  loss_mlm: 0.4837  loss_prd: 0.2119  loss_mrtd: 0.2005  time: 1.3360  data: 0.0002  max mem: 17738
Train Epoch: [4]  [2460/2461]  eta: 0:00:01  lr: 0.000010  loss_cl: 7.7719  loss_pitm: 0.0079  loss_mlm: 0.4716  loss_prd: 0.0664  loss_mrtd: 0.2988  time: 1.3375  data: 0.0002  max mem: 17738
Train Epoch: [4] Total time: 0:55:03 (1.3422 s / it)
Averaged stats: lr: 0.0000  loss_cl: 9.1533  loss_pitm: 0.0935  loss_mlm: 0.5568  loss_prd: 0.2597  loss_mrtd: 0.2595
Train Epoch的聚类: [5]  [   0/5241]  eta: 1:02:54    time: 0.7201  data: 0.6959  max mem: 17738
Train Epoch的聚类: [5]  [5240/5241]  eta: 0:00:00    time: 0.1715  data: 0.0002  max mem: 17738
Train Epoch的聚类: [5] Total time: 0:14:59 (0.1715 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 5262
-1 (未归入任何簇) 的数量: 3543

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [5]  [   0/2483]  eta: 23:35:16  lr: 0.000009  loss_cl: 10.2381  loss_pitm: 0.2057  loss_mlm: 0.3398  loss_prd: 0.0672  loss_mrtd: 0.2945  time: 34.1990  data: 0.5811  max mem: 17738
Train Epoch: [5]  [  50/2483]  eta: 1:19:06  lr: 0.000009  loss_cl: 9.3964  loss_pitm: 0.0156  loss_mlm: 0.5261  loss_prd: 0.2598  loss_mrtd: 0.3408  time: 1.3118  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 100/2483]  eta: 1:05:16  lr: 0.000009  loss_cl: 9.4154  loss_pitm: 0.0130  loss_mlm: 0.5532  loss_prd: 0.2676  loss_mrtd: 0.2510  time: 1.3344  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 150/2483]  eta: 0:59:54  lr: 0.000009  loss_cl: 9.3273  loss_pitm: 0.0533  loss_mlm: 0.4287  loss_prd: 0.0936  loss_mrtd: 0.3183  time: 1.3300  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 200/2483]  eta: 0:56:40  lr: 0.000009  loss_cl: 9.3065  loss_pitm: 0.0535  loss_mlm: 0.6301  loss_prd: 0.0806  loss_mrtd: 0.3393  time: 1.3338  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 250/2483]  eta: 0:54:15  lr: 0.000009  loss_cl: 9.8178  loss_pitm: 0.1833  loss_mlm: 0.3663  loss_prd: 0.0605  loss_mrtd: 0.1678  time: 1.3181  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 300/2483]  eta: 0:52:15  lr: 0.000009  loss_cl: 9.6367  loss_pitm: 0.1031  loss_mlm: 0.6349  loss_prd: 0.0555  loss_mrtd: 0.3208  time: 1.3296  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 350/2483]  eta: 0:50:34  lr: 0.000009  loss_cl: 9.5571  loss_pitm: 0.0089  loss_mlm: 0.5288  loss_prd: 0.0529  loss_mrtd: 0.2561  time: 1.3288  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 400/2483]  eta: 0:49:03  lr: 0.000009  loss_cl: 9.4342  loss_pitm: 0.0525  loss_mlm: 0.3886  loss_prd: 0.3584  loss_mrtd: 0.2104  time: 1.3514  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 450/2483]  eta: 0:47:34  lr: 0.000009  loss_cl: 9.9374  loss_pitm: 0.1814  loss_mlm: 0.5144  loss_prd: 0.1868  loss_mrtd: 0.2667  time: 1.3242  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 500/2483]  eta: 0:46:10  lr: 0.000009  loss_cl: 9.4724  loss_pitm: 0.1578  loss_mlm: 0.6083  loss_prd: 0.1077  loss_mrtd: 0.1968  time: 1.3402  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 550/2483]  eta: 0:44:50  lr: 0.000009  loss_cl: 9.6299  loss_pitm: 0.0185  loss_mlm: 0.3360  loss_prd: 0.4727  loss_mrtd: 0.2576  time: 1.3459  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 600/2483]  eta: 0:43:32  lr: 0.000009  loss_cl: 9.1884  loss_pitm: 0.0974  loss_mlm: 0.7865  loss_prd: 0.2988  loss_mrtd: 0.3362  time: 1.3337  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 650/2483]  eta: 0:42:15  lr: 0.000009  loss_cl: 9.9820  loss_pitm: 0.0585  loss_mlm: 0.3901  loss_prd: 0.0862  loss_mrtd: 0.2292  time: 1.3491  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 700/2483]  eta: 0:41:00  lr: 0.000009  loss_cl: 9.2855  loss_pitm: 0.1301  loss_mlm: 0.8504  loss_prd: 0.9121  loss_mrtd: 0.2693  time: 1.3492  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 750/2483]  eta: 0:39:45  lr: 0.000009  loss_cl: 9.3085  loss_pitm: 0.1671  loss_mlm: 0.6170  loss_prd: 0.0782  loss_mrtd: 0.2685  time: 1.3073  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 800/2483]  eta: 0:38:29  lr: 0.000009  loss_cl: 9.5665  loss_pitm: 0.1298  loss_mlm: 0.4438  loss_prd: 0.2904  loss_mrtd: 0.2403  time: 1.3255  data: 0.0002  max mem: 17738
Train Epoch: [5]  [ 850/2483]  eta: 0:37:15  lr: 0.000009  loss_cl: 9.2948  loss_pitm: 0.0850  loss_mlm: 0.3656  loss_prd: 0.1430  loss_mrtd: 0.2301  time: 1.3057  data: 0.0001  max mem: 17738
Train Epoch: [5]  [ 900/2483]  eta: 0:36:02  lr: 0.000009  loss_cl: 9.5652  loss_pitm: 0.1202  loss_mlm: 0.5038  loss_prd: 0.5363  loss_mrtd: 0.1996  time: 1.3159  data: 0.0001  max mem: 17738
Train Epoch: [5]  [ 950/2483]  eta: 0:34:50  lr: 0.000009  loss_cl: 9.1937  loss_pitm: 0.0106  loss_mlm: 0.6336  loss_prd: 0.0680  loss_mrtd: 0.2310  time: 1.3178  data: 0.0001  max mem: 17738
Train Epoch: [5]  [1000/2483]  eta: 0:33:38  lr: 0.000009  loss_cl: 9.2760  loss_pitm: 0.0251  loss_mlm: 0.4765  loss_prd: 0.0659  loss_mrtd: 0.3196  time: 1.3069  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1050/2483]  eta: 0:32:27  lr: 0.000009  loss_cl: 9.1973  loss_pitm: 0.1453  loss_mlm: 0.6404  loss_prd: 0.4604  loss_mrtd: 0.2464  time: 1.3223  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1100/2483]  eta: 0:31:16  lr: 0.000009  loss_cl: 8.5394  loss_pitm: 0.1338  loss_mlm: 0.6034  loss_prd: 0.1820  loss_mrtd: 0.1859  time: 1.3167  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1150/2483]  eta: 0:30:06  lr: 0.000009  loss_cl: 9.3864  loss_pitm: 0.2816  loss_mlm: 0.2437  loss_prd: 0.1282  loss_mrtd: 0.2899  time: 1.3182  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1200/2483]  eta: 0:28:57  lr: 0.000009  loss_cl: 8.9272  loss_pitm: 0.0189  loss_mlm: 0.5175  loss_prd: 0.4136  loss_mrtd: 0.2205  time: 1.3163  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1250/2483]  eta: 0:27:47  lr: 0.000009  loss_cl: 9.2121  loss_pitm: 0.0528  loss_mlm: 0.5190  loss_prd: 0.0791  loss_mrtd: 0.1971  time: 1.3172  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1300/2483]  eta: 0:26:38  lr: 0.000009  loss_cl: 8.8135  loss_pitm: 0.0940  loss_mlm: 0.4637  loss_prd: 0.0988  loss_mrtd: 0.2783  time: 1.3200  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1350/2483]  eta: 0:25:29  lr: 0.000009  loss_cl: 8.5287  loss_pitm: 0.0079  loss_mlm: 0.5425  loss_prd: 0.2499  loss_mrtd: 0.1949  time: 1.3204  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1400/2483]  eta: 0:24:20  lr: 0.000009  loss_cl: 8.8108  loss_pitm: 0.0740  loss_mlm: 0.3264  loss_prd: 0.3770  loss_mrtd: 0.2365  time: 1.3132  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1450/2483]  eta: 0:23:11  lr: 0.000009  loss_cl: 9.0801  loss_pitm: 0.0814  loss_mlm: 0.3816  loss_prd: 0.1748  loss_mrtd: 0.2437  time: 1.3164  data: 0.0001  max mem: 17738
Train Epoch: [5]  [1500/2483]  eta: 0:22:03  lr: 0.000009  loss_cl: 8.9741  loss_pitm: 0.1724  loss_mlm: 0.3825  loss_prd: 0.2492  loss_mrtd: 0.2236  time: 1.3175  data: 0.0001  max mem: 17738
Train Epoch: [5]  [1550/2483]  eta: 0:20:55  lr: 0.000009  loss_cl: 8.7634  loss_pitm: 0.0326  loss_mlm: 0.5283  loss_prd: 0.2955  loss_mrtd: 0.3021  time: 1.3094  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1600/2483]  eta: 0:19:47  lr: 0.000009  loss_cl: 8.4432  loss_pitm: 0.2751  loss_mlm: 0.8564  loss_prd: 0.6108  loss_mrtd: 0.2248  time: 1.3129  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1650/2483]  eta: 0:18:39  lr: 0.000009  loss_cl: 8.5806  loss_pitm: 0.0330  loss_mlm: 0.5349  loss_prd: 0.3570  loss_mrtd: 0.3267  time: 1.3126  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1700/2483]  eta: 0:17:31  lr: 0.000009  loss_cl: 8.7176  loss_pitm: 0.2066  loss_mlm: 0.4931  loss_prd: 0.0602  loss_mrtd: 0.2718  time: 1.3147  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1750/2483]  eta: 0:16:23  lr: 0.000009  loss_cl: 8.4483  loss_pitm: 0.1008  loss_mlm: 0.4724  loss_prd: 0.2689  loss_mrtd: 0.2536  time: 1.3138  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1800/2483]  eta: 0:15:15  lr: 0.000009  loss_cl: 8.2979  loss_pitm: 0.0375  loss_mlm: 0.3875  loss_prd: 0.0805  loss_mrtd: 0.2357  time: 1.3202  data: 0.0001  max mem: 17738
Train Epoch: [5]  [1850/2483]  eta: 0:14:08  lr: 0.000009  loss_cl: 8.5341  loss_pitm: 0.0208  loss_mlm: 0.6465  loss_prd: 0.0703  loss_mrtd: 0.2860  time: 1.3181  data: 0.0002  max mem: 17738
Train Epoch: [5]  [1900/2483]  eta: 0:13:00  lr: 0.000009  loss_cl: 8.4252  loss_pitm: 0.0559  loss_mlm: 0.5356  loss_prd: 0.0880  loss_mrtd: 0.2996  time: 1.3132  data: 0.0001  max mem: 17738
Train Epoch: [5]  [1950/2483]  eta: 0:11:53  lr: 0.000009  loss_cl: 7.8865  loss_pitm: 0.0351  loss_mlm: 0.5473  loss_prd: 0.2044  loss_mrtd: 0.2582  time: 1.3067  data: 0.0001  max mem: 17738
Train Epoch: [5]  [2000/2483]  eta: 0:10:46  lr: 0.000009  loss_cl: 7.7465  loss_pitm: 0.0784  loss_mlm: 0.5892  loss_prd: 0.1049  loss_mrtd: 0.3029  time: 1.3162  data: 0.0001  max mem: 17738
Train Epoch: [5]  [2050/2483]  eta: 0:09:39  lr: 0.000009  loss_cl: 7.5537  loss_pitm: 0.0497  loss_mlm: 0.4831  loss_prd: 0.2656  loss_mrtd: 0.1949  time: 1.3129  data: 0.0001  max mem: 17738
Train Epoch: [5]  [2100/2483]  eta: 0:08:32  lr: 0.000009  loss_cl: 7.7612  loss_pitm: 0.0223  loss_mlm: 0.4553  loss_prd: 0.3213  loss_mrtd: 0.2890  time: 1.3147  data: 0.0001  max mem: 17738
Train Epoch: [5]  [2150/2483]  eta: 0:07:25  lr: 0.000009  loss_cl: 8.1922  loss_pitm: 0.0511  loss_mlm: 0.4432  loss_prd: 0.4061  loss_mrtd: 0.2455  time: 1.3236  data: 0.0001  max mem: 17738
Train Epoch: [5]  [2200/2483]  eta: 0:06:18  lr: 0.000009  loss_cl: 7.9169  loss_pitm: 0.1527  loss_mlm: 0.4039  loss_prd: 0.5467  loss_mrtd: 0.1950  time: 1.3229  data: 0.0001  max mem: 17738
Train Epoch: [5]  [2250/2483]  eta: 0:05:11  lr: 0.000009  loss_cl: 7.4406  loss_pitm: 0.0855  loss_mlm: 0.6862  loss_prd: 0.1566  loss_mrtd: 0.2695  time: 1.3075  data: 0.0001  max mem: 17738
Train Epoch: [5]  [2300/2483]  eta: 0:04:04  lr: 0.000009  loss_cl: 8.0287  loss_pitm: 0.2260  loss_mlm: 0.5041  loss_prd: 0.1242  loss_mrtd: 0.2306  time: 1.3221  data: 0.0002  max mem: 17738
Train Epoch: [5]  [2350/2483]  eta: 0:02:57  lr: 0.000009  loss_cl: 7.6489  loss_pitm: 0.2730  loss_mlm: 0.6549  loss_prd: 0.1012  loss_mrtd: 0.2833  time: 1.3165  data: 0.0002  max mem: 17738
Train Epoch: [5]  [2400/2483]  eta: 0:01:50  lr: 0.000009  loss_cl: 6.9870  loss_pitm: 0.1280  loss_mlm: 0.5038  loss_prd: 0.1232  loss_mrtd: 0.3379  time: 1.3192  data: 0.0001  max mem: 17738
Train Epoch: [5]  [2450/2483]  eta: 0:00:44  lr: 0.000009  loss_cl: 7.3937  loss_pitm: 0.1152  loss_mlm: 0.8742  loss_prd: 0.6305  loss_mrtd: 0.3158  time: 1.3141  data: 0.0002  max mem: 17738
Train Epoch: [5]  [2482/2483]  eta: 0:00:01  lr: 0.000009  loss_cl: 7.1661  loss_pitm: 0.2443  loss_mlm: 0.3943  loss_prd: 0.2970  loss_mrtd: 0.2004  time: 1.3219  data: 0.0002  max mem: 17744
Train Epoch: [5] Total time: 0:55:12 (1.3342 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.8523  loss_pitm: 0.0888  loss_mlm: 0.5341  loss_prd: 0.2522  loss_mrtd: 0.2539
Train Epoch的聚类: [6]  [   0/5241]  eta: 0:54:33    time: 0.6246  data: 0.6056  max mem: 17744
Train Epoch的聚类: [6]  [5240/5241]  eta: 0:00:00    time: 0.1717  data: 0.0002  max mem: 17744
Train Epoch的聚类: [6] Total time: 0:14:59 (0.1717 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 5038
-1 (未归入任何簇) 的数量: 3278

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [6]  [   0/2494]  eta: 1 day, 0:28:01  lr: 0.000009  loss_cl: 11.1350  loss_pitm: 0.1848  loss_mlm: 0.6308  loss_prd: 0.3295  loss_mrtd: 0.2017  time: 35.3174  data: 1.0182  max mem: 17744
Train Epoch: [6]  [  50/2494]  eta: 1:19:47  lr: 0.000009  loss_cl: 9.5475  loss_pitm: 0.0794  loss_mlm: 0.5444  loss_prd: 0.2933  loss_mrtd: 0.1674  time: 1.3043  data: 0.0002  max mem: 17744
Train Epoch: [6]  [ 100/2494]  eta: 1:05:24  lr: 0.000009  loss_cl: 9.7556  loss_pitm: 0.0535  loss_mlm: 0.5992  loss_prd: 0.1027  loss_mrtd: 0.3151  time: 1.3193  data: 0.0002  max mem: 17744
Train Epoch: [6]  [ 150/2494]  eta: 0:59:39  lr: 0.000009  loss_cl: 9.6353  loss_pitm: 0.0317  loss_mlm: 0.5527  loss_prd: 0.3040  loss_mrtd: 0.2214  time: 1.2905  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 200/2494]  eta: 0:56:08  lr: 0.000009  loss_cl: 9.9243  loss_pitm: 0.0181  loss_mlm: 0.4289  loss_prd: 0.3095  loss_mrtd: 0.2317  time: 1.2941  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 250/2494]  eta: 0:53:40  lr: 0.000009  loss_cl: 9.3888  loss_pitm: 0.0159  loss_mlm: 0.5822  loss_prd: 0.2850  loss_mrtd: 0.2305  time: 1.3045  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 300/2494]  eta: 0:51:38  lr: 0.000009  loss_cl: 9.6365  loss_pitm: 0.0536  loss_mlm: 0.4745  loss_prd: 0.3170  loss_mrtd: 0.2532  time: 1.3005  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 350/2494]  eta: 0:49:53  lr: 0.000009  loss_cl: 9.2443  loss_pitm: 0.0055  loss_mlm: 0.4558  loss_prd: 0.0728  loss_mrtd: 0.2459  time: 1.2969  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 400/2494]  eta: 0:48:17  lr: 0.000009  loss_cl: 9.9713  loss_pitm: 0.1373  loss_mlm: 0.4520  loss_prd: 0.1726  loss_mrtd: 0.2924  time: 1.2929  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 450/2494]  eta: 0:46:48  lr: 0.000009  loss_cl: 9.5577  loss_pitm: 0.0607  loss_mlm: 0.2659  loss_prd: 0.5240  loss_mrtd: 0.2304  time: 1.2980  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 500/2494]  eta: 0:45:24  lr: 0.000009  loss_cl: 9.5034  loss_pitm: 0.0430  loss_mlm: 0.6551  loss_prd: 0.3439  loss_mrtd: 0.2637  time: 1.2908  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 550/2494]  eta: 0:44:04  lr: 0.000009  loss_cl: 9.8053  loss_pitm: 0.0099  loss_mlm: 0.5359  loss_prd: 0.1222  loss_mrtd: 0.2628  time: 1.3023  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 600/2494]  eta: 0:42:46  lr: 0.000009  loss_cl: 9.6247  loss_pitm: 0.1093  loss_mlm: 0.7358  loss_prd: 0.5145  loss_mrtd: 0.2792  time: 1.2987  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 650/2494]  eta: 0:41:31  lr: 0.000009  loss_cl: 9.6642  loss_pitm: 0.0506  loss_mlm: 0.6977  loss_prd: 0.6438  loss_mrtd: 0.2499  time: 1.3033  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 700/2494]  eta: 0:40:16  lr: 0.000009  loss_cl: 9.4652  loss_pitm: 0.0269  loss_mlm: 0.5145  loss_prd: 0.3103  loss_mrtd: 0.2637  time: 1.2937  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 750/2494]  eta: 0:39:03  lr: 0.000009  loss_cl: 9.5799  loss_pitm: 0.0318  loss_mlm: 0.4736  loss_prd: 0.2359  loss_mrtd: 0.2068  time: 1.3007  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 800/2494]  eta: 0:37:52  lr: 0.000009  loss_cl: 9.5387  loss_pitm: 0.1034  loss_mlm: 0.5302  loss_prd: 0.0741  loss_mrtd: 0.2431  time: 1.2895  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 850/2494]  eta: 0:36:39  lr: 0.000009  loss_cl: 9.7465  loss_pitm: 0.0132  loss_mlm: 0.3715  loss_prd: 0.2894  loss_mrtd: 0.2738  time: 1.2882  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 900/2494]  eta: 0:35:28  lr: 0.000009  loss_cl: 8.8299  loss_pitm: 0.0114  loss_mlm: 0.3481  loss_prd: 0.2504  loss_mrtd: 0.2606  time: 1.2891  data: 0.0001  max mem: 17744
Train Epoch: [6]  [ 950/2494]  eta: 0:34:18  lr: 0.000009  loss_cl: 9.4665  loss_pitm: 0.0250  loss_mlm: 0.6330  loss_prd: 0.3788  loss_mrtd: 0.2313  time: 1.3017  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1000/2494]  eta: 0:33:09  lr: 0.000009  loss_cl: 9.5003  loss_pitm: 0.0228  loss_mlm: 0.8071  loss_prd: 0.3187  loss_mrtd: 0.3154  time: 1.3034  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1050/2494]  eta: 0:32:00  lr: 0.000009  loss_cl: 9.9126  loss_pitm: 0.0207  loss_mlm: 0.4955  loss_prd: 0.0668  loss_mrtd: 0.1836  time: 1.2889  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1100/2494]  eta: 0:30:51  lr: 0.000009  loss_cl: 9.3652  loss_pitm: 0.0426  loss_mlm: 0.5977  loss_prd: 0.0689  loss_mrtd: 0.2171  time: 1.2941  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1150/2494]  eta: 0:29:43  lr: 0.000009  loss_cl: 9.1637  loss_pitm: 0.2350  loss_mlm: 0.4040  loss_prd: 0.2337  loss_mrtd: 0.2434  time: 1.2999  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1200/2494]  eta: 0:28:35  lr: 0.000009  loss_cl: 9.3041  loss_pitm: 0.0678  loss_mlm: 0.1206  loss_prd: 0.2287  loss_mrtd: 0.2136  time: 1.2951  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1250/2494]  eta: 0:27:27  lr: 0.000009  loss_cl: 9.1756  loss_pitm: 0.0164  loss_mlm: 0.5883  loss_prd: 0.2574  loss_mrtd: 0.2518  time: 1.2995  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1300/2494]  eta: 0:26:20  lr: 0.000009  loss_cl: 8.8258  loss_pitm: 0.0065  loss_mlm: 0.5259  loss_prd: 0.0790  loss_mrtd: 0.3078  time: 1.2936  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1350/2494]  eta: 0:25:13  lr: 0.000009  loss_cl: 9.0948  loss_pitm: 0.0376  loss_mlm: 0.5336  loss_prd: 0.1199  loss_mrtd: 0.2625  time: 1.2875  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1400/2494]  eta: 0:24:06  lr: 0.000009  loss_cl: 8.4926  loss_pitm: 0.0387  loss_mlm: 0.4020  loss_prd: 0.0660  loss_mrtd: 0.2628  time: 1.3078  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1450/2494]  eta: 0:22:59  lr: 0.000009  loss_cl: 8.8860  loss_pitm: 0.1081  loss_mlm: 0.4844  loss_prd: 0.3273  loss_mrtd: 0.2381  time: 1.3074  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1500/2494]  eta: 0:21:52  lr: 0.000009  loss_cl: 9.2335  loss_pitm: 0.0333  loss_mlm: 0.8164  loss_prd: 0.2674  loss_mrtd: 0.2678  time: 1.2991  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1550/2494]  eta: 0:20:45  lr: 0.000009  loss_cl: 8.5536  loss_pitm: 0.0581  loss_mlm: 0.4707  loss_prd: 0.4916  loss_mrtd: 0.2378  time: 1.3009  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1600/2494]  eta: 0:19:39  lr: 0.000009  loss_cl: 8.4706  loss_pitm: 0.0365  loss_mlm: 0.7636  loss_prd: 0.4765  loss_mrtd: 0.3341  time: 1.3061  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1650/2494]  eta: 0:18:32  lr: 0.000009  loss_cl: 8.3651  loss_pitm: 0.0079  loss_mlm: 0.4903  loss_prd: 0.0405  loss_mrtd: 0.2231  time: 1.2969  data: 0.0001  max mem: 17744
Train Epoch: [6]  [1700/2494]  eta: 0:17:26  lr: 0.000009  loss_cl: 8.4974  loss_pitm: 0.0181  loss_mlm: 0.3523  loss_prd: 0.2761  loss_mrtd: 0.3166  time: 1.3072  data: 0.0001  max mem: 17752
Train Epoch: [6]  [1750/2494]  eta: 0:16:20  lr: 0.000009  loss_cl: 8.7703  loss_pitm: 0.3632  loss_mlm: 0.2770  loss_prd: 0.1483  loss_mrtd: 0.2675  time: 1.3134  data: 0.0001  max mem: 17752
Train Epoch: [6]  [1800/2494]  eta: 0:15:14  lr: 0.000009  loss_cl: 8.4831  loss_pitm: 0.0562  loss_mlm: 0.4209  loss_prd: 0.0868  loss_mrtd: 0.3116  time: 1.3119  data: 0.0001  max mem: 17752
Train Epoch: [6]  [1850/2494]  eta: 0:14:07  lr: 0.000009  loss_cl: 8.1843  loss_pitm: 0.1472  loss_mlm: 0.4359  loss_prd: 0.0524  loss_mrtd: 0.2290  time: 1.2862  data: 0.0001  max mem: 17752
Train Epoch: [6]  [1900/2494]  eta: 0:13:01  lr: 0.000009  loss_cl: 8.1718  loss_pitm: 0.1117  loss_mlm: 0.5556  loss_prd: 0.2976  loss_mrtd: 0.1980  time: 1.2997  data: 0.0001  max mem: 17758
Train Epoch: [6]  [1950/2494]  eta: 0:11:55  lr: 0.000009  loss_cl: 8.0619  loss_pitm: 0.0740  loss_mlm: 0.6694  loss_prd: 0.4981  loss_mrtd: 0.2796  time: 1.3040  data: 0.0001  max mem: 17758
Train Epoch: [6]  [2000/2494]  eta: 0:10:49  lr: 0.000009  loss_cl: 7.9645  loss_pitm: 0.1060  loss_mlm: 0.2850  loss_prd: 0.4438  loss_mrtd: 0.1742  time: 1.3004  data: 0.0001  max mem: 17758
Train Epoch: [6]  [2050/2494]  eta: 0:09:43  lr: 0.000009  loss_cl: 7.9198  loss_pitm: 0.0352  loss_mlm: 0.7758  loss_prd: 0.0624  loss_mrtd: 0.2726  time: 1.3032  data: 0.0001  max mem: 17758
Train Epoch: [6]  [2100/2494]  eta: 0:08:37  lr: 0.000009  loss_cl: 8.0915  loss_pitm: 0.1440  loss_mlm: 0.8461  loss_prd: 0.1997  loss_mrtd: 0.3613  time: 1.2952  data: 0.0001  max mem: 17758
Train Epoch: [6]  [2150/2494]  eta: 0:07:32  lr: 0.000009  loss_cl: 7.9984  loss_pitm: 0.0142  loss_mlm: 0.3497  loss_prd: 0.5285  loss_mrtd: 0.1930  time: 1.3012  data: 0.0001  max mem: 17758
Train Epoch: [6]  [2200/2494]  eta: 0:06:26  lr: 0.000009  loss_cl: 7.9389  loss_pitm: 0.0425  loss_mlm: 0.4995  loss_prd: 0.0715  loss_mrtd: 0.2044  time: 1.2971  data: 0.0001  max mem: 17758
Train Epoch: [6]  [2250/2494]  eta: 0:05:20  lr: 0.000009  loss_cl: 7.8239  loss_pitm: 0.0542  loss_mlm: 0.7037  loss_prd: 0.0803  loss_mrtd: 0.3056  time: 1.2967  data: 0.0001  max mem: 17758
Train Epoch: [6]  [2300/2494]  eta: 0:04:14  lr: 0.000009  loss_cl: 8.0315  loss_pitm: 0.1178  loss_mlm: 0.8436  loss_prd: 0.5038  loss_mrtd: 0.2492  time: 1.2972  data: 0.0001  max mem: 17758
Train Epoch: [6]  [2350/2494]  eta: 0:03:09  lr: 0.000009  loss_cl: 7.3518  loss_pitm: 0.0611  loss_mlm: 0.9269  loss_prd: 0.6084  loss_mrtd: 0.2446  time: 1.3079  data: 0.0001  max mem: 17758
Train Epoch: [6]  [2400/2494]  eta: 0:02:03  lr: 0.000009  loss_cl: 7.2854  loss_pitm: 0.0754  loss_mlm: 0.2478  loss_prd: 0.5280  loss_mrtd: 0.1678  time: 1.2961  data: 0.0001  max mem: 17758
Train Epoch: [6]  [2450/2494]  eta: 0:00:57  lr: 0.000009  loss_cl: 7.3526  loss_pitm: 0.0396  loss_mlm: 0.5012  loss_prd: 0.0589  loss_mrtd: 0.2890  time: 1.2947  data: 0.0001  max mem: 17758
Train Epoch: [6]  [2493/2494]  eta: 0:00:01  lr: 0.000009  loss_cl: 7.6476  loss_pitm: 0.1363  loss_mlm: 0.3618  loss_prd: 0.3143  loss_mrtd: 0.2338  time: 1.2963  data: 0.0002  max mem: 17758
Train Epoch: [6] Total time: 0:54:32 (1.3123 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.9364  loss_pitm: 0.0780  loss_mlm: 0.5233  loss_prd: 0.2470  loss_mrtd: 0.2495
Train Epoch的聚类: [7]  [   0/5241]  eta: 0:57:23    time: 0.6570  data: 0.6362  max mem: 17758
Train Epoch的聚类: [7]  [5240/5241]  eta: 0:00:00    time: 0.1718  data: 0.0002  max mem: 17758
Train Epoch的聚类: [7] Total time: 0:14:59 (0.1716 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4982
-1 (未归入任何簇) 的数量: 3087

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [7]  [   0/2501]  eta: 23:47:40  lr: 0.000009  loss_cl: 10.6609  loss_pitm: 0.0187  loss_mlm: 0.5652  loss_prd: 0.4871  loss_mrtd: 0.2408  time: 34.2504  data: 0.6435  max mem: 17758
Train Epoch: [7]  [  50/2501]  eta: 1:18:19  lr: 0.000009  loss_cl: 9.8149  loss_pitm: 0.0305  loss_mlm: 0.4716  loss_prd: 0.2120  loss_mrtd: 0.2506  time: 1.2836  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 100/2501]  eta: 1:04:17  lr: 0.000009  loss_cl: 10.0465  loss_pitm: 0.0620  loss_mlm: 0.3975  loss_prd: 0.0720  loss_mrtd: 0.2314  time: 1.2958  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 150/2501]  eta: 0:58:52  lr: 0.000009  loss_cl: 9.8037  loss_pitm: 0.1042  loss_mlm: 0.4853  loss_prd: 0.2185  loss_mrtd: 0.2411  time: 1.2953  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 200/2501]  eta: 0:55:37  lr: 0.000009  loss_cl: 9.0209  loss_pitm: 0.0438  loss_mlm: 0.6086  loss_prd: 0.1608  loss_mrtd: 0.2182  time: 1.2902  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 250/2501]  eta: 0:53:19  lr: 0.000009  loss_cl: 9.5327  loss_pitm: 0.0139  loss_mlm: 0.5038  loss_prd: 0.3278  loss_mrtd: 0.3061  time: 1.3026  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 300/2501]  eta: 0:51:24  lr: 0.000009  loss_cl: 9.4985  loss_pitm: 0.0132  loss_mlm: 0.5934  loss_prd: 0.2351  loss_mrtd: 0.2050  time: 1.3041  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 350/2501]  eta: 0:49:42  lr: 0.000009  loss_cl: 9.7657  loss_pitm: 0.0578  loss_mlm: 0.5660  loss_prd: 0.2735  loss_mrtd: 0.2668  time: 1.2993  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 400/2501]  eta: 0:48:10  lr: 0.000009  loss_cl: 9.7188  loss_pitm: 0.0823  loss_mlm: 0.3713  loss_prd: 0.0431  loss_mrtd: 0.1884  time: 1.2926  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 450/2501]  eta: 0:46:44  lr: 0.000009  loss_cl: 9.6402  loss_pitm: 0.0918  loss_mlm: 0.6072  loss_prd: 0.8630  loss_mrtd: 0.2361  time: 1.3082  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 500/2501]  eta: 0:45:23  lr: 0.000009  loss_cl: 9.6254  loss_pitm: 0.0373  loss_mlm: 0.5614  loss_prd: 0.5144  loss_mrtd: 0.2428  time: 1.3092  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 550/2501]  eta: 0:44:04  lr: 0.000009  loss_cl: 9.6084  loss_pitm: 0.0839  loss_mlm: 0.5252  loss_prd: 0.0939  loss_mrtd: 0.1928  time: 1.3155  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 600/2501]  eta: 0:42:48  lr: 0.000009  loss_cl: 9.6062  loss_pitm: 0.0210  loss_mlm: 0.2799  loss_prd: 0.3143  loss_mrtd: 0.1519  time: 1.2941  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 650/2501]  eta: 0:41:33  lr: 0.000009  loss_cl: 9.7278  loss_pitm: 0.0824  loss_mlm: 0.3989  loss_prd: 0.1332  loss_mrtd: 0.2230  time: 1.3007  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 700/2501]  eta: 0:40:20  lr: 0.000009  loss_cl: 9.3598  loss_pitm: 0.0157  loss_mlm: 0.3965  loss_prd: 0.0826  loss_mrtd: 0.1711  time: 1.3033  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 750/2501]  eta: 0:39:07  lr: 0.000009  loss_cl: 9.5126  loss_pitm: 0.0757  loss_mlm: 0.6191  loss_prd: 0.0888  loss_mrtd: 0.3145  time: 1.2973  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 800/2501]  eta: 0:37:56  lr: 0.000009  loss_cl: 9.5261  loss_pitm: 0.0301  loss_mlm: 0.3155  loss_prd: 0.3155  loss_mrtd: 0.2016  time: 1.2999  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 850/2501]  eta: 0:36:45  lr: 0.000009  loss_cl: 9.6421  loss_pitm: 0.0927  loss_mlm: 0.3672  loss_prd: 0.2871  loss_mrtd: 0.1656  time: 1.3043  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 900/2501]  eta: 0:35:35  lr: 0.000009  loss_cl: 9.2236  loss_pitm: 0.0289  loss_mlm: 0.4031  loss_prd: 0.4418  loss_mrtd: 0.2698  time: 1.3004  data: 0.0001  max mem: 17758
Train Epoch: [7]  [ 950/2501]  eta: 0:34:26  lr: 0.000009  loss_cl: 9.1904  loss_pitm: 0.0274  loss_mlm: 0.3915  loss_prd: 0.2189  loss_mrtd: 0.2152  time: 1.3019  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1000/2501]  eta: 0:33:16  lr: 0.000009  loss_cl: 9.2057  loss_pitm: 0.0472  loss_mlm: 0.8656  loss_prd: 0.2213  loss_mrtd: 0.2498  time: 1.3019  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1050/2501]  eta: 0:32:08  lr: 0.000009  loss_cl: 9.5989  loss_pitm: 0.2829  loss_mlm: 0.3211  loss_prd: 0.4689  loss_mrtd: 0.1673  time: 1.2993  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1100/2501]  eta: 0:30:59  lr: 0.000009  loss_cl: 9.0038  loss_pitm: 0.0090  loss_mlm: 0.3446  loss_prd: 0.0701  loss_mrtd: 0.2420  time: 1.2950  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1150/2501]  eta: 0:29:51  lr: 0.000009  loss_cl: 8.8395  loss_pitm: 0.0081  loss_mlm: 0.3021  loss_prd: 0.0524  loss_mrtd: 0.2026  time: 1.2969  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1200/2501]  eta: 0:28:43  lr: 0.000009  loss_cl: 9.1325  loss_pitm: 0.0205  loss_mlm: 0.4891  loss_prd: 0.0944  loss_mrtd: 0.2540  time: 1.2961  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1250/2501]  eta: 0:27:36  lr: 0.000009  loss_cl: 8.7486  loss_pitm: 0.0250  loss_mlm: 0.4712  loss_prd: 0.4142  loss_mrtd: 0.2482  time: 1.3018  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1300/2501]  eta: 0:26:29  lr: 0.000009  loss_cl: 9.2820  loss_pitm: 0.0089  loss_mlm: 0.3747  loss_prd: 0.0700  loss_mrtd: 0.2183  time: 1.2972  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1350/2501]  eta: 0:25:22  lr: 0.000009  loss_cl: 9.1153  loss_pitm: 0.3414  loss_mlm: 0.5082  loss_prd: 0.2538  loss_mrtd: 0.2566  time: 1.3118  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1400/2501]  eta: 0:24:15  lr: 0.000009  loss_cl: 8.9148  loss_pitm: 0.0775  loss_mlm: 0.7901  loss_prd: 0.3987  loss_mrtd: 0.3728  time: 1.2948  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1450/2501]  eta: 0:23:08  lr: 0.000009  loss_cl: 8.7569  loss_pitm: 0.3513  loss_mlm: 0.3849  loss_prd: 0.2044  loss_mrtd: 0.2586  time: 1.3050  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1500/2501]  eta: 0:22:01  lr: 0.000009  loss_cl: 8.6211  loss_pitm: 0.0160  loss_mlm: 0.4486  loss_prd: 0.2851  loss_mrtd: 0.2261  time: 1.2989  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1550/2501]  eta: 0:20:54  lr: 0.000009  loss_cl: 9.0202  loss_pitm: 0.1387  loss_mlm: 0.4896  loss_prd: 0.2330  loss_mrtd: 0.2586  time: 1.3042  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1600/2501]  eta: 0:19:48  lr: 0.000009  loss_cl: 8.5137  loss_pitm: 0.0621  loss_mlm: 0.4407  loss_prd: 0.2229  loss_mrtd: 0.2629  time: 1.2924  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1650/2501]  eta: 0:18:41  lr: 0.000009  loss_cl: 8.3513  loss_pitm: 0.0318  loss_mlm: 0.4890  loss_prd: 0.1112  loss_mrtd: 0.1768  time: 1.2866  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1700/2501]  eta: 0:17:35  lr: 0.000009  loss_cl: 8.4223  loss_pitm: 0.0188  loss_mlm: 0.8626  loss_prd: 0.0864  loss_mrtd: 0.2439  time: 1.3198  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1750/2501]  eta: 0:16:29  lr: 0.000009  loss_cl: 8.2728  loss_pitm: 0.0303  loss_mlm: 0.4214  loss_prd: 0.5137  loss_mrtd: 0.2716  time: 1.3028  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1800/2501]  eta: 0:15:22  lr: 0.000009  loss_cl: 8.0127  loss_pitm: 0.0989  loss_mlm: 0.7196  loss_prd: 0.2552  loss_mrtd: 0.2859  time: 1.2928  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1850/2501]  eta: 0:14:16  lr: 0.000009  loss_cl: 8.0172  loss_pitm: 0.0575  loss_mlm: 0.5536  loss_prd: 0.4219  loss_mrtd: 0.3058  time: 1.2925  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1900/2501]  eta: 0:13:10  lr: 0.000009  loss_cl: 8.1378  loss_pitm: 0.0928  loss_mlm: 0.7285  loss_prd: 0.5300  loss_mrtd: 0.2409  time: 1.2932  data: 0.0001  max mem: 17758
Train Epoch: [7]  [1950/2501]  eta: 0:12:04  lr: 0.000009  loss_cl: 7.8300  loss_pitm: 0.0169  loss_mlm: 0.6270  loss_prd: 0.2404  loss_mrtd: 0.2346  time: 1.2991  data: 0.0001  max mem: 17758
Train Epoch: [7]  [2000/2501]  eta: 0:10:58  lr: 0.000009  loss_cl: 7.6014  loss_pitm: 0.0214  loss_mlm: 0.4660  loss_prd: 0.1854  loss_mrtd: 0.2080  time: 1.3079  data: 0.0001  max mem: 17758
Train Epoch: [7]  [2050/2501]  eta: 0:09:52  lr: 0.000009  loss_cl: 7.8966  loss_pitm: 0.1719  loss_mlm: 0.6604  loss_prd: 0.3045  loss_mrtd: 0.2277  time: 1.3030  data: 0.0001  max mem: 17758
Train Epoch: [7]  [2100/2501]  eta: 0:08:46  lr: 0.000009  loss_cl: 7.8463  loss_pitm: 0.0432  loss_mlm: 0.7890  loss_prd: 0.0917  loss_mrtd: 0.2720  time: 1.3052  data: 0.0001  max mem: 17758
Train Epoch: [7]  [2150/2501]  eta: 0:07:41  lr: 0.000009  loss_cl: 8.3310  loss_pitm: 0.0928  loss_mlm: 0.4111  loss_prd: 0.4064  loss_mrtd: 0.2392  time: 1.3155  data: 0.0001  max mem: 17758
Train Epoch: [7]  [2200/2501]  eta: 0:06:35  lr: 0.000009  loss_cl: 7.7699  loss_pitm: 0.0348  loss_mlm: 0.6106  loss_prd: 0.0584  loss_mrtd: 0.2640  time: 1.3018  data: 0.0001  max mem: 17758
Train Epoch: [7]  [2250/2501]  eta: 0:05:29  lr: 0.000009  loss_cl: 7.5455  loss_pitm: 0.0215  loss_mlm: 0.3808  loss_prd: 0.0801  loss_mrtd: 0.1853  time: 1.2950  data: 0.0002  max mem: 17758
Train Epoch: [7]  [2300/2501]  eta: 0:04:23  lr: 0.000009  loss_cl: 7.5597  loss_pitm: 0.0080  loss_mlm: 0.8490  loss_prd: 0.5266  loss_mrtd: 0.2917  time: 1.3042  data: 0.0002  max mem: 17758
Train Epoch: [7]  [2350/2501]  eta: 0:03:18  lr: 0.000009  loss_cl: 7.7472  loss_pitm: 0.0133  loss_mlm: 0.6794  loss_prd: 0.2915  loss_mrtd: 0.2636  time: 1.3021  data: 0.0001  max mem: 17758
Train Epoch: [7]  [2400/2501]  eta: 0:02:12  lr: 0.000009  loss_cl: 7.5350  loss_pitm: 0.0405  loss_mlm: 0.4756  loss_prd: 0.0691  loss_mrtd: 0.3117  time: 1.3054  data: 0.0001  max mem: 17758
Train Epoch: [7]  [2450/2501]  eta: 0:01:06  lr: 0.000009  loss_cl: 6.9375  loss_pitm: 0.2336  loss_mlm: 0.6696  loss_prd: 0.0779  loss_mrtd: 0.2240  time: 1.3006  data: 0.0001  max mem: 17758
Train Epoch: [7]  [2500/2501]  eta: 0:00:01  lr: 0.000009  loss_cl: 6.7905  loss_pitm: 0.0109  loss_mlm: 0.5671  loss_prd: 0.3095  loss_mrtd: 0.2113  time: 1.2991  data: 0.0002  max mem: 17758
Train Epoch: [7] Total time: 0:54:42 (1.3123 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.8478  loss_pitm: 0.0759  loss_mlm: 0.5081  loss_prd: 0.2466  loss_mrtd: 0.2447
Train Epoch的聚类: [8]  [   0/5241]  eta: 0:58:05    time: 0.6651  data: 0.6431  max mem: 17758
Train Epoch的聚类: [8]  [5240/5241]  eta: 0:00:00    time: 0.1714  data: 0.0002  max mem: 17758
Train Epoch的聚类: [8] Total time: 0:14:58 (0.1714 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4782
-1 (未归入任何簇) 的数量: 2912

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [8]  [   0/2508]  eta: 22:03:45  lr: 0.000009  loss_cl: 10.5849  loss_pitm: 0.0910  loss_mlm: 0.7424  loss_prd: 0.1172  loss_mrtd: 0.2629  time: 31.6688  data: 0.5511  max mem: 17758
Train Epoch: [8]  [  50/2508]  eta: 1:16:33  lr: 0.000009  loss_cl: 9.5999  loss_pitm: 0.2693  loss_mlm: 0.6001  loss_prd: 0.2959  loss_mrtd: 0.2935  time: 1.2833  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 100/2508]  eta: 1:03:23  lr: 0.000009  loss_cl: 9.9515  loss_pitm: 0.0283  loss_mlm: 0.6176  loss_prd: 0.0592  loss_mrtd: 0.3025  time: 1.2855  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 150/2508]  eta: 0:58:17  lr: 0.000009  loss_cl: 9.5583  loss_pitm: 0.0257  loss_mlm: 0.4787  loss_prd: 0.0854  loss_mrtd: 0.2393  time: 1.2869  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 200/2508]  eta: 0:55:15  lr: 0.000009  loss_cl: 9.6285  loss_pitm: 0.0539  loss_mlm: 0.1755  loss_prd: 0.3010  loss_mrtd: 0.2632  time: 1.2908  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 250/2508]  eta: 0:53:05  lr: 0.000009  loss_cl: 9.7515  loss_pitm: 0.0075  loss_mlm: 0.5217  loss_prd: 0.0747  loss_mrtd: 0.2888  time: 1.3050  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 300/2508]  eta: 0:51:12  lr: 0.000009  loss_cl: 9.4509  loss_pitm: 0.0932  loss_mlm: 0.6712  loss_prd: 0.0638  loss_mrtd: 0.1757  time: 1.2984  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 350/2508]  eta: 0:49:38  lr: 0.000009  loss_cl: 9.8257  loss_pitm: 0.2115  loss_mlm: 0.6825  loss_prd: 0.1343  loss_mrtd: 0.2909  time: 1.3006  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 400/2508]  eta: 0:48:08  lr: 0.000009  loss_cl: 9.5863  loss_pitm: 0.0111  loss_mlm: 0.3383  loss_prd: 0.2890  loss_mrtd: 0.2253  time: 1.2991  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 450/2508]  eta: 0:46:43  lr: 0.000009  loss_cl: 9.4264  loss_pitm: 0.0147  loss_mlm: 0.4099  loss_prd: 0.2962  loss_mrtd: 0.2885  time: 1.2935  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 500/2508]  eta: 0:45:21  lr: 0.000009  loss_cl: 9.4940  loss_pitm: 0.0442  loss_mlm: 0.6155  loss_prd: 0.1084  loss_mrtd: 0.2872  time: 1.2877  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 550/2508]  eta: 0:44:02  lr: 0.000009  loss_cl: 9.4647  loss_pitm: 0.0149  loss_mlm: 0.2260  loss_prd: 0.0563  loss_mrtd: 0.2364  time: 1.2979  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 600/2508]  eta: 0:42:47  lr: 0.000009  loss_cl: 9.2671  loss_pitm: 0.0191  loss_mlm: 0.9003  loss_prd: 0.0851  loss_mrtd: 0.3286  time: 1.3039  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 650/2508]  eta: 0:41:33  lr: 0.000009  loss_cl: 9.1460  loss_pitm: 0.0245  loss_mlm: 0.1608  loss_prd: 0.0648  loss_mrtd: 0.2125  time: 1.2937  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 700/2508]  eta: 0:40:20  lr: 0.000009  loss_cl: 9.4312  loss_pitm: 0.0103  loss_mlm: 0.4693  loss_prd: 0.0659  loss_mrtd: 0.1901  time: 1.2944  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 750/2508]  eta: 0:39:08  lr: 0.000009  loss_cl: 9.3792  loss_pitm: 0.0268  loss_mlm: 0.5178  loss_prd: 0.6758  loss_mrtd: 0.2096  time: 1.2892  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 800/2508]  eta: 0:37:57  lr: 0.000009  loss_cl: 9.1264  loss_pitm: 0.0246  loss_mlm: 0.4849  loss_prd: 0.2690  loss_mrtd: 0.2568  time: 1.2981  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 850/2508]  eta: 0:36:48  lr: 0.000009  loss_cl: 9.4328  loss_pitm: 0.0210  loss_mlm: 0.5337  loss_prd: 0.0831  loss_mrtd: 0.2450  time: 1.3099  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 900/2508]  eta: 0:35:38  lr: 0.000009  loss_cl: 9.4379  loss_pitm: 0.1923  loss_mlm: 0.3915  loss_prd: 0.4447  loss_mrtd: 0.2874  time: 1.2945  data: 0.0001  max mem: 17758
Train Epoch: [8]  [ 950/2508]  eta: 0:34:29  lr: 0.000009  loss_cl: 9.2510  loss_pitm: 0.0285  loss_mlm: 0.6403  loss_prd: 0.0651  loss_mrtd: 0.2604  time: 1.3016  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1000/2508]  eta: 0:33:21  lr: 0.000009  loss_cl: 9.4303  loss_pitm: 0.0174  loss_mlm: 0.3303  loss_prd: 0.0556  loss_mrtd: 0.1953  time: 1.3023  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1050/2508]  eta: 0:32:13  lr: 0.000009  loss_cl: 9.2145  loss_pitm: 0.0793  loss_mlm: 0.4098  loss_prd: 0.0568  loss_mrtd: 0.2301  time: 1.2939  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1100/2508]  eta: 0:31:05  lr: 0.000009  loss_cl: 9.3990  loss_pitm: 0.1544  loss_mlm: 0.2172  loss_prd: 0.2312  loss_mrtd: 0.2001  time: 1.2930  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1150/2508]  eta: 0:29:57  lr: 0.000009  loss_cl: 9.3805  loss_pitm: 0.0341  loss_mlm: 0.3690  loss_prd: 0.5083  loss_mrtd: 0.2106  time: 1.2961  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1200/2508]  eta: 0:28:49  lr: 0.000009  loss_cl: 8.7429  loss_pitm: 0.0613  loss_mlm: 0.3160  loss_prd: 0.1984  loss_mrtd: 0.2307  time: 1.2929  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1250/2508]  eta: 0:27:42  lr: 0.000009  loss_cl: 9.1782  loss_pitm: 0.0089  loss_mlm: 0.4732  loss_prd: 0.2773  loss_mrtd: 0.2486  time: 1.3074  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1300/2508]  eta: 0:26:35  lr: 0.000009  loss_cl: 8.7445  loss_pitm: 0.0652  loss_mlm: 0.2809  loss_prd: 0.4175  loss_mrtd: 0.2176  time: 1.3078  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1350/2508]  eta: 0:25:28  lr: 0.000009  loss_cl: 8.6699  loss_pitm: 0.0259  loss_mlm: 0.8298  loss_prd: 0.0780  loss_mrtd: 0.2507  time: 1.3018  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1400/2508]  eta: 0:24:21  lr: 0.000009  loss_cl: 9.4925  loss_pitm: 0.1214  loss_mlm: 0.6688  loss_prd: 0.0898  loss_mrtd: 0.2183  time: 1.2957  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1450/2508]  eta: 0:23:15  lr: 0.000009  loss_cl: 8.5665  loss_pitm: 0.0038  loss_mlm: 0.3481  loss_prd: 0.0539  loss_mrtd: 0.3060  time: 1.3142  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1500/2508]  eta: 0:22:08  lr: 0.000009  loss_cl: 8.3910  loss_pitm: 0.0168  loss_mlm: 0.6344  loss_prd: 0.0666  loss_mrtd: 0.2936  time: 1.3059  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1550/2508]  eta: 0:21:02  lr: 0.000009  loss_cl: 8.9147  loss_pitm: 0.3309  loss_mlm: 0.5557  loss_prd: 0.2548  loss_mrtd: 0.2170  time: 1.3154  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1600/2508]  eta: 0:19:56  lr: 0.000009  loss_cl: 8.7989  loss_pitm: 0.0143  loss_mlm: 0.9357  loss_prd: 0.0554  loss_mrtd: 0.2623  time: 1.3083  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1650/2508]  eta: 0:18:50  lr: 0.000009  loss_cl: 8.4438  loss_pitm: 0.0331  loss_mlm: 0.2922  loss_prd: 0.3428  loss_mrtd: 0.2260  time: 1.3050  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1700/2508]  eta: 0:17:43  lr: 0.000009  loss_cl: 8.2361  loss_pitm: 0.0220  loss_mlm: 0.3184  loss_prd: 0.0658  loss_mrtd: 0.2565  time: 1.3067  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1750/2508]  eta: 0:16:37  lr: 0.000009  loss_cl: 8.5767  loss_pitm: 0.0540  loss_mlm: 0.3619  loss_prd: 0.4944  loss_mrtd: 0.2044  time: 1.2958  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1800/2508]  eta: 0:15:31  lr: 0.000009  loss_cl: 8.2523  loss_pitm: 0.0177  loss_mlm: 0.5105  loss_prd: 0.0646  loss_mrtd: 0.2846  time: 1.3115  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1850/2508]  eta: 0:14:25  lr: 0.000009  loss_cl: 8.1592  loss_pitm: 0.1214  loss_mlm: 0.6829  loss_prd: 0.0665  loss_mrtd: 0.2098  time: 1.3116  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1900/2508]  eta: 0:13:19  lr: 0.000009  loss_cl: 7.7834  loss_pitm: 0.0539  loss_mlm: 0.5283  loss_prd: 0.0552  loss_mrtd: 0.2604  time: 1.3075  data: 0.0001  max mem: 17758
Train Epoch: [8]  [1950/2508]  eta: 0:12:13  lr: 0.000009  loss_cl: 8.0489  loss_pitm: 0.0218  loss_mlm: 0.4089  loss_prd: 0.0529  loss_mrtd: 0.1722  time: 1.2949  data: 0.0001  max mem: 17758
Train Epoch: [8]  [2000/2508]  eta: 0:11:07  lr: 0.000009  loss_cl: 8.1863  loss_pitm: 0.0100  loss_mlm: 0.5813  loss_prd: 0.0754  loss_mrtd: 0.1430  time: 1.3027  data: 0.0001  max mem: 17758
Train Epoch: [8]  [2050/2508]  eta: 0:10:02  lr: 0.000009  loss_cl: 7.4373  loss_pitm: 0.0118  loss_mlm: 0.3693  loss_prd: 0.2099  loss_mrtd: 0.3188  time: 1.3078  data: 0.0001  max mem: 17758
Train Epoch: [8]  [2100/2508]  eta: 0:08:56  lr: 0.000009  loss_cl: 7.9973  loss_pitm: 0.1228  loss_mlm: 0.4977  loss_prd: 0.0779  loss_mrtd: 0.2565  time: 1.2882  data: 0.0001  max mem: 17758
Train Epoch: [8]  [2150/2508]  eta: 0:07:50  lr: 0.000009  loss_cl: 7.6955  loss_pitm: 0.0268  loss_mlm: 0.2805  loss_prd: 0.0584  loss_mrtd: 0.2043  time: 1.2976  data: 0.0001  max mem: 17758
Train Epoch: [8]  [2200/2508]  eta: 0:06:44  lr: 0.000009  loss_cl: 7.3122  loss_pitm: 0.0601  loss_mlm: 0.9447  loss_prd: 0.2817  loss_mrtd: 0.2611  time: 1.2947  data: 0.0001  max mem: 17758
Train Epoch: [8]  [2250/2508]  eta: 0:05:38  lr: 0.000009  loss_cl: 7.6301  loss_pitm: 0.0377  loss_mlm: 0.4689  loss_prd: 0.0753  loss_mrtd: 0.2036  time: 1.3021  data: 0.0001  max mem: 17758
Train Epoch: [8]  [2300/2508]  eta: 0:04:33  lr: 0.000009  loss_cl: 7.4447  loss_pitm: 0.1177  loss_mlm: 0.7268  loss_prd: 0.4884  loss_mrtd: 0.3066  time: 1.3036  data: 0.0001  max mem: 17758
Train Epoch: [8]  [2350/2508]  eta: 0:03:27  lr: 0.000009  loss_cl: 7.4216  loss_pitm: 0.0160  loss_mlm: 0.4875  loss_prd: 0.0670  loss_mrtd: 0.2266  time: 1.2839  data: 0.0001  max mem: 17758
Train Epoch: [8]  [2400/2508]  eta: 0:02:21  lr: 0.000009  loss_cl: 7.5675  loss_pitm: 0.0104  loss_mlm: 0.3758  loss_prd: 0.0616  loss_mrtd: 0.2308  time: 1.3000  data: 0.0001  max mem: 17758
Train Epoch: [8]  [2450/2508]  eta: 0:01:16  lr: 0.000009  loss_cl: 7.7032  loss_pitm: 0.0603  loss_mlm: 0.3245  loss_prd: 0.6076  loss_mrtd: 0.1924  time: 1.3026  data: 0.0001  max mem: 17758
Train Epoch: [8]  [2500/2508]  eta: 0:00:10  lr: 0.000009  loss_cl: 7.1856  loss_pitm: 0.1016  loss_mlm: 0.4929  loss_prd: 0.0884  loss_mrtd: 0.2177  time: 1.3034  data: 0.0002  max mem: 17758
Train Epoch: [8]  [2507/2508]  eta: 0:00:01  lr: 0.000009  loss_cl: 7.1447  loss_pitm: 0.1081  loss_mlm: 0.7313  loss_prd: 0.3128  loss_mrtd: 0.1812  time: 1.2957  data: 0.0002  max mem: 17758
Train Epoch: [8] Total time: 0:54:50 (1.3118 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.7822  loss_pitm: 0.0716  loss_mlm: 0.5002  loss_prd: 0.2412  loss_mrtd: 0.2419
Train Epoch的聚类: [9]  [   0/5241]  eta: 0:53:59    time: 0.6181  data: 0.5944  max mem: 17758
Train Epoch的聚类: [9]  [5240/5241]  eta: 0:00:00    time: 0.1716  data: 0.0002  max mem: 17758
Train Epoch的聚类: [9] Total time: 0:14:59 (0.1716 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4735
-1 (未归入任何簇) 的数量: 2676

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [9]  [   0/2517]  eta: 18:13:18  lr: 0.000008  loss_cl: 9.8856  loss_pitm: 0.4079  loss_mlm: 0.7763  loss_prd: 0.1631  loss_mrtd: 0.1837  time: 26.0621  data: 0.6401  max mem: 17758
Train Epoch: [9]  [  50/2517]  eta: 1:12:04  lr: 0.000008  loss_cl: 9.5363  loss_pitm: 0.0132  loss_mlm: 0.3471  loss_prd: 0.2760  loss_mrtd: 0.2025  time: 1.2709  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 100/2517]  eta: 1:01:21  lr: 0.000008  loss_cl: 9.3976  loss_pitm: 0.0176  loss_mlm: 0.6243  loss_prd: 0.2506  loss_mrtd: 0.2970  time: 1.2937  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 150/2517]  eta: 0:57:16  lr: 0.000008  loss_cl: 9.7447  loss_pitm: 0.0149  loss_mlm: 0.4313  loss_prd: 0.1888  loss_mrtd: 0.2849  time: 1.2976  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 200/2517]  eta: 0:54:35  lr: 0.000008  loss_cl: 9.4383  loss_pitm: 0.0710  loss_mlm: 0.3627  loss_prd: 0.1402  loss_mrtd: 0.2170  time: 1.2913  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 250/2517]  eta: 0:52:34  lr: 0.000008  loss_cl: 9.4153  loss_pitm: 0.0470  loss_mlm: 0.3522  loss_prd: 0.3217  loss_mrtd: 0.2834  time: 1.2982  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 300/2517]  eta: 0:50:49  lr: 0.000008  loss_cl: 9.5582  loss_pitm: 0.1715  loss_mlm: 0.5824  loss_prd: 0.4847  loss_mrtd: 0.2847  time: 1.2996  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 350/2517]  eta: 0:49:18  lr: 0.000008  loss_cl: 9.4218  loss_pitm: 0.0312  loss_mlm: 0.4267  loss_prd: 0.0913  loss_mrtd: 0.1673  time: 1.2972  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 400/2517]  eta: 0:47:53  lr: 0.000008  loss_cl: 9.4239  loss_pitm: 0.0095  loss_mlm: 0.8674  loss_prd: 0.3605  loss_mrtd: 0.2144  time: 1.3001  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 450/2517]  eta: 0:46:32  lr: 0.000008  loss_cl: 9.4932  loss_pitm: 0.3412  loss_mlm: 0.3570  loss_prd: 0.0416  loss_mrtd: 0.2275  time: 1.3091  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 500/2517]  eta: 0:45:13  lr: 0.000008  loss_cl: 9.7326  loss_pitm: 0.0571  loss_mlm: 0.5882  loss_prd: 0.4916  loss_mrtd: 0.2031  time: 1.2909  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 550/2517]  eta: 0:43:56  lr: 0.000008  loss_cl: 9.4302  loss_pitm: 0.0581  loss_mlm: 0.4087  loss_prd: 0.2075  loss_mrtd: 0.1756  time: 1.2968  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 600/2517]  eta: 0:42:41  lr: 0.000008  loss_cl: 9.7094  loss_pitm: 0.0482  loss_mlm: 0.1430  loss_prd: 0.0752  loss_mrtd: 0.1875  time: 1.2953  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 650/2517]  eta: 0:41:29  lr: 0.000008  loss_cl: 9.6582  loss_pitm: 0.0594  loss_mlm: 0.3681  loss_prd: 0.3081  loss_mrtd: 0.2242  time: 1.2998  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 700/2517]  eta: 0:40:18  lr: 0.000008  loss_cl: 9.2229  loss_pitm: 0.1745  loss_mlm: 0.8175  loss_prd: 0.1236  loss_mrtd: 0.2538  time: 1.2978  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 750/2517]  eta: 0:39:08  lr: 0.000008  loss_cl: 9.1774  loss_pitm: 0.0157  loss_mlm: 0.2859  loss_prd: 0.2854  loss_mrtd: 0.2089  time: 1.3041  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 800/2517]  eta: 0:37:58  lr: 0.000008  loss_cl: 9.4687  loss_pitm: 0.0127  loss_mlm: 0.3662  loss_prd: 0.3213  loss_mrtd: 0.2228  time: 1.2921  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 850/2517]  eta: 0:36:49  lr: 0.000008  loss_cl: 9.3343  loss_pitm: 0.0395  loss_mlm: 0.2644  loss_prd: 0.0445  loss_mrtd: 0.1761  time: 1.2990  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 900/2517]  eta: 0:35:39  lr: 0.000008  loss_cl: 9.2162  loss_pitm: 0.1437  loss_mlm: 0.4526  loss_prd: 0.4012  loss_mrtd: 0.2243  time: 1.2863  data: 0.0001  max mem: 17758
Train Epoch: [9]  [ 950/2517]  eta: 0:34:31  lr: 0.000008  loss_cl: 9.1054  loss_pitm: 0.0493  loss_mlm: 0.5446  loss_prd: 0.0633  loss_mrtd: 0.2209  time: 1.3010  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1000/2517]  eta: 0:33:23  lr: 0.000008  loss_cl: 8.9147  loss_pitm: 0.1251  loss_mlm: 0.6570  loss_prd: 0.3239  loss_mrtd: 0.2773  time: 1.2933  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1050/2517]  eta: 0:32:15  lr: 0.000008  loss_cl: 9.1757  loss_pitm: 0.0912  loss_mlm: 0.3522  loss_prd: 0.1080  loss_mrtd: 0.2014  time: 1.2967  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1100/2517]  eta: 0:31:07  lr: 0.000008  loss_cl: 9.1915  loss_pitm: 0.0272  loss_mlm: 0.6357  loss_prd: 0.0523  loss_mrtd: 0.2872  time: 1.2887  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1150/2517]  eta: 0:30:00  lr: 0.000008  loss_cl: 8.9528  loss_pitm: 0.0154  loss_mlm: 0.3590  loss_prd: 0.0508  loss_mrtd: 0.2256  time: 1.2889  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1200/2517]  eta: 0:28:53  lr: 0.000008  loss_cl: 9.3797  loss_pitm: 0.0702  loss_mlm: 0.4288  loss_prd: 0.0417  loss_mrtd: 0.1868  time: 1.2983  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1250/2517]  eta: 0:27:47  lr: 0.000008  loss_cl: 8.7590  loss_pitm: 0.0067  loss_mlm: 0.4891  loss_prd: 0.2806  loss_mrtd: 0.3299  time: 1.3097  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1300/2517]  eta: 0:26:40  lr: 0.000008  loss_cl: 8.7337  loss_pitm: 0.0720  loss_mlm: 0.6303  loss_prd: 0.0532  loss_mrtd: 0.2555  time: 1.3009  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1350/2517]  eta: 0:25:34  lr: 0.000008  loss_cl: 8.7202  loss_pitm: 0.0197  loss_mlm: 0.7329  loss_prd: 0.2579  loss_mrtd: 0.2454  time: 1.2941  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1400/2517]  eta: 0:24:27  lr: 0.000008  loss_cl: 8.5372  loss_pitm: 0.0040  loss_mlm: 0.3440  loss_prd: 0.0537  loss_mrtd: 0.2809  time: 1.2957  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1450/2517]  eta: 0:23:21  lr: 0.000008  loss_cl: 8.6403  loss_pitm: 0.0078  loss_mlm: 0.4418  loss_prd: 0.0485  loss_mrtd: 0.2020  time: 1.2948  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1500/2517]  eta: 0:22:15  lr: 0.000008  loss_cl: 8.5087  loss_pitm: 0.0308  loss_mlm: 0.1935  loss_prd: 0.2219  loss_mrtd: 0.1898  time: 1.2934  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1550/2517]  eta: 0:21:09  lr: 0.000008  loss_cl: 8.5841  loss_pitm: 0.0079  loss_mlm: 0.7350  loss_prd: 0.4741  loss_mrtd: 0.2485  time: 1.2958  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1600/2517]  eta: 0:20:02  lr: 0.000008  loss_cl: 8.3015  loss_pitm: 0.0611  loss_mlm: 0.6033  loss_prd: 0.0488  loss_mrtd: 0.2497  time: 1.3012  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1650/2517]  eta: 0:18:56  lr: 0.000008  loss_cl: 8.7604  loss_pitm: 0.0280  loss_mlm: 0.5340  loss_prd: 0.2567  loss_mrtd: 0.2450  time: 1.2946  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1700/2517]  eta: 0:17:51  lr: 0.000008  loss_cl: 8.3902  loss_pitm: 0.0658  loss_mlm: 0.2891  loss_prd: 0.2575  loss_mrtd: 0.2172  time: 1.2996  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1750/2517]  eta: 0:16:45  lr: 0.000008  loss_cl: 8.6167  loss_pitm: 0.0289  loss_mlm: 0.4681  loss_prd: 0.0864  loss_mrtd: 0.3070  time: 1.3009  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1800/2517]  eta: 0:15:39  lr: 0.000008  loss_cl: 8.1000  loss_pitm: 0.0270  loss_mlm: 0.3048  loss_prd: 0.0442  loss_mrtd: 0.1850  time: 1.2973  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1850/2517]  eta: 0:14:33  lr: 0.000008  loss_cl: 8.0006  loss_pitm: 0.1436  loss_mlm: 0.3156  loss_prd: 0.0606  loss_mrtd: 0.2553  time: 1.2883  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1900/2517]  eta: 0:13:27  lr: 0.000008  loss_cl: 8.4018  loss_pitm: 0.0308  loss_mlm: 0.3386  loss_prd: 0.2660  loss_mrtd: 0.1997  time: 1.2939  data: 0.0001  max mem: 17758
Train Epoch: [9]  [1950/2517]  eta: 0:12:22  lr: 0.000008  loss_cl: 8.2928  loss_pitm: 0.0137  loss_mlm: 0.3990  loss_prd: 0.2725  loss_mrtd: 0.2641  time: 1.3060  data: 0.0001  max mem: 17758
Train Epoch: [9]  [2000/2517]  eta: 0:11:16  lr: 0.000008  loss_cl: 7.8141  loss_pitm: 0.0049  loss_mlm: 0.9241  loss_prd: 0.2171  loss_mrtd: 0.3466  time: 1.3001  data: 0.0001  max mem: 17758
Train Epoch: [9]  [2050/2517]  eta: 0:10:11  lr: 0.000008  loss_cl: 8.2307  loss_pitm: 0.0561  loss_mlm: 0.6656  loss_prd: 0.2867  loss_mrtd: 0.2076  time: 1.2925  data: 0.0001  max mem: 17758
Train Epoch: [9]  [2100/2517]  eta: 0:09:05  lr: 0.000008  loss_cl: 8.0412  loss_pitm: 0.0829  loss_mlm: 0.3755  loss_prd: 0.2884  loss_mrtd: 0.2559  time: 1.2906  data: 0.0001  max mem: 17758
Train Epoch: [9]  [2150/2517]  eta: 0:08:00  lr: 0.000008  loss_cl: 7.8982  loss_pitm: 0.1336  loss_mlm: 0.4826  loss_prd: 0.2663  loss_mrtd: 0.2243  time: 1.2973  data: 0.0001  max mem: 17758
Train Epoch: [9]  [2200/2517]  eta: 0:06:54  lr: 0.000008  loss_cl: 7.5516  loss_pitm: 0.0127  loss_mlm: 0.6555  loss_prd: 0.0888  loss_mrtd: 0.2139  time: 1.2997  data: 0.0001  max mem: 17758
Train Epoch: [9]  [2250/2517]  eta: 0:05:49  lr: 0.000008  loss_cl: 7.6851  loss_pitm: 0.1971  loss_mlm: 0.8993  loss_prd: 0.2196  loss_mrtd: 0.2896  time: 1.2940  data: 0.0001  max mem: 17758
Train Epoch: [9]  [2300/2517]  eta: 0:04:43  lr: 0.000008  loss_cl: 7.7909  loss_pitm: 0.2497  loss_mlm: 0.5748  loss_prd: 0.0661  loss_mrtd: 0.2879  time: 1.2878  data: 0.0001  max mem: 17758
Train Epoch: [9]  [2350/2517]  eta: 0:03:38  lr: 0.000008  loss_cl: 7.1458  loss_pitm: 0.0935  loss_mlm: 0.4272  loss_prd: 0.1399  loss_mrtd: 0.2062  time: 1.2964  data: 0.0001  max mem: 17758
Train Epoch: [9]  [2400/2517]  eta: 0:02:32  lr: 0.000008  loss_cl: 7.2008  loss_pitm: 0.0163  loss_mlm: 0.2764  loss_prd: 0.2791  loss_mrtd: 0.2223  time: 1.3061  data: 0.0001  max mem: 17758
Train Epoch: [9]  [2450/2517]  eta: 0:01:27  lr: 0.000008  loss_cl: 7.1753  loss_pitm: 0.1522  loss_mlm: 0.5677  loss_prd: 0.0536  loss_mrtd: 0.3077  time: 1.3112  data: 0.0001  max mem: 17758
Train Epoch: [9]  [2500/2517]  eta: 0:00:22  lr: 0.000008  loss_cl: 7.0107  loss_pitm: 0.0299  loss_mlm: 0.5065  loss_prd: 0.6236  loss_mrtd: 0.2515  time: 1.3062  data: 0.0001  max mem: 17758
Train Epoch: [9]  [2516/2517]  eta: 0:00:01  lr: 0.000008  loss_cl: 7.0822  loss_pitm: 0.1745  loss_mlm: 0.1249  loss_prd: 0.2780  loss_mrtd: 0.1972  time: 1.2931  data: 0.0002  max mem: 17758
Train Epoch: [9] Total time: 0:54:48 (1.3067 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.7617  loss_pitm: 0.0673  loss_mlm: 0.4882  loss_prd: 0.2377  loss_mrtd: 0.2391
Train Epoch的聚类: [10]  [   0/5241]  eta: 0:55:39    time: 0.6373  data: 0.6132  max mem: 17758
Train Epoch的聚类: [10]  [5240/5241]  eta: 0:00:00    time: 0.1716  data: 0.0002  max mem: 17758
Train Epoch的聚类: [10] Total time: 0:14:59 (0.1716 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4662
-1 (未归入任何簇) 的数量: 2620

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [10]  [   0/2519]  eta: 1 day, 1:33:03  lr: 0.000008  loss_cl: 10.6192  loss_pitm: 0.0037  loss_mlm: 0.6476  loss_prd: 0.0540  loss_mrtd: 0.2661  time: 36.5157  data: 0.6129  max mem: 17758
Train Epoch: [10]  [  50/2519]  eta: 1:20:40  lr: 0.000008  loss_cl: 9.5703  loss_pitm: 0.0109  loss_mlm: 0.7883  loss_prd: 0.4406  loss_mrtd: 0.2907  time: 1.2785  data: 0.0001  max mem: 17758
Train Epoch: [10]  [ 100/2519]  eta: 1:05:49  lr: 0.000008  loss_cl: 9.5152  loss_pitm: 0.0161  loss_mlm: 0.4188  loss_prd: 0.1269  loss_mrtd: 0.2812  time: 1.2918  data: 0.0001  max mem: 17758
Train Epoch: [10]  [ 150/2519]  eta: 1:00:06  lr: 0.000008  loss_cl: 9.6679  loss_pitm: 0.0184  loss_mlm: 0.4888  loss_prd: 0.0595  loss_mrtd: 0.2553  time: 1.3021  data: 0.0001  max mem: 17758
Train Epoch: [10]  [ 200/2519]  eta: 0:56:39  lr: 0.000008  loss_cl: 9.5275  loss_pitm: 0.1336  loss_mlm: 0.2936  loss_prd: 0.0675  loss_mrtd: 0.2211  time: 1.2942  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 250/2519]  eta: 0:54:09  lr: 0.000008  loss_cl: 9.4932  loss_pitm: 0.0179  loss_mlm: 0.4709  loss_prd: 0.2737  loss_mrtd: 0.2738  time: 1.2964  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 300/2519]  eta: 0:52:06  lr: 0.000008  loss_cl: 9.5316  loss_pitm: 0.1292  loss_mlm: 0.5298  loss_prd: 0.0428  loss_mrtd: 0.2384  time: 1.2927  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 350/2519]  eta: 0:50:19  lr: 0.000008  loss_cl: 9.6731  loss_pitm: 0.1686  loss_mlm: 0.3919  loss_prd: 0.0408  loss_mrtd: 0.2149  time: 1.2894  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 400/2519]  eta: 0:48:44  lr: 0.000008  loss_cl: 9.5515  loss_pitm: 0.0115  loss_mlm: 0.3816  loss_prd: 0.2801  loss_mrtd: 0.2434  time: 1.2973  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 450/2519]  eta: 0:47:16  lr: 0.000008  loss_cl: 9.1523  loss_pitm: 0.0318  loss_mlm: 0.7848  loss_prd: 0.0494  loss_mrtd: 0.2966  time: 1.3065  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 500/2519]  eta: 0:45:53  lr: 0.000008  loss_cl: 9.4571  loss_pitm: 0.0341  loss_mlm: 0.3252  loss_prd: 0.3041  loss_mrtd: 0.2535  time: 1.2963  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 550/2519]  eta: 0:44:33  lr: 0.000008  loss_cl: 9.1523  loss_pitm: 0.1437  loss_mlm: 0.4984  loss_prd: 0.1035  loss_mrtd: 0.2474  time: 1.2937  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 600/2519]  eta: 0:43:15  lr: 0.000008  loss_cl: 9.6034  loss_pitm: 0.1519  loss_mlm: 0.3631  loss_prd: 0.2407  loss_mrtd: 0.1631  time: 1.2841  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 650/2519]  eta: 0:42:00  lr: 0.000008  loss_cl: 9.5531  loss_pitm: 0.0076  loss_mlm: 0.9389  loss_prd: 0.2686  loss_mrtd: 0.1919  time: 1.3033  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 700/2519]  eta: 0:40:46  lr: 0.000008  loss_cl: 9.1512  loss_pitm: 0.0879  loss_mlm: 0.5834  loss_prd: 0.4120  loss_mrtd: 0.3067  time: 1.2888  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 750/2519]  eta: 0:39:34  lr: 0.000008  loss_cl: 9.2140  loss_pitm: 0.0360  loss_mlm: 0.2931  loss_prd: 0.2771  loss_mrtd: 0.3111  time: 1.3008  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 800/2519]  eta: 0:38:23  lr: 0.000008  loss_cl: 9.4255  loss_pitm: 0.1193  loss_mlm: 0.2791  loss_prd: 0.0617  loss_mrtd: 0.1669  time: 1.2983  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 850/2519]  eta: 0:37:12  lr: 0.000008  loss_cl: 8.9374  loss_pitm: 0.1557  loss_mlm: 0.2712  loss_prd: 0.0780  loss_mrtd: 0.2037  time: 1.2982  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 900/2519]  eta: 0:36:02  lr: 0.000008  loss_cl: 9.1384  loss_pitm: 0.0297  loss_mlm: 0.4154  loss_prd: 0.4056  loss_mrtd: 0.2362  time: 1.3033  data: 0.0001  max mem: 17763
Train Epoch: [10]  [ 950/2519]  eta: 0:34:53  lr: 0.000008  loss_cl: 8.9652  loss_pitm: 0.0124  loss_mlm: 0.3010  loss_prd: 0.2887  loss_mrtd: 0.1644  time: 1.2963  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1000/2519]  eta: 0:33:44  lr: 0.000008  loss_cl: 9.3451  loss_pitm: 0.0328  loss_mlm: 0.5024  loss_prd: 0.0813  loss_mrtd: 0.2020  time: 1.3225  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1050/2519]  eta: 0:32:35  lr: 0.000008  loss_cl: 8.9697  loss_pitm: 0.0296  loss_mlm: 0.6748  loss_prd: 0.0842  loss_mrtd: 0.2016  time: 1.2918  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1100/2519]  eta: 0:31:26  lr: 0.000008  loss_cl: 8.9177  loss_pitm: 0.0106  loss_mlm: 0.7448  loss_prd: 0.0566  loss_mrtd: 0.2585  time: 1.3012  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1150/2519]  eta: 0:30:18  lr: 0.000008  loss_cl: 8.8953  loss_pitm: 0.0717  loss_mlm: 0.3793  loss_prd: 0.3791  loss_mrtd: 0.2230  time: 1.2972  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1200/2519]  eta: 0:29:10  lr: 0.000008  loss_cl: 8.6213  loss_pitm: 0.2204  loss_mlm: 0.4212  loss_prd: 0.2300  loss_mrtd: 0.1645  time: 1.3014  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1250/2519]  eta: 0:28:02  lr: 0.000008  loss_cl: 8.9761  loss_pitm: 0.0178  loss_mlm: 0.4925  loss_prd: 0.2120  loss_mrtd: 0.2382  time: 1.3093  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1300/2519]  eta: 0:26:55  lr: 0.000008  loss_cl: 8.7027  loss_pitm: 0.0129  loss_mlm: 0.3823  loss_prd: 0.2633  loss_mrtd: 0.2407  time: 1.2944  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1350/2519]  eta: 0:25:48  lr: 0.000008  loss_cl: 8.9630  loss_pitm: 0.1200  loss_mlm: 0.7239  loss_prd: 0.0499  loss_mrtd: 0.2459  time: 1.3075  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1400/2519]  eta: 0:24:40  lr: 0.000008  loss_cl: 8.5662  loss_pitm: 0.0219  loss_mlm: 0.5458  loss_prd: 0.6464  loss_mrtd: 0.2439  time: 1.2894  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1450/2519]  eta: 0:23:33  lr: 0.000008  loss_cl: 8.2974  loss_pitm: 0.1711  loss_mlm: 0.4586  loss_prd: 0.2890  loss_mrtd: 0.2540  time: 1.2853  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1500/2519]  eta: 0:22:26  lr: 0.000008  loss_cl: 8.7062  loss_pitm: 0.0232  loss_mlm: 0.4739  loss_prd: 0.0724  loss_mrtd: 0.1921  time: 1.2918  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1550/2519]  eta: 0:21:19  lr: 0.000008  loss_cl: 8.8478  loss_pitm: 0.0172  loss_mlm: 0.7296  loss_prd: 0.0773  loss_mrtd: 0.2542  time: 1.2969  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1600/2519]  eta: 0:20:13  lr: 0.000008  loss_cl: 8.6955  loss_pitm: 0.3636  loss_mlm: 0.5877  loss_prd: 0.0796  loss_mrtd: 0.2195  time: 1.2957  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1650/2519]  eta: 0:19:06  lr: 0.000008  loss_cl: 8.6898  loss_pitm: 0.0917  loss_mlm: 0.3115  loss_prd: 0.1060  loss_mrtd: 0.2434  time: 1.2962  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1700/2519]  eta: 0:18:00  lr: 0.000008  loss_cl: 8.4609  loss_pitm: 0.0070  loss_mlm: 0.5452  loss_prd: 0.0568  loss_mrtd: 0.3134  time: 1.2952  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1750/2519]  eta: 0:16:53  lr: 0.000008  loss_cl: 8.6224  loss_pitm: 0.0198  loss_mlm: 0.5254  loss_prd: 0.0600  loss_mrtd: 0.2071  time: 1.2948  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1800/2519]  eta: 0:15:47  lr: 0.000008  loss_cl: 8.1951  loss_pitm: 0.0288  loss_mlm: 0.3231  loss_prd: 0.0838  loss_mrtd: 0.2255  time: 1.3046  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1850/2519]  eta: 0:14:41  lr: 0.000008  loss_cl: 8.1056  loss_pitm: 0.0776  loss_mlm: 0.2884  loss_prd: 0.3041  loss_mrtd: 0.1722  time: 1.2968  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1900/2519]  eta: 0:13:35  lr: 0.000008  loss_cl: 8.3682  loss_pitm: 0.0302  loss_mlm: 0.6055  loss_prd: 0.0969  loss_mrtd: 0.2378  time: 1.2994  data: 0.0001  max mem: 17763
Train Epoch: [10]  [1950/2519]  eta: 0:12:28  lr: 0.000008  loss_cl: 8.1620  loss_pitm: 0.0879  loss_mlm: 0.8534  loss_prd: 0.1021  loss_mrtd: 0.2902  time: 1.2987  data: 0.0001  max mem: 17763
Train Epoch: [10]  [2000/2519]  eta: 0:11:22  lr: 0.000008  loss_cl: 7.9669  loss_pitm: 0.0202  loss_mlm: 0.5037  loss_prd: 0.6087  loss_mrtd: 0.2150  time: 1.3036  data: 0.0001  max mem: 17763
Train Epoch: [10]  [2050/2519]  eta: 0:10:17  lr: 0.000008  loss_cl: 7.9207  loss_pitm: 0.0151  loss_mlm: 0.6340  loss_prd: 0.5148  loss_mrtd: 0.2214  time: 1.3088  data: 0.0001  max mem: 17763
Train Epoch: [10]  [2100/2519]  eta: 0:09:11  lr: 0.000008  loss_cl: 7.6484  loss_pitm: 0.0272  loss_mlm: 0.3037  loss_prd: 0.5877  loss_mrtd: 0.1742  time: 1.3063  data: 0.0001  max mem: 17763
Train Epoch: [10]  [2150/2519]  eta: 0:08:05  lr: 0.000008  loss_cl: 8.0987  loss_pitm: 0.0533  loss_mlm: 0.4350  loss_prd: 0.2566  loss_mrtd: 0.1706  time: 1.3127  data: 0.0001  max mem: 17763
Train Epoch: [10]  [2200/2519]  eta: 0:06:59  lr: 0.000008  loss_cl: 7.8380  loss_pitm: 0.0875  loss_mlm: 0.5711  loss_prd: 0.2832  loss_mrtd: 0.3019  time: 1.3062  data: 0.0001  max mem: 17763
Train Epoch: [10]  [2250/2519]  eta: 0:05:53  lr: 0.000008  loss_cl: 7.8229  loss_pitm: 0.1176  loss_mlm: 0.7965  loss_prd: 0.1703  loss_mrtd: 0.2382  time: 1.2823  data: 0.0001  max mem: 17763
Train Epoch: [10]  [2300/2519]  eta: 0:04:47  lr: 0.000008  loss_cl: 7.8809  loss_pitm: 0.0286  loss_mlm: 0.4825  loss_prd: 0.0808  loss_mrtd: 0.2723  time: 1.2878  data: 0.0001  max mem: 17763
Train Epoch: [10]  [2350/2519]  eta: 0:03:41  lr: 0.000008  loss_cl: 7.4280  loss_pitm: 0.0061  loss_mlm: 0.5163  loss_prd: 0.2651  loss_mrtd: 0.2359  time: 1.3016  data: 0.0001  max mem: 17763
Train Epoch: [10]  [2400/2519]  eta: 0:02:36  lr: 0.000008  loss_cl: 7.2156  loss_pitm: 0.2388  loss_mlm: 0.3691  loss_prd: 0.0522  loss_mrtd: 0.2776  time: 1.2979  data: 0.0001  max mem: 17763
Train Epoch: [10]  [2450/2519]  eta: 0:01:30  lr: 0.000008  loss_cl: 7.5143  loss_pitm: 0.0515  loss_mlm: 0.5847  loss_prd: 0.0692  loss_mrtd: 0.2808  time: 1.2850  data: 0.0001  max mem: 17763
Train Epoch: [10]  [2500/2519]  eta: 0:00:24  lr: 0.000008  loss_cl: 7.3632  loss_pitm: 0.1557  loss_mlm: 0.6190  loss_prd: 0.0611  loss_mrtd: 0.3124  time: 1.3127  data: 0.0001  max mem: 17763
Train Epoch: [10]  [2518/2519]  eta: 0:00:01  lr: 0.000008  loss_cl: 7.3392  loss_pitm: 0.1267  loss_mlm: 0.2931  loss_prd: 0.3913  loss_mrtd: 0.1362  time: 1.2961  data: 0.0002  max mem: 17763
Train Epoch: [10] Total time: 0:55:06 (1.3127 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.7590  loss_pitm: 0.0626  loss_mlm: 0.4814  loss_prd: 0.2303  loss_mrtd: 0.2356
Train Epoch的聚类: [11]  [   0/5241]  eta: 0:52:35    time: 0.6021  data: 0.5818  max mem: 17763
Train Epoch的聚类: [11]  [5240/5241]  eta: 0:00:00    time: 0.1713  data: 0.0002  max mem: 17763
Train Epoch的聚类: [11] Total time: 0:14:58 (0.1714 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4588
-1 (未归入任何簇) 的数量: 2513

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [11]  [   0/2523]  eta: 19:49:30  lr: 0.000007  loss_cl: 10.2945  loss_pitm: 0.0942  loss_mlm: 0.2495  loss_prd: 0.2690  loss_mrtd: 0.1173  time: 28.2878  data: 0.5821  max mem: 17763
Train Epoch: [11]  [  50/2523]  eta: 1:14:00  lr: 0.000007  loss_cl: 9.3938  loss_pitm: 0.0328  loss_mlm: 0.4018  loss_prd: 0.0813  loss_mrtd: 0.2158  time: 1.2812  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 100/2523]  eta: 1:02:20  lr: 0.000007  loss_cl: 9.2553  loss_pitm: 0.0150  loss_mlm: 0.3672  loss_prd: 0.4929  loss_mrtd: 0.1469  time: 1.2917  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 150/2523]  eta: 0:57:47  lr: 0.000007  loss_cl: 9.3320  loss_pitm: 0.0155  loss_mlm: 0.4239  loss_prd: 0.0629  loss_mrtd: 0.1832  time: 1.2945  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 200/2523]  eta: 0:54:58  lr: 0.000007  loss_cl: 9.7755  loss_pitm: 0.0363  loss_mlm: 0.2786  loss_prd: 0.4622  loss_mrtd: 0.2260  time: 1.2950  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 250/2523]  eta: 0:52:53  lr: 0.000007  loss_cl: 9.4367  loss_pitm: 0.0395  loss_mlm: 0.3298  loss_prd: 0.0986  loss_mrtd: 0.1800  time: 1.3011  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 300/2523]  eta: 0:51:08  lr: 0.000007  loss_cl: 9.7051  loss_pitm: 0.3021  loss_mlm: 0.4685  loss_prd: 0.3123  loss_mrtd: 0.2330  time: 1.2918  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 350/2523]  eta: 0:49:32  lr: 0.000007  loss_cl: 9.0573  loss_pitm: 0.0049  loss_mlm: 0.4261  loss_prd: 0.2691  loss_mrtd: 0.1793  time: 1.2954  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 400/2523]  eta: 0:48:03  lr: 0.000007  loss_cl: 9.5250  loss_pitm: 0.0910  loss_mlm: 0.4793  loss_prd: 0.1088  loss_mrtd: 0.1898  time: 1.2848  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 450/2523]  eta: 0:46:43  lr: 0.000007  loss_cl: 9.7193  loss_pitm: 0.0239  loss_mlm: 0.3496  loss_prd: 0.0425  loss_mrtd: 0.2552  time: 1.3054  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 500/2523]  eta: 0:45:25  lr: 0.000007  loss_cl: 9.7872  loss_pitm: 0.0202  loss_mlm: 0.6739  loss_prd: 0.1039  loss_mrtd: 0.3082  time: 1.2891  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 550/2523]  eta: 0:44:08  lr: 0.000007  loss_cl: 9.7465  loss_pitm: 0.0039  loss_mlm: 0.4395  loss_prd: 0.0493  loss_mrtd: 0.2040  time: 1.2969  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 600/2523]  eta: 0:42:54  lr: 0.000007  loss_cl: 9.4680  loss_pitm: 0.0143  loss_mlm: 0.4102  loss_prd: 0.0476  loss_mrtd: 0.2001  time: 1.3020  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 650/2523]  eta: 0:41:41  lr: 0.000007  loss_cl: 9.2706  loss_pitm: 0.0229  loss_mlm: 0.7655  loss_prd: 0.0772  loss_mrtd: 0.2139  time: 1.3004  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 700/2523]  eta: 0:40:30  lr: 0.000007  loss_cl: 9.2173  loss_pitm: 0.0447  loss_mlm: 0.5759  loss_prd: 0.2695  loss_mrtd: 0.2258  time: 1.2993  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 750/2523]  eta: 0:39:20  lr: 0.000007  loss_cl: 9.0695  loss_pitm: 0.0139  loss_mlm: 0.6116  loss_prd: 0.0663  loss_mrtd: 0.2609  time: 1.2972  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 800/2523]  eta: 0:38:10  lr: 0.000007  loss_cl: 9.0233  loss_pitm: 0.0142  loss_mlm: 0.3013  loss_prd: 0.0957  loss_mrtd: 0.2389  time: 1.3025  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 850/2523]  eta: 0:37:00  lr: 0.000007  loss_cl: 8.8981  loss_pitm: 0.0114  loss_mlm: 0.2458  loss_prd: 0.0677  loss_mrtd: 0.2111  time: 1.2981  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 900/2523]  eta: 0:35:51  lr: 0.000007  loss_cl: 9.5159  loss_pitm: 0.0320  loss_mlm: 0.2943  loss_prd: 0.5743  loss_mrtd: 0.2649  time: 1.2976  data: 0.0001  max mem: 17763
Train Epoch: [11]  [ 950/2523]  eta: 0:34:42  lr: 0.000007  loss_cl: 9.4054  loss_pitm: 0.0092  loss_mlm: 0.4490  loss_prd: 0.2642  loss_mrtd: 0.2771  time: 1.2919  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1000/2523]  eta: 0:33:34  lr: 0.000007  loss_cl: 8.7970  loss_pitm: 0.0028  loss_mlm: 0.9182  loss_prd: 0.2824  loss_mrtd: 0.2980  time: 1.2985  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1050/2523]  eta: 0:32:26  lr: 0.000007  loss_cl: 8.9021  loss_pitm: 0.0032  loss_mlm: 0.4437  loss_prd: 0.0529  loss_mrtd: 0.1972  time: 1.2987  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1100/2523]  eta: 0:31:18  lr: 0.000007  loss_cl: 9.0316  loss_pitm: 0.0211  loss_mlm: 0.4400  loss_prd: 0.0448  loss_mrtd: 0.2200  time: 1.2910  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1150/2523]  eta: 0:30:10  lr: 0.000007  loss_cl: 8.6618  loss_pitm: 0.0264  loss_mlm: 0.2723  loss_prd: 0.3474  loss_mrtd: 0.2274  time: 1.2941  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1200/2523]  eta: 0:29:03  lr: 0.000007  loss_cl: 9.3278  loss_pitm: 0.0306  loss_mlm: 0.6446  loss_prd: 0.2564  loss_mrtd: 0.2156  time: 1.2950  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1250/2523]  eta: 0:27:56  lr: 0.000007  loss_cl: 9.1855  loss_pitm: 0.0061  loss_mlm: 0.4809  loss_prd: 0.0544  loss_mrtd: 0.2241  time: 1.3021  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1300/2523]  eta: 0:26:49  lr: 0.000007  loss_cl: 8.7844  loss_pitm: 0.0465  loss_mlm: 0.5118  loss_prd: 0.0788  loss_mrtd: 0.2128  time: 1.2984  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1350/2523]  eta: 0:25:43  lr: 0.000007  loss_cl: 8.6263  loss_pitm: 0.0520  loss_mlm: 0.3529  loss_prd: 0.4661  loss_mrtd: 0.2731  time: 1.2983  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1400/2523]  eta: 0:24:36  lr: 0.000007  loss_cl: 8.6074  loss_pitm: 0.0747  loss_mlm: 0.4538  loss_prd: 0.5802  loss_mrtd: 0.2069  time: 1.2974  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1450/2523]  eta: 0:23:30  lr: 0.000007  loss_cl: 8.5018  loss_pitm: 0.0450  loss_mlm: 0.3432  loss_prd: 0.1647  loss_mrtd: 0.2359  time: 1.2943  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1500/2523]  eta: 0:22:23  lr: 0.000007  loss_cl: 8.0431  loss_pitm: 0.0267  loss_mlm: 0.4002  loss_prd: 0.3062  loss_mrtd: 0.1523  time: 1.3012  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1550/2523]  eta: 0:21:17  lr: 0.000007  loss_cl: 8.6358  loss_pitm: 0.1416  loss_mlm: 0.7309  loss_prd: 0.2865  loss_mrtd: 0.3077  time: 1.2934  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1600/2523]  eta: 0:20:11  lr: 0.000007  loss_cl: 7.9576  loss_pitm: 0.1056  loss_mlm: 0.5294  loss_prd: 0.0420  loss_mrtd: 0.2367  time: 1.2918  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1650/2523]  eta: 0:19:05  lr: 0.000007  loss_cl: 8.7904  loss_pitm: 0.1385  loss_mlm: 0.2045  loss_prd: 0.0517  loss_mrtd: 0.2156  time: 1.2862  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1700/2523]  eta: 0:17:59  lr: 0.000007  loss_cl: 8.0160  loss_pitm: 0.0094  loss_mlm: 0.6255  loss_prd: 0.5517  loss_mrtd: 0.2736  time: 1.3046  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1750/2523]  eta: 0:16:53  lr: 0.000007  loss_cl: 8.5079  loss_pitm: 0.1120  loss_mlm: 0.5568  loss_prd: 0.1028  loss_mrtd: 0.2947  time: 1.3013  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1800/2523]  eta: 0:15:47  lr: 0.000007  loss_cl: 8.3877  loss_pitm: 0.0378  loss_mlm: 0.1740  loss_prd: 0.0708  loss_mrtd: 0.2242  time: 1.2997  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1850/2523]  eta: 0:14:42  lr: 0.000007  loss_cl: 8.5071  loss_pitm: 0.0450  loss_mlm: 0.3518  loss_prd: 0.0964  loss_mrtd: 0.2897  time: 1.2971  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1900/2523]  eta: 0:13:36  lr: 0.000007  loss_cl: 8.2969  loss_pitm: 0.0345  loss_mlm: 0.6894  loss_prd: 0.2779  loss_mrtd: 0.2407  time: 1.3127  data: 0.0001  max mem: 17763
Train Epoch: [11]  [1950/2523]  eta: 0:12:30  lr: 0.000007  loss_cl: 8.2456  loss_pitm: 0.0379  loss_mlm: 0.2555  loss_prd: 0.0647  loss_mrtd: 0.2554  time: 1.3145  data: 0.0001  max mem: 17763
Train Epoch: [11]  [2000/2523]  eta: 0:11:25  lr: 0.000007  loss_cl: 7.7347  loss_pitm: 0.0573  loss_mlm: 0.3586  loss_prd: 0.2526  loss_mrtd: 0.2530  time: 1.3000  data: 0.0001  max mem: 17763
Train Epoch: [11]  [2050/2523]  eta: 0:10:19  lr: 0.000007  loss_cl: 7.8160  loss_pitm: 0.0166  loss_mlm: 0.8856  loss_prd: 0.6070  loss_mrtd: 0.2727  time: 1.3040  data: 0.0001  max mem: 17763
Train Epoch: [11]  [2100/2523]  eta: 0:09:14  lr: 0.000007  loss_cl: 8.1940  loss_pitm: 0.3459  loss_mlm: 0.3990  loss_prd: 0.6472  loss_mrtd: 0.2500  time: 1.2993  data: 0.0001  max mem: 17763
Train Epoch: [11]  [2150/2523]  eta: 0:08:08  lr: 0.000007  loss_cl: 7.6802  loss_pitm: 0.1289  loss_mlm: 0.6173  loss_prd: 0.2733  loss_mrtd: 0.1989  time: 1.3022  data: 0.0001  max mem: 17763
Train Epoch: [11]  [2200/2523]  eta: 0:07:02  lr: 0.000007  loss_cl: 7.6721  loss_pitm: 0.1256  loss_mlm: 0.3935  loss_prd: 0.0683  loss_mrtd: 0.3419  time: 1.2893  data: 0.0001  max mem: 17763
Train Epoch: [11]  [2250/2523]  eta: 0:05:57  lr: 0.000007  loss_cl: 7.8735  loss_pitm: 0.1521  loss_mlm: 0.3685  loss_prd: 0.1151  loss_mrtd: 0.2451  time: 1.2993  data: 0.0001  max mem: 17763
Train Epoch: [11]  [2300/2523]  eta: 0:04:51  lr: 0.000007  loss_cl: 7.7129  loss_pitm: 0.0293  loss_mlm: 0.5241  loss_prd: 0.3119  loss_mrtd: 0.2409  time: 1.2961  data: 0.0001  max mem: 17763
Train Epoch: [11]  [2350/2523]  eta: 0:03:46  lr: 0.000007  loss_cl: 7.3583  loss_pitm: 0.0100  loss_mlm: 0.5847  loss_prd: 0.5716  loss_mrtd: 0.2457  time: 1.3005  data: 0.0001  max mem: 17763
Train Epoch: [11]  [2400/2523]  eta: 0:02:40  lr: 0.000007  loss_cl: 7.1853  loss_pitm: 0.0041  loss_mlm: 0.3801  loss_prd: 0.0586  loss_mrtd: 0.2852  time: 1.2873  data: 0.0001  max mem: 17763
Train Epoch: [11]  [2450/2523]  eta: 0:01:35  lr: 0.000007  loss_cl: 7.6527  loss_pitm: 0.0134  loss_mlm: 0.3026  loss_prd: 0.0521  loss_mrtd: 0.2004  time: 1.2962  data: 0.0001  max mem: 17763
Train Epoch: [11]  [2500/2523]  eta: 0:00:30  lr: 0.000007  loss_cl: 7.1359  loss_pitm: 0.0075  loss_mlm: 0.4367  loss_prd: 0.0618  loss_mrtd: 0.2436  time: 1.2942  data: 0.0001  max mem: 17763
Train Epoch: [11]  [2522/2523]  eta: 0:00:01  lr: 0.000007  loss_cl: 7.1406  loss_pitm: 0.0107  loss_mlm: 0.6050  loss_prd: 0.4609  loss_mrtd: 0.2044  time: 1.2936  data: 0.0001  max mem: 17763
Train Epoch: [11] Total time: 0:54:59 (1.3076 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.6605  loss_pitm: 0.0620  loss_mlm: 0.4762  loss_prd: 0.2295  loss_mrtd: 0.2333
Train Epoch的聚类: [12]  [   0/5241]  eta: 1:01:01    time: 0.6986  data: 0.6762  max mem: 17763
Train Epoch的聚类: [12]  [5240/5241]  eta: 0:00:00    time: 0.1715  data: 0.0002  max mem: 17763
Train Epoch的聚类: [12] Total time: 0:14:59 (0.1715 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4554
-1 (未归入任何簇) 的数量: 2403

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [12]  [   0/2527]  eta: 19:49:39  lr: 0.000007  loss_cl: 10.1824  loss_pitm: 0.0110  loss_mlm: 0.3867  loss_prd: 0.4397  loss_mrtd: 0.2855  time: 28.2467  data: 0.6494  max mem: 17763
Train Epoch: [12]  [  50/2527]  eta: 1:13:53  lr: 0.000007  loss_cl: 10.0915  loss_pitm: 0.0066  loss_mlm: 0.2381  loss_prd: 0.2481  loss_mrtd: 0.1762  time: 1.2763  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 100/2527]  eta: 1:02:19  lr: 0.000007  loss_cl: 9.8622  loss_pitm: 0.2096  loss_mlm: 0.5470  loss_prd: 0.2348  loss_mrtd: 0.2697  time: 1.2916  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 150/2527]  eta: 0:57:43  lr: 0.000007  loss_cl: 9.1815  loss_pitm: 0.0165  loss_mlm: 0.8990  loss_prd: 0.0604  loss_mrtd: 0.1636  time: 1.2960  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 200/2527]  eta: 0:54:57  lr: 0.000007  loss_cl: 10.0181  loss_pitm: 0.0142  loss_mlm: 0.3738  loss_prd: 0.0981  loss_mrtd: 0.2267  time: 1.2949  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 250/2527]  eta: 0:52:54  lr: 0.000007  loss_cl: 9.6160  loss_pitm: 0.1016  loss_mlm: 0.7534  loss_prd: 0.0575  loss_mrtd: 0.1662  time: 1.3089  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 300/2527]  eta: 0:51:11  lr: 0.000007  loss_cl: 9.3348  loss_pitm: 0.2299  loss_mlm: 0.2675  loss_prd: 0.0463  loss_mrtd: 0.3013  time: 1.3074  data: 0.0002  max mem: 17763
Train Epoch: [12]  [ 350/2527]  eta: 0:49:35  lr: 0.000007  loss_cl: 9.4323  loss_pitm: 0.0092  loss_mlm: 0.4843  loss_prd: 0.1161  loss_mrtd: 0.1726  time: 1.2875  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 400/2527]  eta: 0:48:08  lr: 0.000007  loss_cl: 9.7217  loss_pitm: 0.0053  loss_mlm: 0.2539  loss_prd: 0.0559  loss_mrtd: 0.1420  time: 1.2819  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 450/2527]  eta: 0:46:45  lr: 0.000007  loss_cl: 9.2142  loss_pitm: 0.2606  loss_mlm: 0.2640  loss_prd: 0.2564  loss_mrtd: 0.2306  time: 1.2970  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 500/2527]  eta: 0:45:28  lr: 0.000007  loss_cl: 9.5705  loss_pitm: 0.0070  loss_mlm: 0.4395  loss_prd: 0.0541  loss_mrtd: 0.2014  time: 1.3052  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 550/2527]  eta: 0:44:12  lr: 0.000007  loss_cl: 9.4499  loss_pitm: 0.1306  loss_mlm: 0.5988  loss_prd: 0.4123  loss_mrtd: 0.1743  time: 1.2928  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 600/2527]  eta: 0:42:59  lr: 0.000007  loss_cl: 9.4190  loss_pitm: 0.0162  loss_mlm: 0.4806  loss_prd: 0.0989  loss_mrtd: 0.2046  time: 1.2923  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 650/2527]  eta: 0:41:46  lr: 0.000007  loss_cl: 9.8094  loss_pitm: 0.0200  loss_mlm: 0.4775  loss_prd: 0.3243  loss_mrtd: 0.2058  time: 1.2883  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 700/2527]  eta: 0:40:33  lr: 0.000007  loss_cl: 9.6771  loss_pitm: 0.0104  loss_mlm: 0.5903  loss_prd: 0.2896  loss_mrtd: 0.2245  time: 1.3007  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 750/2527]  eta: 0:39:22  lr: 0.000007  loss_cl: 9.2749  loss_pitm: 0.0273  loss_mlm: 0.9327  loss_prd: 0.0794  loss_mrtd: 0.2869  time: 1.2935  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 800/2527]  eta: 0:38:13  lr: 0.000007  loss_cl: 9.0665  loss_pitm: 0.0232  loss_mlm: 0.1046  loss_prd: 0.0471  loss_mrtd: 0.3608  time: 1.2934  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 850/2527]  eta: 0:37:03  lr: 0.000007  loss_cl: 9.4088  loss_pitm: 0.0892  loss_mlm: 0.4479  loss_prd: 0.0683  loss_mrtd: 0.2322  time: 1.3026  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 900/2527]  eta: 0:35:55  lr: 0.000007  loss_cl: 9.0242  loss_pitm: 0.1728  loss_mlm: 0.4793  loss_prd: 0.0996  loss_mrtd: 0.1934  time: 1.2987  data: 0.0001  max mem: 17763
Train Epoch: [12]  [ 950/2527]  eta: 0:34:46  lr: 0.000007  loss_cl: 9.1510  loss_pitm: 0.0185  loss_mlm: 0.6597  loss_prd: 0.0488  loss_mrtd: 0.1916  time: 1.2933  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1000/2527]  eta: 0:33:38  lr: 0.000007  loss_cl: 9.2337  loss_pitm: 0.0163  loss_mlm: 0.4870  loss_prd: 0.1649  loss_mrtd: 0.2087  time: 1.2967  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1050/2527]  eta: 0:32:31  lr: 0.000007  loss_cl: 8.7451  loss_pitm: 0.0324  loss_mlm: 0.2167  loss_prd: 0.0565  loss_mrtd: 0.2129  time: 1.2958  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1100/2527]  eta: 0:31:23  lr: 0.000007  loss_cl: 8.5161  loss_pitm: 0.0072  loss_mlm: 0.4517  loss_prd: 0.0717  loss_mrtd: 0.2401  time: 1.2950  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1150/2527]  eta: 0:30:15  lr: 0.000007  loss_cl: 9.0294  loss_pitm: 0.0083  loss_mlm: 0.3663  loss_prd: 0.0575  loss_mrtd: 0.2684  time: 1.2971  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1200/2527]  eta: 0:29:08  lr: 0.000007  loss_cl: 9.2286  loss_pitm: 0.0570  loss_mlm: 0.4698  loss_prd: 0.2894  loss_mrtd: 0.2275  time: 1.2927  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1250/2527]  eta: 0:28:01  lr: 0.000007  loss_cl: 8.8564  loss_pitm: 0.0080  loss_mlm: 0.7196  loss_prd: 0.0487  loss_mrtd: 0.2895  time: 1.2954  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1300/2527]  eta: 0:26:54  lr: 0.000007  loss_cl: 9.0014  loss_pitm: 0.1072  loss_mlm: 0.4800  loss_prd: 0.0606  loss_mrtd: 0.2628  time: 1.2938  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1350/2527]  eta: 0:25:48  lr: 0.000007  loss_cl: 8.8175  loss_pitm: 0.1427  loss_mlm: 0.6893  loss_prd: 0.0551  loss_mrtd: 0.2744  time: 1.3017  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1400/2527]  eta: 0:24:41  lr: 0.000007  loss_cl: 8.7437  loss_pitm: 0.0242  loss_mlm: 0.2580  loss_prd: 0.1555  loss_mrtd: 0.1603  time: 1.3026  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1450/2527]  eta: 0:23:35  lr: 0.000007  loss_cl: 8.6332  loss_pitm: 0.0812  loss_mlm: 0.4412  loss_prd: 0.0473  loss_mrtd: 0.2796  time: 1.2906  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1500/2527]  eta: 0:22:29  lr: 0.000007  loss_cl: 8.1864  loss_pitm: 0.0407  loss_mlm: 0.2657  loss_prd: 0.3602  loss_mrtd: 0.2468  time: 1.3043  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1550/2527]  eta: 0:21:23  lr: 0.000007  loss_cl: 8.6297  loss_pitm: 0.1504  loss_mlm: 0.6201  loss_prd: 0.0600  loss_mrtd: 0.2016  time: 1.2994  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1600/2527]  eta: 0:20:16  lr: 0.000007  loss_cl: 8.5113  loss_pitm: 0.0357  loss_mlm: 0.6861  loss_prd: 0.1773  loss_mrtd: 0.2657  time: 1.2919  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1650/2527]  eta: 0:19:11  lr: 0.000007  loss_cl: 8.4618  loss_pitm: 0.0058  loss_mlm: 0.4733  loss_prd: 0.0608  loss_mrtd: 0.2159  time: 1.2971  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1700/2527]  eta: 0:18:05  lr: 0.000007  loss_cl: 8.2712  loss_pitm: 0.0062  loss_mlm: 0.7323  loss_prd: 0.5426  loss_mrtd: 0.2604  time: 1.2914  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1750/2527]  eta: 0:16:59  lr: 0.000007  loss_cl: 8.0689  loss_pitm: 0.0065  loss_mlm: 0.9604  loss_prd: 0.1094  loss_mrtd: 0.2221  time: 1.2964  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1800/2527]  eta: 0:15:53  lr: 0.000007  loss_cl: 8.3156  loss_pitm: 0.1540  loss_mlm: 0.4804  loss_prd: 0.1307  loss_mrtd: 0.2852  time: 1.3207  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1850/2527]  eta: 0:14:47  lr: 0.000007  loss_cl: 8.3166  loss_pitm: 0.0660  loss_mlm: 0.3537  loss_prd: 0.0707  loss_mrtd: 0.1809  time: 1.3076  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1900/2527]  eta: 0:13:42  lr: 0.000007  loss_cl: 8.0533  loss_pitm: 0.0409  loss_mlm: 0.3750  loss_prd: 0.0529  loss_mrtd: 0.2025  time: 1.3007  data: 0.0001  max mem: 17763
Train Epoch: [12]  [1950/2527]  eta: 0:12:36  lr: 0.000007  loss_cl: 8.0461  loss_pitm: 0.0176  loss_mlm: 0.5451  loss_prd: 0.0515  loss_mrtd: 0.3171  time: 1.2952  data: 0.0001  max mem: 17763
Train Epoch: [12]  [2000/2527]  eta: 0:11:30  lr: 0.000007  loss_cl: 8.0532  loss_pitm: 0.0515  loss_mlm: 0.7790  loss_prd: 0.1120  loss_mrtd: 0.1684  time: 1.2917  data: 0.0001  max mem: 17763
Train Epoch: [12]  [2050/2527]  eta: 0:10:24  lr: 0.000007  loss_cl: 8.1466  loss_pitm: 0.0337  loss_mlm: 0.6164  loss_prd: 0.0624  loss_mrtd: 0.2083  time: 1.2960  data: 0.0001  max mem: 17763
Train Epoch: [12]  [2100/2527]  eta: 0:09:19  lr: 0.000007  loss_cl: 7.7404  loss_pitm: 0.1302  loss_mlm: 0.4495  loss_prd: 0.1898  loss_mrtd: 0.1917  time: 1.3000  data: 0.0001  max mem: 17763
Train Epoch: [12]  [2150/2527]  eta: 0:08:13  lr: 0.000007  loss_cl: 7.8272  loss_pitm: 0.0049  loss_mlm: 0.4128  loss_prd: 0.2862  loss_mrtd: 0.2324  time: 1.2994  data: 0.0001  max mem: 17763
Train Epoch: [12]  [2200/2527]  eta: 0:07:08  lr: 0.000007  loss_cl: 7.4598  loss_pitm: 0.0160  loss_mlm: 0.3003  loss_prd: 0.2954  loss_mrtd: 0.2822  time: 1.3051  data: 0.0001  max mem: 17763
Train Epoch: [12]  [2250/2527]  eta: 0:06:02  lr: 0.000007  loss_cl: 7.4665  loss_pitm: 0.5346  loss_mlm: 0.2099  loss_prd: 0.0553  loss_mrtd: 0.1722  time: 1.2999  data: 0.0001  max mem: 17763
Train Epoch: [12]  [2300/2527]  eta: 0:04:57  lr: 0.000007  loss_cl: 7.6971  loss_pitm: 0.0225  loss_mlm: 0.7003  loss_prd: 0.0716  loss_mrtd: 0.3421  time: 1.3032  data: 0.0001  max mem: 17763
Train Epoch: [12]  [2350/2527]  eta: 0:03:51  lr: 0.000007  loss_cl: 7.3767  loss_pitm: 0.0150  loss_mlm: 0.4949  loss_prd: 0.0695  loss_mrtd: 0.1773  time: 1.3016  data: 0.0001  max mem: 17763
Train Epoch: [12]  [2400/2527]  eta: 0:02:46  lr: 0.000007  loss_cl: 7.5624  loss_pitm: 0.0135  loss_mlm: 0.7362  loss_prd: 0.2819  loss_mrtd: 0.2355  time: 1.2973  data: 0.0001  max mem: 17763
Train Epoch: [12]  [2450/2527]  eta: 0:01:40  lr: 0.000007  loss_cl: 7.4934  loss_pitm: 0.0362  loss_mlm: 0.7054  loss_prd: 0.0402  loss_mrtd: 0.2510  time: 1.3031  data: 0.0001  max mem: 17763
Train Epoch: [12]  [2500/2527]  eta: 0:00:35  lr: 0.000007  loss_cl: 7.4669  loss_pitm: 0.0118  loss_mlm: 0.5504  loss_prd: 0.5098  loss_mrtd: 0.2552  time: 1.2977  data: 0.0001  max mem: 17763
Train Epoch: [12]  [2526/2527]  eta: 0:00:01  lr: 0.000007  loss_cl: 7.3666  loss_pitm: 0.0356  loss_mlm: 0.2181  loss_prd: 0.3345  loss_mrtd: 0.2129  time: 1.2837  data: 0.0002  max mem: 17763
Train Epoch: [12] Total time: 0:55:05 (1.3080 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.6802  loss_pitm: 0.0603  loss_mlm: 0.4658  loss_prd: 0.2256  loss_mrtd: 0.2319
Train Epoch的聚类: [13]  [   0/5241]  eta: 0:57:56    time: 0.6634  data: 0.6434  max mem: 17763
Train Epoch的聚类: [13]  [5240/5241]  eta: 0:00:00    time: 0.1716  data: 0.0002  max mem: 17763
Train Epoch的聚类: [13] Total time: 0:14:59 (0.1717 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4517
-1 (未归入任何簇) 的数量: 2394

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [13]  [   0/2528]  eta: 21:51:43  lr: 0.000006  loss_cl: 10.1050  loss_pitm: 0.1066  loss_mlm: 0.2932  loss_prd: 0.2113  loss_mrtd: 0.1943  time: 31.1327  data: 0.6606  max mem: 17763
Train Epoch: [13]  [  50/2528]  eta: 1:17:45  lr: 0.000006  loss_cl: 9.7834  loss_pitm: 0.0211  loss_mlm: 0.4966  loss_prd: 0.2691  loss_mrtd: 0.2480  time: 1.3525  data: 0.0002  max mem: 17763
Train Epoch: [13]  [ 100/2528]  eta: 1:04:19  lr: 0.000006  loss_cl: 9.9343  loss_pitm: 0.0829  loss_mlm: 0.8169  loss_prd: 0.3747  loss_mrtd: 0.2076  time: 1.2919  data: 0.0002  max mem: 17763
Train Epoch: [13]  [ 150/2528]  eta: 0:59:10  lr: 0.000006  loss_cl: 9.2908  loss_pitm: 0.0159  loss_mlm: 0.4810  loss_prd: 0.2582  loss_mrtd: 0.2598  time: 1.2968  data: 0.0001  max mem: 17763
Train Epoch: [13]  [ 200/2528]  eta: 0:56:04  lr: 0.000006  loss_cl: 9.9419  loss_pitm: 0.0047  loss_mlm: 0.4759  loss_prd: 0.2800  loss_mrtd: 0.2581  time: 1.3067  data: 0.0001  max mem: 17763
Train Epoch: [13]  [ 250/2528]  eta: 0:53:43  lr: 0.000006  loss_cl: 9.6136  loss_pitm: 0.0649  loss_mlm: 0.3948  loss_prd: 0.1058  loss_mrtd: 0.2614  time: 1.3028  data: 0.0001  max mem: 17763
Train Epoch: [13]  [ 300/2528]  eta: 0:51:52  lr: 0.000006  loss_cl: 9.5436  loss_pitm: 0.0085  loss_mlm: 0.5157  loss_prd: 0.2752  loss_mrtd: 0.2144  time: 1.3027  data: 0.0002  max mem: 17763
Train Epoch: [13]  [ 350/2528]  eta: 0:50:14  lr: 0.000006  loss_cl: 9.6558  loss_pitm: 0.0519  loss_mlm: 0.3085  loss_prd: 0.0511  loss_mrtd: 0.2244  time: 1.3216  data: 0.0002  max mem: 17763
Train Epoch: [13]  [ 400/2528]  eta: 0:48:43  lr: 0.000006  loss_cl: 9.6299  loss_pitm: 0.0041  loss_mlm: 0.7241  loss_prd: 0.4777  loss_mrtd: 0.2570  time: 1.3033  data: 0.0002  max mem: 17763
Train Epoch: [13]  [ 450/2528]  eta: 0:47:19  lr: 0.000006  loss_cl: 9.7507  loss_pitm: 0.0704  loss_mlm: 0.6036  loss_prd: 0.3400  loss_mrtd: 0.2262  time: 1.3058  data: 0.0001  max mem: 17763
Train Epoch: [13]  [ 500/2528]  eta: 0:45:56  lr: 0.000006  loss_cl: 9.4844  loss_pitm: 0.0086  loss_mlm: 0.4118  loss_prd: 0.0614  loss_mrtd: 0.2566  time: 1.2949  data: 0.0002  max mem: 17763
Train Epoch: [13]  [ 550/2528]  eta: 0:44:39  lr: 0.000006  loss_cl: 9.3972  loss_pitm: 0.4116  loss_mlm: 0.3914  loss_prd: 0.2731  loss_mrtd: 0.1846  time: 1.2982  data: 0.0001  max mem: 17763
Train Epoch: [13]  [ 600/2528]  eta: 0:43:22  lr: 0.000006  loss_cl: 9.6109  loss_pitm: 0.0643  loss_mlm: 0.5685  loss_prd: 0.0482  loss_mrtd: 0.2474  time: 1.3017  data: 0.0002  max mem: 17763
Train Epoch: [13]  [ 650/2528]  eta: 0:42:08  lr: 0.000006  loss_cl: 9.3878  loss_pitm: 0.1880  loss_mlm: 0.5475  loss_prd: 0.0659  loss_mrtd: 0.1956  time: 1.3034  data: 0.0002  max mem: 17763
Train Epoch: [13]  [ 700/2528]  eta: 0:40:55  lr: 0.000006  loss_cl: 9.3227  loss_pitm: 0.0083  loss_mlm: 0.2083  loss_prd: 0.4839  loss_mrtd: 0.1868  time: 1.3027  data: 0.0002  max mem: 17763
Train Epoch: [13]  [ 750/2528]  eta: 0:39:43  lr: 0.000006  loss_cl: 9.3773  loss_pitm: 0.0065  loss_mlm: 0.4604  loss_prd: 0.0413  loss_mrtd: 0.2283  time: 1.2997  data: 0.0002  max mem: 17763
Train Epoch: [13]  [ 800/2528]  eta: 0:38:34  lr: 0.000006  loss_cl: 9.0902  loss_pitm: 0.0087  loss_mlm: 0.5944  loss_prd: 0.2827  loss_mrtd: 0.1976  time: 1.3132  data: 0.0002  max mem: 17763
Train Epoch: [13]  [ 850/2528]  eta: 0:37:23  lr: 0.000006  loss_cl: 9.5052  loss_pitm: 0.1653  loss_mlm: 0.3752  loss_prd: 0.3295  loss_mrtd: 0.2272  time: 1.3136  data: 0.0002  max mem: 17763
Train Epoch: [13]  [ 900/2528]  eta: 0:36:13  lr: 0.000006  loss_cl: 9.3142  loss_pitm: 0.0062  loss_mlm: 0.3980  loss_prd: 0.2321  loss_mrtd: 0.2773  time: 1.2906  data: 0.0001  max mem: 17763
Train Epoch: [13]  [ 950/2528]  eta: 0:35:04  lr: 0.000006  loss_cl: 9.2240  loss_pitm: 0.0040  loss_mlm: 0.8903  loss_prd: 0.0704  loss_mrtd: 0.2412  time: 1.3142  data: 0.0002  max mem: 17763
Train Epoch: [13]  [1000/2528]  eta: 0:33:55  lr: 0.000006  loss_cl: 8.7785  loss_pitm: 0.1016  loss_mlm: 0.4745  loss_prd: 0.2675  loss_mrtd: 0.2482  time: 1.3070  data: 0.0002  max mem: 17763
Train Epoch: [13]  [1050/2528]  eta: 0:32:46  lr: 0.000006  loss_cl: 8.9080  loss_pitm: 0.0078  loss_mlm: 0.6085  loss_prd: 0.0744  loss_mrtd: 0.2768  time: 1.2929  data: 0.0002  max mem: 17763
Train Epoch: [13]  [1100/2528]  eta: 0:31:38  lr: 0.000006  loss_cl: 8.9151  loss_pitm: 0.0271  loss_mlm: 0.1833  loss_prd: 0.0898  loss_mrtd: 0.1950  time: 1.3023  data: 0.0002  max mem: 17763
Train Epoch: [13]  [1150/2528]  eta: 0:30:29  lr: 0.000006  loss_cl: 8.8167  loss_pitm: 0.0153  loss_mlm: 0.4074  loss_prd: 0.0518  loss_mrtd: 0.2362  time: 1.3019  data: 0.0002  max mem: 17763
Train Epoch: [13]  [1200/2528]  eta: 0:29:22  lr: 0.000006  loss_cl: 8.8734  loss_pitm: 0.0476  loss_mlm: 0.2783  loss_prd: 0.3421  loss_mrtd: 0.2029  time: 1.3046  data: 0.0002  max mem: 17763
Train Epoch: [13]  [1250/2528]  eta: 0:28:14  lr: 0.000006  loss_cl: 8.4338  loss_pitm: 0.0523  loss_mlm: 0.6870  loss_prd: 0.4096  loss_mrtd: 0.2676  time: 1.3036  data: 0.0002  max mem: 17763
Train Epoch: [13]  [1300/2528]  eta: 0:27:07  lr: 0.000006  loss_cl: 8.6693  loss_pitm: 0.0157  loss_mlm: 0.4954  loss_prd: 0.0587  loss_mrtd: 0.2084  time: 1.3070  data: 0.0002  max mem: 17763
Train Epoch: [13]  [1350/2528]  eta: 0:26:00  lr: 0.000006  loss_cl: 8.8886  loss_pitm: 0.0375  loss_mlm: 0.3389  loss_prd: 0.3214  loss_mrtd: 0.1453  time: 1.2977  data: 0.0001  max mem: 17763
Train Epoch: [13]  [1400/2528]  eta: 0:24:53  lr: 0.000006  loss_cl: 8.5566  loss_pitm: 0.0111  loss_mlm: 0.3651  loss_prd: 0.0457  loss_mrtd: 0.2382  time: 1.3021  data: 0.0002  max mem: 17763
Train Epoch: [13]  [1450/2528]  eta: 0:23:46  lr: 0.000006  loss_cl: 8.8299  loss_pitm: 0.0228  loss_mlm: 0.6790  loss_prd: 0.1801  loss_mrtd: 0.1910  time: 1.2977  data: 0.0001  max mem: 17763
Train Epoch: [13]  [1500/2528]  eta: 0:22:39  lr: 0.000006  loss_cl: 8.4315  loss_pitm: 0.1561  loss_mlm: 0.4603  loss_prd: 0.0600  loss_mrtd: 0.2387  time: 1.2878  data: 0.0001  max mem: 17763
Train Epoch: [13]  [1550/2528]  eta: 0:21:32  lr: 0.000006  loss_cl: 8.7911  loss_pitm: 0.0323  loss_mlm: 0.6942  loss_prd: 0.0614  loss_mrtd: 0.2206  time: 1.3008  data: 0.0001  max mem: 17763
Train Epoch: [13]  [1600/2528]  eta: 0:20:25  lr: 0.000006  loss_cl: 8.5450  loss_pitm: 0.1084  loss_mlm: 0.2741  loss_prd: 0.4806  loss_mrtd: 0.1970  time: 1.2923  data: 0.0001  max mem: 17763
Train Epoch: [13]  [1650/2528]  eta: 0:19:18  lr: 0.000006  loss_cl: 8.5123  loss_pitm: 0.0586  loss_mlm: 0.5734  loss_prd: 0.9651  loss_mrtd: 0.2489  time: 1.2946  data: 0.0001  max mem: 17763
Train Epoch: [13]  [1700/2528]  eta: 0:18:12  lr: 0.000006  loss_cl: 8.4787  loss_pitm: 0.0602  loss_mlm: 0.4141  loss_prd: 0.0749  loss_mrtd: 0.2479  time: 1.2997  data: 0.0001  max mem: 17763
Train Epoch: [13]  [1750/2528]  eta: 0:17:05  lr: 0.000006  loss_cl: 8.1331  loss_pitm: 0.0151  loss_mlm: 0.5557  loss_prd: 0.4828  loss_mrtd: 0.2562  time: 1.2987  data: 0.0001  max mem: 17763
Train Epoch: [13]  [1800/2528]  eta: 0:15:59  lr: 0.000006  loss_cl: 8.1188  loss_pitm: 0.0099  loss_mlm: 0.8100  loss_prd: 0.2949  loss_mrtd: 0.2109  time: 1.2968  data: 0.0001  max mem: 17763
Train Epoch: [13]  [1850/2528]  eta: 0:14:53  lr: 0.000006  loss_cl: 8.0357  loss_pitm: 0.0159  loss_mlm: 0.3226  loss_prd: 0.0712  loss_mrtd: 0.2159  time: 1.3014  data: 0.0002  max mem: 17763
Train Epoch: [13]  [1900/2528]  eta: 0:13:47  lr: 0.000006  loss_cl: 8.2502  loss_pitm: 0.0804  loss_mlm: 0.4182  loss_prd: 0.0945  loss_mrtd: 0.2114  time: 1.3023  data: 0.0002  max mem: 17763
Train Epoch: [13]  [1950/2528]  eta: 0:12:40  lr: 0.000006  loss_cl: 7.8943  loss_pitm: 0.0132  loss_mlm: 0.5898  loss_prd: 0.0468  loss_mrtd: 0.2817  time: 1.2933  data: 0.0002  max mem: 17763
Train Epoch: [13]  [2000/2528]  eta: 0:11:34  lr: 0.000006  loss_cl: 7.9627  loss_pitm: 0.1432  loss_mlm: 0.4449  loss_prd: 0.0514  loss_mrtd: 0.2271  time: 1.3043  data: 0.0002  max mem: 17763
Train Epoch: [13]  [2050/2528]  eta: 0:10:28  lr: 0.000006  loss_cl: 7.7856  loss_pitm: 0.0099  loss_mlm: 0.3981  loss_prd: 0.4264  loss_mrtd: 0.1701  time: 1.2997  data: 0.0002  max mem: 17763
Train Epoch: [13]  [2100/2528]  eta: 0:09:22  lr: 0.000006  loss_cl: 7.9329  loss_pitm: 0.0276  loss_mlm: 0.5379  loss_prd: 0.3036  loss_mrtd: 0.2232  time: 1.2944  data: 0.0002  max mem: 17763
Train Epoch: [13]  [2150/2528]  eta: 0:08:17  lr: 0.000006  loss_cl: 8.1130  loss_pitm: 0.1738  loss_mlm: 0.3662  loss_prd: 0.2608  loss_mrtd: 0.2100  time: 1.3057  data: 0.0002  max mem: 17763
Train Epoch: [13]  [2200/2528]  eta: 0:07:11  lr: 0.000006  loss_cl: 7.6370  loss_pitm: 0.0016  loss_mlm: 0.5366  loss_prd: 0.0494  loss_mrtd: 0.2266  time: 1.3028  data: 0.0002  max mem: 17763
Train Epoch: [13]  [2250/2528]  eta: 0:06:05  lr: 0.000006  loss_cl: 8.0272  loss_pitm: 0.1193  loss_mlm: 0.4605  loss_prd: 0.1998  loss_mrtd: 0.1931  time: 1.3096  data: 0.0002  max mem: 17763
Train Epoch: [13]  [2300/2528]  eta: 0:04:59  lr: 0.000006  loss_cl: 7.4684  loss_pitm: 0.0331  loss_mlm: 0.4389  loss_prd: 0.5310  loss_mrtd: 0.2532  time: 1.3112  data: 0.0003  max mem: 17763
Train Epoch: [13]  [2350/2528]  eta: 0:03:53  lr: 0.000006  loss_cl: 7.3893  loss_pitm: 0.0585  loss_mlm: 0.7625  loss_prd: 0.4989  loss_mrtd: 0.2451  time: 1.3051  data: 0.0002  max mem: 17763
Train Epoch: [13]  [2400/2528]  eta: 0:02:48  lr: 0.000006  loss_cl: 8.0072  loss_pitm: 0.0284  loss_mlm: 0.3266  loss_prd: 0.0511  loss_mrtd: 0.3011  time: 1.3136  data: 0.0003  max mem: 17763
Train Epoch: [13]  [2450/2528]  eta: 0:01:42  lr: 0.000006  loss_cl: 7.3702  loss_pitm: 0.0497  loss_mlm: 0.1533  loss_prd: 0.1181  loss_mrtd: 0.2166  time: 1.2932  data: 0.0002  max mem: 17763
Train Epoch: [13]  [2500/2528]  eta: 0:00:36  lr: 0.000006  loss_cl: 6.7184  loss_pitm: 0.1078  loss_mlm: 0.5562  loss_prd: 0.1746  loss_mrtd: 0.2693  time: 1.3208  data: 0.0003  max mem: 17763
Train Epoch: [13]  [2527/2528]  eta: 0:00:01  lr: 0.000006  loss_cl: 7.6776  loss_pitm: 0.0655  loss_mlm: 0.5514  loss_prd: 0.0696  loss_mrtd: 0.2624  time: 1.3315  data: 0.0003  max mem: 17763
Train Epoch: [13] Total time: 0:55:21 (1.3141 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.6752  loss_pitm: 0.0567  loss_mlm: 0.4609  loss_prd: 0.2227  loss_mrtd: 0.2289
Train Epoch的聚类: [14]  [   0/5241]  eta: 0:55:39    time: 0.6372  data: 0.6131  max mem: 17763
Train Epoch的聚类: [14]  [5240/5241]  eta: 0:00:00    time: 0.1711  data: 0.0002  max mem: 17763
Train Epoch的聚类: [14] Total time: 0:14:56 (0.1711 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4432
-1 (未归入任何簇) 的数量: 2223

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [14]  [   0/2534]  eta: 1:59:22  lr: 0.000006  loss_cl: 10.4579  loss_pitm: 0.0094  loss_mlm: 0.4799  loss_prd: 0.2310  loss_mrtd: 0.2696  time: 2.8266  data: 1.0845  max mem: 17763
Train Epoch: [14]  [  50/2534]  eta: 0:55:23  lr: 0.000006  loss_cl: 9.7437  loss_pitm: 0.0154  loss_mlm: 0.3863  loss_prd: 0.2500  loss_mrtd: 0.2802  time: 1.3058  data: 0.0003  max mem: 17763
Train Epoch: [14]  [ 100/2534]  eta: 0:54:03  lr: 0.000006  loss_cl: 9.9505  loss_pitm: 0.0876  loss_mlm: 0.6231  loss_prd: 0.5277  loss_mrtd: 0.1942  time: 1.3347  data: 0.0003  max mem: 17763
Train Epoch: [14]  [ 150/2534]  eta: 0:52:42  lr: 0.000006  loss_cl: 9.7144  loss_pitm: 0.1187  loss_mlm: 0.3193  loss_prd: 0.5617  loss_mrtd: 0.2198  time: 1.3072  data: 0.0004  max mem: 17763
Train Epoch: [14]  [ 200/2534]  eta: 0:51:29  lr: 0.000006  loss_cl: 9.6828  loss_pitm: 0.1643  loss_mlm: 0.3467  loss_prd: 0.3161  loss_mrtd: 0.1861  time: 1.3214  data: 0.0003  max mem: 17763
Train Epoch: [14]  [ 250/2534]  eta: 0:50:24  lr: 0.000006  loss_cl: 9.6438  loss_pitm: 0.0699  loss_mlm: 0.5384  loss_prd: 0.2661  loss_mrtd: 0.1820  time: 1.3190  data: 0.0003  max mem: 17763
Train Epoch: [14]  [ 300/2534]  eta: 0:49:16  lr: 0.000006  loss_cl: 9.4553  loss_pitm: 0.0094  loss_mlm: 0.3069  loss_prd: 0.0731  loss_mrtd: 0.1632  time: 1.3222  data: 0.0004  max mem: 17763
Train Epoch: [14]  [ 350/2534]  eta: 0:48:09  lr: 0.000006  loss_cl: 9.4821  loss_pitm: 0.0306  loss_mlm: 0.3667  loss_prd: 0.0659  loss_mrtd: 0.2195  time: 1.3195  data: 0.0003  max mem: 17763
Train Epoch: [14]  [ 400/2534]  eta: 0:47:03  lr: 0.000006  loss_cl: 9.0255  loss_pitm: 0.0992  loss_mlm: 0.4460  loss_prd: 0.3817  loss_mrtd: 0.1733  time: 1.3232  data: 0.0004  max mem: 17763
Train Epoch: [14]  [ 450/2534]  eta: 0:45:57  lr: 0.000006  loss_cl: 9.0116  loss_pitm: 0.0184  loss_mlm: 0.4046  loss_prd: 0.2918  loss_mrtd: 0.2180  time: 1.3256  data: 0.0003  max mem: 17763
Train Epoch: [14]  [ 500/2534]  eta: 0:44:51  lr: 0.000006  loss_cl: 9.5411  loss_pitm: 0.0022  loss_mlm: 0.3638  loss_prd: 0.0612  loss_mrtd: 0.2647  time: 1.3240  data: 0.0003  max mem: 17763
Train Epoch: [14]  [ 550/2534]  eta: 0:43:47  lr: 0.000006  loss_cl: 9.6700  loss_pitm: 0.0363  loss_mlm: 0.3449  loss_prd: 0.0716  loss_mrtd: 0.1720  time: 1.3413  data: 0.0004  max mem: 17763
Train Epoch: [14]  [ 600/2534]  eta: 0:42:41  lr: 0.000006  loss_cl: 9.1641  loss_pitm: 0.0300  loss_mlm: 0.7674  loss_prd: 0.3045  loss_mrtd: 0.2049  time: 1.3221  data: 0.0004  max mem: 17763
Train Epoch: [14]  [ 650/2534]  eta: 0:41:35  lr: 0.000006  loss_cl: 9.5253  loss_pitm: 0.0318  loss_mlm: 0.5851  loss_prd: 0.3085  loss_mrtd: 0.2491  time: 1.3408  data: 0.0004  max mem: 17763
Train Epoch: [14]  [ 700/2534]  eta: 0:40:29  lr: 0.000006  loss_cl: 8.9054  loss_pitm: 0.0606  loss_mlm: 0.3299  loss_prd: 0.3048  loss_mrtd: 0.2091  time: 1.3250  data: 0.0004  max mem: 17763
Train Epoch: [14]  [ 750/2534]  eta: 0:39:22  lr: 0.000006  loss_cl: 9.3716  loss_pitm: 0.0529  loss_mlm: 0.3977  loss_prd: 0.6812  loss_mrtd: 0.2745  time: 1.3184  data: 0.0003  max mem: 17763
Train Epoch: [14]  [ 800/2534]  eta: 0:38:15  lr: 0.000006  loss_cl: 9.4178  loss_pitm: 0.1169  loss_mlm: 0.3879  loss_prd: 0.0864  loss_mrtd: 0.2322  time: 1.3213  data: 0.0002  max mem: 17763
Train Epoch: [14]  [ 850/2534]  eta: 0:37:09  lr: 0.000006  loss_cl: 9.0933  loss_pitm: 0.0691  loss_mlm: 0.3952  loss_prd: 0.0558  loss_mrtd: 0.1914  time: 1.3327  data: 0.0005  max mem: 17763
Train Epoch: [14]  [ 900/2534]  eta: 0:36:04  lr: 0.000006  loss_cl: 8.8919  loss_pitm: 0.0320  loss_mlm: 0.8245  loss_prd: 0.0727  loss_mrtd: 0.2974  time: 1.3276  data: 0.0004  max mem: 17763
Train Epoch: [14]  [ 950/2534]  eta: 0:34:58  lr: 0.000006  loss_cl: 9.1246  loss_pitm: 0.0057  loss_mlm: 0.5018  loss_prd: 0.0530  loss_mrtd: 0.2370  time: 1.3396  data: 0.0006  max mem: 17772
Train Epoch: [14]  [1000/2534]  eta: 0:33:53  lr: 0.000006  loss_cl: 8.9237  loss_pitm: 0.0377  loss_mlm: 0.4700  loss_prd: 0.0593  loss_mrtd: 0.2528  time: 1.3317  data: 0.0004  max mem: 17772
Train Epoch: [14]  [1050/2534]  eta: 0:32:47  lr: 0.000006  loss_cl: 9.0978  loss_pitm: 0.0228  loss_mlm: 0.4227  loss_prd: 0.0419  loss_mrtd: 0.2475  time: 1.3278  data: 0.0005  max mem: 17772
Train Epoch: [14]  [1100/2534]  eta: 0:31:40  lr: 0.000006  loss_cl: 9.2277  loss_pitm: 0.0820  loss_mlm: 0.3995  loss_prd: 0.2429  loss_mrtd: 0.2401  time: 1.3348  data: 0.0003  max mem: 17772
Train Epoch: [14]  [1150/2534]  eta: 0:30:34  lr: 0.000006  loss_cl: 9.1158  loss_pitm: 0.0458  loss_mlm: 0.4373  loss_prd: 0.0728  loss_mrtd: 0.2536  time: 1.3316  data: 0.0003  max mem: 17772
Train Epoch: [14]  [1200/2534]  eta: 0:29:29  lr: 0.000006  loss_cl: 8.8688  loss_pitm: 0.0636  loss_mlm: 0.4672  loss_prd: 0.3324  loss_mrtd: 0.1536  time: 1.3276  data: 0.0002  max mem: 17772
Train Epoch: [14]  [1250/2534]  eta: 0:28:23  lr: 0.000006  loss_cl: 8.6283  loss_pitm: 0.0845  loss_mlm: 0.3769  loss_prd: 0.3384  loss_mrtd: 0.2504  time: 1.3290  data: 0.0003  max mem: 17772
Train Epoch: [14]  [1300/2534]  eta: 0:27:16  lr: 0.000006  loss_cl: 8.9238  loss_pitm: 0.0081  loss_mlm: 0.2512  loss_prd: 0.5099  loss_mrtd: 0.1887  time: 1.3264  data: 0.0003  max mem: 17772
Train Epoch: [14]  [1350/2534]  eta: 0:26:10  lr: 0.000006  loss_cl: 8.1437  loss_pitm: 0.0126  loss_mlm: 0.2563  loss_prd: 0.0859  loss_mrtd: 0.2104  time: 1.3341  data: 0.0003  max mem: 17772
Train Epoch: [14]  [1400/2534]  eta: 0:25:04  lr: 0.000006  loss_cl: 8.5684  loss_pitm: 0.1064  loss_mlm: 0.3546  loss_prd: 0.3082  loss_mrtd: 0.2285  time: 1.3386  data: 0.0003  max mem: 17772
Train Epoch: [14]  [1450/2534]  eta: 0:23:58  lr: 0.000006  loss_cl: 8.6698  loss_pitm: 0.0556  loss_mlm: 0.6328  loss_prd: 0.0862  loss_mrtd: 0.2509  time: 1.3248  data: 0.0003  max mem: 17772
Train Epoch: [14]  [1500/2534]  eta: 0:22:52  lr: 0.000006  loss_cl: 8.3911  loss_pitm: 0.0041  loss_mlm: 0.1269  loss_prd: 0.2920  loss_mrtd: 0.1546  time: 1.3308  data: 0.0003  max mem: 17772
Train Epoch: [14]  [1550/2534]  eta: 0:21:45  lr: 0.000006  loss_cl: 8.3415  loss_pitm: 0.0351  loss_mlm: 0.6304  loss_prd: 0.2256  loss_mrtd: 0.2678  time: 1.3275  data: 0.0003  max mem: 17772
Train Epoch: [14]  [1600/2534]  eta: 0:20:39  lr: 0.000006  loss_cl: 8.6371  loss_pitm: 0.0229  loss_mlm: 0.4311  loss_prd: 0.0562  loss_mrtd: 0.1940  time: 1.3329  data: 0.0004  max mem: 17772
Train Epoch: [14]  [1650/2534]  eta: 0:19:33  lr: 0.000006  loss_cl: 8.2358  loss_pitm: 0.0229  loss_mlm: 0.3273  loss_prd: 0.0945  loss_mrtd: 0.2532  time: 1.3423  data: 0.0005  max mem: 17772
Train Epoch: [14]  [1700/2534]  eta: 0:18:27  lr: 0.000006  loss_cl: 8.3369  loss_pitm: 0.0633  loss_mlm: 0.4663  loss_prd: 0.2801  loss_mrtd: 0.2272  time: 1.3311  data: 0.0002  max mem: 17772
Train Epoch: [14]  [1750/2534]  eta: 0:17:21  lr: 0.000006  loss_cl: 8.0063  loss_pitm: 0.0032  loss_mlm: 0.5405  loss_prd: 0.2779  loss_mrtd: 0.2573  time: 1.3416  data: 0.0004  max mem: 17772
Train Epoch: [14]  [1800/2534]  eta: 0:16:14  lr: 0.000006  loss_cl: 8.0254  loss_pitm: 0.0638  loss_mlm: 0.5025  loss_prd: 0.0990  loss_mrtd: 0.2368  time: 1.3331  data: 0.0003  max mem: 17772
Train Epoch: [14]  [1850/2534]  eta: 0:15:08  lr: 0.000006  loss_cl: 8.3287  loss_pitm: 0.0158  loss_mlm: 0.6106  loss_prd: 0.0475  loss_mrtd: 0.2144  time: 1.3318  data: 0.0003  max mem: 17772
Train Epoch: [14]  [1900/2534]  eta: 0:14:02  lr: 0.000006  loss_cl: 8.3961  loss_pitm: 0.0155  loss_mlm: 0.3337  loss_prd: 0.0706  loss_mrtd: 0.2113  time: 1.3266  data: 0.0004  max mem: 17772
Train Epoch: [14]  [1950/2534]  eta: 0:12:55  lr: 0.000006  loss_cl: 8.0390  loss_pitm: 0.0065  loss_mlm: 0.5556  loss_prd: 0.5680  loss_mrtd: 0.2292  time: 1.3272  data: 0.0004  max mem: 17772
Train Epoch: [14]  [2000/2534]  eta: 0:11:49  lr: 0.000006  loss_cl: 8.0767  loss_pitm: 0.0382  loss_mlm: 0.5289  loss_prd: 0.0624  loss_mrtd: 0.1775  time: 1.3014  data: 0.0003  max mem: 17772
Train Epoch: [14]  [2050/2534]  eta: 0:10:42  lr: 0.000006  loss_cl: 7.7927  loss_pitm: 0.0903  loss_mlm: 0.7791  loss_prd: 0.0775  loss_mrtd: 0.2482  time: 1.3208  data: 0.0003  max mem: 17772
Train Epoch: [14]  [2100/2534]  eta: 0:09:36  lr: 0.000006  loss_cl: 7.7673  loss_pitm: 0.0151  loss_mlm: 0.5029  loss_prd: 0.7008  loss_mrtd: 0.2211  time: 1.3445  data: 0.0005  max mem: 17772
Train Epoch: [14]  [2150/2534]  eta: 0:08:30  lr: 0.000006  loss_cl: 7.6609  loss_pitm: 0.1528  loss_mlm: 0.4325  loss_prd: 0.2035  loss_mrtd: 0.2005  time: 1.3316  data: 0.0004  max mem: 17772
Train Epoch: [14]  [2200/2534]  eta: 0:07:23  lr: 0.000006  loss_cl: 7.8693  loss_pitm: 0.0335  loss_mlm: 0.5200  loss_prd: 0.0545  loss_mrtd: 0.2249  time: 1.3160  data: 0.0003  max mem: 17772
Train Epoch: [14]  [2250/2534]  eta: 0:06:17  lr: 0.000006  loss_cl: 7.8782  loss_pitm: 0.0089  loss_mlm: 0.1187  loss_prd: 0.2590  loss_mrtd: 0.1764  time: 1.3275  data: 0.0003  max mem: 17772
Train Epoch: [14]  [2300/2534]  eta: 0:05:10  lr: 0.000006  loss_cl: 7.3370  loss_pitm: 0.0853  loss_mlm: 0.6351  loss_prd: 0.0559  loss_mrtd: 0.2406  time: 1.3367  data: 0.0004  max mem: 17772
Train Epoch: [14]  [2350/2534]  eta: 0:04:04  lr: 0.000006  loss_cl: 7.5663  loss_pitm: 0.0119  loss_mlm: 0.6185  loss_prd: 0.2427  loss_mrtd: 0.2612  time: 1.3301  data: 0.0004  max mem: 17772
Train Epoch: [14]  [2400/2534]  eta: 0:02:58  lr: 0.000006  loss_cl: 7.2193  loss_pitm: 0.1119  loss_mlm: 0.4552  loss_prd: 0.2202  loss_mrtd: 0.2216  time: 1.3440  data: 0.0007  max mem: 17773
Train Epoch: [14]  [2450/2534]  eta: 0:01:51  lr: 0.000006  loss_cl: 7.4042  loss_pitm: 0.0208  loss_mlm: 0.2995  loss_prd: 0.0603  loss_mrtd: 0.1503  time: 1.3259  data: 0.0003  max mem: 17773
Train Epoch: [14]  [2500/2534]  eta: 0:00:45  lr: 0.000006  loss_cl: 7.2265  loss_pitm: 0.1064  loss_mlm: 0.2442  loss_prd: 0.2866  loss_mrtd: 0.1961  time: 1.3259  data: 0.0004  max mem: 17773
Train Epoch: [14]  [2533/2534]  eta: 0:00:01  lr: 0.000006  loss_cl: 7.3946  loss_pitm: 0.0315  loss_mlm: 0.2354  loss_prd: 0.2554  loss_mrtd: 0.2011  time: 1.3136  data: 0.0004  max mem: 17773
Train Epoch: [14] Total time: 0:56:05 (1.3282 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.6657  loss_pitm: 0.0559  loss_mlm: 0.4551  loss_prd: 0.2196  loss_mrtd: 0.2282
Train Epoch的聚类: [15]  [   0/5241]  eta: 1:15:31    time: 0.8645  data: 0.8398  max mem: 17773
Train Epoch的聚类: [15]  [5240/5241]  eta: 0:00:00    time: 0.1713  data: 0.0003  max mem: 17773
Train Epoch的聚类: [15] Total time: 0:14:58 (0.1714 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4416
-1 (未归入任何簇) 的数量: 2114

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [15]  [   0/2538]  eta: 1 day, 5:27:00  lr: 0.000006  loss_cl: 9.8900  loss_pitm: 0.0174  loss_mlm: 0.1609  loss_prd: 0.0560  loss_mrtd: 0.1572  time: 41.7731  data: 1.6499  max mem: 17773
Train Epoch: [15]  [  50/2538]  eta: 1:26:51  lr: 0.000006  loss_cl: 9.3848  loss_pitm: 0.0164  loss_mlm: 0.5833  loss_prd: 0.0395  loss_mrtd: 0.2516  time: 1.3130  data: 0.0003  max mem: 17773
Train Epoch: [15]  [ 100/2538]  eta: 1:09:45  lr: 0.000006  loss_cl: 10.0022  loss_pitm: 0.0246  loss_mlm: 0.3087  loss_prd: 0.6868  loss_mrtd: 0.1931  time: 1.3338  data: 0.0004  max mem: 17773
Train Epoch: [15]  [ 150/2538]  eta: 1:03:05  lr: 0.000006  loss_cl: 9.5176  loss_pitm: 0.0192  loss_mlm: 0.4014  loss_prd: 0.4494  loss_mrtd: 0.1890  time: 1.3138  data: 0.0002  max mem: 17773
Train Epoch: [15]  [ 200/2538]  eta: 0:59:10  lr: 0.000006  loss_cl: 9.7377  loss_pitm: 0.0160  loss_mlm: 0.2713  loss_prd: 0.3127  loss_mrtd: 0.2048  time: 1.3113  data: 0.0003  max mem: 17773
Train Epoch: [15]  [ 250/2538]  eta: 0:56:21  lr: 0.000006  loss_cl: 9.7647  loss_pitm: 0.0206  loss_mlm: 0.4612  loss_prd: 0.1198  loss_mrtd: 0.1879  time: 1.3118  data: 0.0003  max mem: 17773
Train Epoch: [15]  [ 300/2538]  eta: 0:54:07  lr: 0.000006  loss_cl: 9.1234  loss_pitm: 0.0145  loss_mlm: 0.6475  loss_prd: 0.2214  loss_mrtd: 0.3085  time: 1.3237  data: 0.0002  max mem: 17773
Train Epoch: [15]  [ 350/2538]  eta: 0:52:11  lr: 0.000006  loss_cl: 9.1533  loss_pitm: 0.0526  loss_mlm: 0.4860  loss_prd: 0.0477  loss_mrtd: 0.2004  time: 1.3131  data: 0.0002  max mem: 17773
Train Epoch: [15]  [ 400/2538]  eta: 0:50:29  lr: 0.000006  loss_cl: 9.2816  loss_pitm: 0.0327  loss_mlm: 0.9877  loss_prd: 0.0654  loss_mrtd: 0.2675  time: 1.3219  data: 0.0002  max mem: 17773
Train Epoch: [15]  [ 450/2538]  eta: 0:48:54  lr: 0.000006  loss_cl: 9.6071  loss_pitm: 0.0378  loss_mlm: 0.6538  loss_prd: 0.1833  loss_mrtd: 0.2578  time: 1.3084  data: 0.0002  max mem: 17773
Train Epoch: [15]  [ 500/2538]  eta: 0:47:27  lr: 0.000006  loss_cl: 9.4529  loss_pitm: 0.0091  loss_mlm: 0.7054  loss_prd: 0.0876  loss_mrtd: 0.3261  time: 1.3340  data: 0.0002  max mem: 17773
Train Epoch: [15]  [ 550/2538]  eta: 0:46:06  lr: 0.000006  loss_cl: 9.0243  loss_pitm: 0.0088  loss_mlm: 0.5666  loss_prd: 0.3575  loss_mrtd: 0.2468  time: 1.3234  data: 0.0002  max mem: 17773
Train Epoch: [15]  [ 600/2538]  eta: 0:44:45  lr: 0.000006  loss_cl: 9.0668  loss_pitm: 0.0037  loss_mlm: 0.2281  loss_prd: 0.2921  loss_mrtd: 0.2666  time: 1.3387  data: 0.0003  max mem: 17773
Train Epoch: [15]  [ 650/2538]  eta: 0:43:27  lr: 0.000006  loss_cl: 9.3590  loss_pitm: 0.0076  loss_mlm: 0.4729  loss_prd: 0.4633  loss_mrtd: 0.2328  time: 1.3212  data: 0.0002  max mem: 17773
Train Epoch: [15]  [ 700/2538]  eta: 0:42:12  lr: 0.000006  loss_cl: 8.6472  loss_pitm: 0.0329  loss_mlm: 0.3121  loss_prd: 0.2722  loss_mrtd: 0.2343  time: 1.3334  data: 0.0002  max mem: 17773
Train Epoch: [15]  [ 750/2538]  eta: 0:40:58  lr: 0.000006  loss_cl: 9.1255  loss_pitm: 0.0059  loss_mlm: 0.5966  loss_prd: 0.1153  loss_mrtd: 0.1677  time: 1.3383  data: 0.0002  max mem: 17773
Train Epoch: [15]  [ 800/2538]  eta: 0:39:45  lr: 0.000006  loss_cl: 9.1114  loss_pitm: 0.0181  loss_mlm: 0.6289  loss_prd: 0.6630  loss_mrtd: 0.1981  time: 1.3288  data: 0.0002  max mem: 17773
Train Epoch: [15]  [ 850/2538]  eta: 0:38:31  lr: 0.000006  loss_cl: 9.2000  loss_pitm: 0.0061  loss_mlm: 0.4738  loss_prd: 0.0507  loss_mrtd: 0.1990  time: 1.3242  data: 0.0003  max mem: 17773
Train Epoch: [15]  [ 900/2538]  eta: 0:37:20  lr: 0.000006  loss_cl: 9.3017  loss_pitm: 0.0070  loss_mlm: 0.4415  loss_prd: 0.4460  loss_mrtd: 0.2355  time: 1.3285  data: 0.0002  max mem: 17773
Train Epoch: [15]  [ 950/2538]  eta: 0:36:08  lr: 0.000006  loss_cl: 9.2844  loss_pitm: 0.0234  loss_mlm: 0.7245  loss_prd: 0.4200  loss_mrtd: 0.3238  time: 1.3263  data: 0.0002  max mem: 17773
Train Epoch: [15]  [1000/2538]  eta: 0:34:56  lr: 0.000006  loss_cl: 8.9704  loss_pitm: 0.0729  loss_mlm: 0.3940  loss_prd: 0.4186  loss_mrtd: 0.2712  time: 1.3269  data: 0.0003  max mem: 17773
Train Epoch: [15]  [1050/2538]  eta: 0:33:46  lr: 0.000006  loss_cl: 8.8623  loss_pitm: 0.0069  loss_mlm: 0.2860  loss_prd: 0.0439  loss_mrtd: 0.2455  time: 1.3311  data: 0.0003  max mem: 17773
Train Epoch: [15]  [1100/2538]  eta: 0:32:35  lr: 0.000006  loss_cl: 9.0666  loss_pitm: 0.0806  loss_mlm: 0.2122  loss_prd: 0.2752  loss_mrtd: 0.1555  time: 1.3229  data: 0.0002  max mem: 17773
Train Epoch: [15]  [1150/2538]  eta: 0:31:26  lr: 0.000006  loss_cl: 8.7552  loss_pitm: 0.0044  loss_mlm: 0.2078  loss_prd: 0.0480  loss_mrtd: 0.2289  time: 1.3327  data: 0.0003  max mem: 17773
Train Epoch: [15]  [1200/2538]  eta: 0:30:17  lr: 0.000006  loss_cl: 8.2017  loss_pitm: 0.0155  loss_mlm: 0.3560  loss_prd: 0.0552  loss_mrtd: 0.1579  time: 1.3324  data: 0.0003  max mem: 17773
Train Epoch: [15]  [1250/2538]  eta: 0:29:07  lr: 0.000006  loss_cl: 8.8793  loss_pitm: 0.0160  loss_mlm: 0.5940  loss_prd: 0.2178  loss_mrtd: 0.1683  time: 1.3453  data: 0.0003  max mem: 17773
Train Epoch: [15]  [1300/2538]  eta: 0:27:58  lr: 0.000006  loss_cl: 8.7373  loss_pitm: 0.0617  loss_mlm: 0.4348  loss_prd: 0.3789  loss_mrtd: 0.2722  time: 1.3368  data: 0.0003  max mem: 17773
Train Epoch: [15]  [1350/2538]  eta: 0:26:49  lr: 0.000006  loss_cl: 8.7110  loss_pitm: 0.0210  loss_mlm: 0.2806  loss_prd: 0.1673  loss_mrtd: 0.1757  time: 1.3351  data: 0.0003  max mem: 17773
Train Epoch: [15]  [1400/2538]  eta: 0:25:41  lr: 0.000006  loss_cl: 8.5431  loss_pitm: 0.0399  loss_mlm: 0.6111  loss_prd: 0.0642  loss_mrtd: 0.1520  time: 1.3410  data: 0.0003  max mem: 17773
Train Epoch: [15]  [1450/2538]  eta: 0:24:33  lr: 0.000006  loss_cl: 8.5981  loss_pitm: 0.0188  loss_mlm: 0.2086  loss_prd: 0.0664  loss_mrtd: 0.1389  time: 1.3543  data: 0.0003  max mem: 17773
Train Epoch: [15]  [1500/2538]  eta: 0:23:24  lr: 0.000006  loss_cl: 8.4242  loss_pitm: 0.0081  loss_mlm: 0.3374  loss_prd: 0.0582  loss_mrtd: 0.1915  time: 1.3291  data: 0.0003  max mem: 17773
Train Epoch: [15]  [1550/2538]  eta: 0:22:16  lr: 0.000006  loss_cl: 8.2774  loss_pitm: 0.0844  loss_mlm: 0.5348  loss_prd: 0.1273  loss_mrtd: 0.2142  time: 1.3334  data: 0.0002  max mem: 17773
Train Epoch: [15]  [1600/2538]  eta: 0:21:08  lr: 0.000006  loss_cl: 8.2066  loss_pitm: 0.0171  loss_mlm: 0.5867  loss_prd: 0.0817  loss_mrtd: 0.2768  time: 1.3362  data: 0.0002  max mem: 17773
Train Epoch: [15]  [1650/2538]  eta: 0:19:59  lr: 0.000006  loss_cl: 8.0814  loss_pitm: 0.0032  loss_mlm: 0.3196  loss_prd: 0.0549  loss_mrtd: 0.2076  time: 1.3170  data: 0.0002  max mem: 17773
Train Epoch: [15]  [1700/2538]  eta: 0:18:51  lr: 0.000006  loss_cl: 7.9583  loss_pitm: 0.2034  loss_mlm: 0.4600  loss_prd: 0.0433  loss_mrtd: 0.2796  time: 1.3408  data: 0.0003  max mem: 17773
Train Epoch: [15]  [1750/2538]  eta: 0:17:43  lr: 0.000006  loss_cl: 8.1728  loss_pitm: 0.0198  loss_mlm: 0.5439  loss_prd: 0.4761  loss_mrtd: 0.1668  time: 1.3134  data: 0.0002  max mem: 17773
Train Epoch: [15]  [1800/2538]  eta: 0:16:35  lr: 0.000006  loss_cl: 7.9487  loss_pitm: 0.0805  loss_mlm: 0.3747  loss_prd: 0.0477  loss_mrtd: 0.2584  time: 1.3117  data: 0.0002  max mem: 17773
Train Epoch: [15]  [1850/2538]  eta: 0:15:27  lr: 0.000006  loss_cl: 8.2521  loss_pitm: 0.0101  loss_mlm: 0.4588  loss_prd: 0.1917  loss_mrtd: 0.2652  time: 1.3188  data: 0.0002  max mem: 17773
Train Epoch: [15]  [1900/2538]  eta: 0:14:19  lr: 0.000006  loss_cl: 7.7466  loss_pitm: 0.0175  loss_mlm: 0.3620  loss_prd: 0.8231  loss_mrtd: 0.2484  time: 1.3102  data: 0.0002  max mem: 17773
Train Epoch: [15]  [1950/2538]  eta: 0:13:11  lr: 0.000006  loss_cl: 7.4658  loss_pitm: 0.0065  loss_mlm: 0.4258  loss_prd: 0.3219  loss_mrtd: 0.2068  time: 1.3254  data: 0.0003  max mem: 17773
Train Epoch: [15]  [2000/2538]  eta: 0:12:03  lr: 0.000006  loss_cl: 7.5350  loss_pitm: 0.0247  loss_mlm: 0.3460  loss_prd: 0.0730  loss_mrtd: 0.2236  time: 1.3185  data: 0.0003  max mem: 17773
Train Epoch: [15]  [2050/2538]  eta: 0:10:56  lr: 0.000006  loss_cl: 7.4169  loss_pitm: 0.0051  loss_mlm: 0.3907  loss_prd: 0.2032  loss_mrtd: 0.1680  time: 1.3429  data: 0.0004  max mem: 17773
Train Epoch: [15]  [2100/2538]  eta: 0:09:48  lr: 0.000006  loss_cl: 7.6072  loss_pitm: 0.0062  loss_mlm: 0.3324  loss_prd: 0.0568  loss_mrtd: 0.2192  time: 1.3275  data: 0.0004  max mem: 17773
Train Epoch: [15]  [2150/2538]  eta: 0:08:41  lr: 0.000006  loss_cl: 7.5747  loss_pitm: 0.0048  loss_mlm: 0.6639  loss_prd: 0.0516  loss_mrtd: 0.2500  time: 1.3281  data: 0.0004  max mem: 17773
Train Epoch: [15]  [2200/2538]  eta: 0:07:34  lr: 0.000006  loss_cl: 7.7638  loss_pitm: 0.0158  loss_mlm: 0.4489  loss_prd: 0.2560  loss_mrtd: 0.2311  time: 1.3212  data: 0.0002  max mem: 17778
Train Epoch: [15]  [2250/2538]  eta: 0:06:26  lr: 0.000006  loss_cl: 7.8896  loss_pitm: 0.0413  loss_mlm: 0.6266  loss_prd: 0.0813  loss_mrtd: 0.1864  time: 1.3226  data: 0.0003  max mem: 17778
Train Epoch: [15]  [2300/2538]  eta: 0:05:19  lr: 0.000006  loss_cl: 7.6641  loss_pitm: 0.0134  loss_mlm: 0.3246  loss_prd: 0.0603  loss_mrtd: 0.2360  time: 1.3225  data: 0.0003  max mem: 17778
Train Epoch: [15]  [2350/2538]  eta: 0:04:12  lr: 0.000006  loss_cl: 7.3303  loss_pitm: 0.0486  loss_mlm: 0.5383  loss_prd: 0.2700  loss_mrtd: 0.2544  time: 1.3118  data: 0.0002  max mem: 17778
Train Epoch: [15]  [2400/2538]  eta: 0:03:05  lr: 0.000006  loss_cl: 7.1474  loss_pitm: 0.0135  loss_mlm: 0.3910  loss_prd: 0.1615  loss_mrtd: 0.2079  time: 1.3188  data: 0.0002  max mem: 17778
Train Epoch: [15]  [2450/2538]  eta: 0:01:58  lr: 0.000006  loss_cl: 7.5644  loss_pitm: 0.1982  loss_mlm: 0.5890  loss_prd: 0.0526  loss_mrtd: 0.2539  time: 1.3261  data: 0.0007  max mem: 17778
Train Epoch: [15]  [2500/2538]  eta: 0:00:50  lr: 0.000006  loss_cl: 7.3698  loss_pitm: 0.0163  loss_mlm: 0.5445  loss_prd: 0.0498  loss_mrtd: 0.2493  time: 1.3156  data: 0.0002  max mem: 17778
Train Epoch: [15]  [2537/2538]  eta: 0:00:01  lr: 0.000006  loss_cl: 7.0488  loss_pitm: 0.0473  loss_mlm: 0.4163  loss_prd: 0.3050  loss_mrtd: 0.2292  time: 1.3168  data: 0.0002  max mem: 17778
Train Epoch: [15] Total time: 0:56:42 (1.3405 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5962  loss_pitm: 0.0526  loss_mlm: 0.4497  loss_prd: 0.2165  loss_mrtd: 0.2259
Train Epoch的聚类: [16]  [   0/5241]  eta: 0:51:15    time: 0.5868  data: 0.5632  max mem: 17778
Train Epoch的聚类: [16]  [5240/5241]  eta: 0:00:00    time: 0.1714  data: 0.0002  max mem: 17778
Train Epoch的聚类: [16] Total time: 0:14:58 (0.1714 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4358
-1 (未归入任何簇) 的数量: 2042

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [16]  [   0/2541]  eta: 1 day, 2:20:55  lr: 0.000005  loss_cl: 9.9874  loss_pitm: 0.3538  loss_mlm: 0.1832  loss_prd: 0.2481  loss_mrtd: 0.1536  time: 37.3299  data: 1.5165  max mem: 17778
Train Epoch: [16]  [  50/2541]  eta: 1:23:15  lr: 0.000005  loss_cl: 9.4922  loss_pitm: 0.0133  loss_mlm: 0.8516  loss_prd: 0.1956  loss_mrtd: 0.3073  time: 1.2934  data: 0.0002  max mem: 17778
Train Epoch: [16]  [ 100/2541]  eta: 1:07:50  lr: 0.000005  loss_cl: 9.5031  loss_pitm: 0.0326  loss_mlm: 0.3767  loss_prd: 0.2895  loss_mrtd: 0.2527  time: 1.3239  data: 0.0004  max mem: 17778
Train Epoch: [16]  [ 150/2541]  eta: 1:01:59  lr: 0.000005  loss_cl: 9.9198  loss_pitm: 0.0150  loss_mlm: 0.5900  loss_prd: 0.0857  loss_mrtd: 0.2082  time: 1.3251  data: 0.0004  max mem: 17778
Train Epoch: [16]  [ 200/2541]  eta: 0:58:29  lr: 0.000005  loss_cl: 9.3979  loss_pitm: 0.0597  loss_mlm: 0.4510  loss_prd: 0.5335  loss_mrtd: 0.2682  time: 1.3194  data: 0.0005  max mem: 17778
Train Epoch: [16]  [ 250/2541]  eta: 0:55:58  lr: 0.000005  loss_cl: 9.6015  loss_pitm: 0.0341  loss_mlm: 0.4149  loss_prd: 0.3228  loss_mrtd: 0.2878  time: 1.3328  data: 0.0002  max mem: 17778
Train Epoch: [16]  [ 300/2541]  eta: 0:53:55  lr: 0.000005  loss_cl: 9.3490  loss_pitm: 0.1609  loss_mlm: 0.4304  loss_prd: 0.2905  loss_mrtd: 0.2648  time: 1.3191  data: 0.0003  max mem: 17778
Train Epoch: [16]  [ 350/2541]  eta: 0:52:08  lr: 0.000005  loss_cl: 9.6494  loss_pitm: 0.0429  loss_mlm: 0.6975  loss_prd: 0.0475  loss_mrtd: 0.2407  time: 1.3203  data: 0.0002  max mem: 17778
Train Epoch: [16]  [ 400/2541]  eta: 0:50:32  lr: 0.000005  loss_cl: 9.2739  loss_pitm: 0.0147  loss_mlm: 0.5625  loss_prd: 0.0458  loss_mrtd: 0.2087  time: 1.3266  data: 0.0002  max mem: 17778
Train Epoch: [16]  [ 450/2541]  eta: 0:49:02  lr: 0.000005  loss_cl: 9.2934  loss_pitm: 0.0534  loss_mlm: 0.5204  loss_prd: 0.1475  loss_mrtd: 0.2513  time: 1.3292  data: 0.0005  max mem: 17778
Train Epoch: [16]  [ 500/2541]  eta: 0:47:38  lr: 0.000005  loss_cl: 9.0382  loss_pitm: 0.1147  loss_mlm: 0.4664  loss_prd: 0.2516  loss_mrtd: 0.2779  time: 1.3346  data: 0.0003  max mem: 17778
Train Epoch: [16]  [ 550/2541]  eta: 0:46:18  lr: 0.000005  loss_cl: 9.1548  loss_pitm: 0.1153  loss_mlm: 0.3640  loss_prd: 0.0506  loss_mrtd: 0.2127  time: 1.3397  data: 0.0003  max mem: 17778
Train Epoch: [16]  [ 600/2541]  eta: 0:44:55  lr: 0.000005  loss_cl: 9.1607  loss_pitm: 0.0044  loss_mlm: 0.2937  loss_prd: 0.0443  loss_mrtd: 0.1721  time: 1.3118  data: 0.0003  max mem: 17778
Train Epoch: [16]  [ 650/2541]  eta: 0:43:38  lr: 0.000005  loss_cl: 8.9383  loss_pitm: 0.0031  loss_mlm: 0.6898  loss_prd: 0.0509  loss_mrtd: 0.2144  time: 1.3240  data: 0.0002  max mem: 17778
Train Epoch: [16]  [ 700/2541]  eta: 0:42:20  lr: 0.000005  loss_cl: 9.0539  loss_pitm: 0.0897  loss_mlm: 0.5959  loss_prd: 0.0669  loss_mrtd: 0.2272  time: 1.3147  data: 0.0002  max mem: 17778
Train Epoch: [16]  [ 750/2541]  eta: 0:41:05  lr: 0.000005  loss_cl: 9.5585  loss_pitm: 0.0136  loss_mlm: 0.5177  loss_prd: 0.0598  loss_mrtd: 0.1666  time: 1.3289  data: 0.0004  max mem: 17778
Train Epoch: [16]  [ 800/2541]  eta: 0:39:50  lr: 0.000005  loss_cl: 8.7848  loss_pitm: 0.0071  loss_mlm: 0.3351  loss_prd: 0.0706  loss_mrtd: 0.1807  time: 1.3183  data: 0.0002  max mem: 17778
Train Epoch: [16]  [ 850/2541]  eta: 0:38:36  lr: 0.000005  loss_cl: 8.2987  loss_pitm: 0.0069  loss_mlm: 0.5228  loss_prd: 0.6788  loss_mrtd: 0.2205  time: 1.3148  data: 0.0002  max mem: 17778
Train Epoch: [16]  [ 900/2541]  eta: 0:37:22  lr: 0.000005  loss_cl: 8.9481  loss_pitm: 0.1014  loss_mlm: 0.3242  loss_prd: 0.3713  loss_mrtd: 0.2308  time: 1.3110  data: 0.0002  max mem: 17778
Train Epoch: [16]  [ 950/2541]  eta: 0:36:10  lr: 0.000005  loss_cl: 8.6677  loss_pitm: 0.0204  loss_mlm: 0.4766  loss_prd: 0.2710  loss_mrtd: 0.2072  time: 1.3157  data: 0.0002  max mem: 17778
Train Epoch: [16]  [1000/2541]  eta: 0:34:58  lr: 0.000005  loss_cl: 9.2823  loss_pitm: 0.0790  loss_mlm: 0.6570  loss_prd: 0.0539  loss_mrtd: 0.2591  time: 1.3033  data: 0.0002  max mem: 17778
Train Epoch: [16]  [1050/2541]  eta: 0:33:46  lr: 0.000005  loss_cl: 8.6851  loss_pitm: 0.0643  loss_mlm: 0.4371  loss_prd: 0.0614  loss_mrtd: 0.2633  time: 1.3058  data: 0.0002  max mem: 17778
Train Epoch: [16]  [1100/2541]  eta: 0:32:34  lr: 0.000005  loss_cl: 8.9239  loss_pitm: 0.0459  loss_mlm: 0.6585  loss_prd: 0.0928  loss_mrtd: 0.2447  time: 1.2996  data: 0.0001  max mem: 17778
Train Epoch: [16]  [1150/2541]  eta: 0:31:23  lr: 0.000005  loss_cl: 8.7452  loss_pitm: 0.0890  loss_mlm: 0.1620  loss_prd: 0.2928  loss_mrtd: 0.2098  time: 1.3013  data: 0.0001  max mem: 17778
Train Epoch: [16]  [1200/2541]  eta: 0:30:13  lr: 0.000005  loss_cl: 7.9964  loss_pitm: 0.0184  loss_mlm: 0.5883  loss_prd: 0.3195  loss_mrtd: 0.2049  time: 1.3088  data: 0.0002  max mem: 17778
Train Epoch: [16]  [1250/2541]  eta: 0:29:03  lr: 0.000005  loss_cl: 8.9138  loss_pitm: 0.0159  loss_mlm: 0.4181  loss_prd: 0.4312  loss_mrtd: 0.1893  time: 1.3061  data: 0.0002  max mem: 17778
Train Epoch: [16]  [1300/2541]  eta: 0:27:53  lr: 0.000005  loss_cl: 8.5541  loss_pitm: 0.0360  loss_mlm: 0.3939  loss_prd: 0.2796  loss_mrtd: 0.1964  time: 1.2944  data: 0.0001  max mem: 17778
Train Epoch: [16]  [1350/2541]  eta: 0:26:44  lr: 0.000005  loss_cl: 8.6686  loss_pitm: 0.0112  loss_mlm: 0.5459  loss_prd: 0.0522  loss_mrtd: 0.2081  time: 1.3042  data: 0.0001  max mem: 17778
Train Epoch: [16]  [1400/2541]  eta: 0:25:35  lr: 0.000005  loss_cl: 8.6413  loss_pitm: 0.0043  loss_mlm: 0.5992  loss_prd: 0.0443  loss_mrtd: 0.2877  time: 1.3016  data: 0.0002  max mem: 17778
Train Epoch: [16]  [1450/2541]  eta: 0:24:26  lr: 0.000005  loss_cl: 8.5374  loss_pitm: 0.0225  loss_mlm: 0.5611  loss_prd: 0.0741  loss_mrtd: 0.1625  time: 1.3110  data: 0.0002  max mem: 17778
Train Epoch: [16]  [1500/2541]  eta: 0:23:17  lr: 0.000005  loss_cl: 8.3065  loss_pitm: 0.0578  loss_mlm: 0.4376  loss_prd: 0.0392  loss_mrtd: 0.2409  time: 1.3094  data: 0.0002  max mem: 17778
Train Epoch: [16]  [1550/2541]  eta: 0:22:09  lr: 0.000005  loss_cl: 8.5925  loss_pitm: 0.0585  loss_mlm: 0.3712  loss_prd: 0.3305  loss_mrtd: 0.1673  time: 1.3124  data: 0.0002  max mem: 17778
Train Epoch: [16]  [1600/2541]  eta: 0:21:01  lr: 0.000005  loss_cl: 8.4192  loss_pitm: 0.0091  loss_mlm: 0.2222  loss_prd: 0.2938  loss_mrtd: 0.1379  time: 1.3142  data: 0.0002  max mem: 17778
Train Epoch: [16]  [1650/2541]  eta: 0:19:53  lr: 0.000005  loss_cl: 7.9564  loss_pitm: 0.0090  loss_mlm: 0.4625  loss_prd: 0.0517  loss_mrtd: 0.2245  time: 1.2949  data: 0.0002  max mem: 17778
Train Epoch: [16]  [1700/2541]  eta: 0:18:45  lr: 0.000005  loss_cl: 8.2032  loss_pitm: 0.0049  loss_mlm: 0.2088  loss_prd: 0.3242  loss_mrtd: 0.1373  time: 1.3064  data: 0.0002  max mem: 17778
Train Epoch: [16]  [1750/2541]  eta: 0:17:37  lr: 0.000005  loss_cl: 8.0203  loss_pitm: 0.0153  loss_mlm: 0.3897  loss_prd: 0.3043  loss_mrtd: 0.1594  time: 1.2998  data: 0.0001  max mem: 17778
Train Epoch: [16]  [1800/2541]  eta: 0:16:30  lr: 0.000005  loss_cl: 8.2380  loss_pitm: 0.0119  loss_mlm: 0.5340  loss_prd: 0.0502  loss_mrtd: 0.2266  time: 1.3068  data: 0.0001  max mem: 17778
Train Epoch: [16]  [1850/2541]  eta: 0:15:22  lr: 0.000005  loss_cl: 8.5255  loss_pitm: 0.0571  loss_mlm: 0.5165  loss_prd: 0.0692  loss_mrtd: 0.2168  time: 1.3016  data: 0.0001  max mem: 17778
Train Epoch: [16]  [1900/2541]  eta: 0:14:15  lr: 0.000005  loss_cl: 8.2900  loss_pitm: 0.0933  loss_mlm: 0.3748  loss_prd: 0.1280  loss_mrtd: 0.2512  time: 1.3020  data: 0.0001  max mem: 17778
Train Epoch: [16]  [1950/2541]  eta: 0:13:08  lr: 0.000005  loss_cl: 8.0791  loss_pitm: 0.1093  loss_mlm: 0.6870  loss_prd: 0.1593  loss_mrtd: 0.2039  time: 1.3043  data: 0.0001  max mem: 17778
Train Epoch: [16]  [2000/2541]  eta: 0:12:01  lr: 0.000005  loss_cl: 8.0103  loss_pitm: 0.1650  loss_mlm: 0.3962  loss_prd: 0.0537  loss_mrtd: 0.2525  time: 1.3051  data: 0.0001  max mem: 17778
Train Epoch: [16]  [2050/2541]  eta: 0:10:54  lr: 0.000005  loss_cl: 8.0466  loss_pitm: 0.0297  loss_mlm: 0.6345  loss_prd: 0.0453  loss_mrtd: 0.2000  time: 1.3167  data: 0.0001  max mem: 17778
Train Epoch: [16]  [2100/2541]  eta: 0:09:47  lr: 0.000005  loss_cl: 7.8792  loss_pitm: 0.0133  loss_mlm: 0.2955  loss_prd: 0.0697  loss_mrtd: 0.2884  time: 1.3002  data: 0.0001  max mem: 17778
Train Epoch: [16]  [2150/2541]  eta: 0:08:40  lr: 0.000005  loss_cl: 7.2299  loss_pitm: 0.0114  loss_mlm: 0.5036  loss_prd: 0.0579  loss_mrtd: 0.2526  time: 1.3063  data: 0.0002  max mem: 17778
Train Epoch: [16]  [2200/2541]  eta: 0:07:33  lr: 0.000005  loss_cl: 8.1615  loss_pitm: 0.0113  loss_mlm: 0.6266  loss_prd: 0.0985  loss_mrtd: 0.2222  time: 1.3144  data: 0.0001  max mem: 17778
Train Epoch: [16]  [2250/2541]  eta: 0:06:27  lr: 0.000005  loss_cl: 7.3752  loss_pitm: 0.1875  loss_mlm: 0.4598  loss_prd: 0.3162  loss_mrtd: 0.2929  time: 1.3053  data: 0.0001  max mem: 17778
Train Epoch: [16]  [2300/2541]  eta: 0:05:20  lr: 0.000005  loss_cl: 7.5961  loss_pitm: 0.2676  loss_mlm: 0.1982  loss_prd: 0.0558  loss_mrtd: 0.2067  time: 1.2952  data: 0.0001  max mem: 17778
Train Epoch: [16]  [2350/2541]  eta: 0:04:13  lr: 0.000005  loss_cl: 7.6103  loss_pitm: 0.0062  loss_mlm: 0.2893  loss_prd: 0.0550  loss_mrtd: 0.2789  time: 1.3064  data: 0.0001  max mem: 17778
Train Epoch: [16]  [2400/2541]  eta: 0:03:07  lr: 0.000005  loss_cl: 7.4219  loss_pitm: 0.2505  loss_mlm: 0.4207  loss_prd: 0.6656  loss_mrtd: 0.2386  time: 1.3102  data: 0.0001  max mem: 17778
Train Epoch: [16]  [2450/2541]  eta: 0:02:00  lr: 0.000005  loss_cl: 7.3791  loss_pitm: 0.0172  loss_mlm: 0.3710  loss_prd: 0.2812  loss_mrtd: 0.2416  time: 1.3071  data: 0.0001  max mem: 17778
Train Epoch: [16]  [2500/2541]  eta: 0:00:54  lr: 0.000005  loss_cl: 7.2305  loss_pitm: 0.0183  loss_mlm: 0.2820  loss_prd: 0.0555  loss_mrtd: 0.1953  time: 1.3060  data: 0.0001  max mem: 17778
Train Epoch: [16]  [2540/2541]  eta: 0:00:01  lr: 0.000005  loss_cl: 7.3283  loss_pitm: 0.2817  loss_mlm: 0.2337  loss_prd: 0.0625  loss_mrtd: 0.1117  time: 1.3089  data: 0.0002  max mem: 17778
Train Epoch: [16] Total time: 0:56:14 (1.3280 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5945  loss_pitm: 0.0513  loss_mlm: 0.4476  loss_prd: 0.2145  loss_mrtd: 0.2252
Train Epoch的聚类: [17]  [   0/5241]  eta: 0:48:54    time: 0.5598  data: 0.5386  max mem: 17778
Train Epoch的聚类: [17]  [5240/5241]  eta: 0:00:00    time: 0.1719  data: 0.0002  max mem: 17778
Train Epoch的聚类: [17] Total time: 0:15:01 (0.1721 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4342
-1 (未归入任何簇) 的数量: 1956

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [17]  [   0/2545]  eta: 1 day, 9:38:08  lr: 0.000005  loss_cl: 10.0862  loss_pitm: 0.0763  loss_mlm: 0.7611  loss_prd: 0.0384  loss_mrtd: 0.2370  time: 47.5790  data: 0.5432  max mem: 17778
Train Epoch: [17]  [  50/2545]  eta: 1:30:45  lr: 0.000005  loss_cl: 9.7829  loss_pitm: 0.0329  loss_mlm: 0.5256  loss_prd: 0.5811  loss_mrtd: 0.1997  time: 1.2804  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 100/2545]  eta: 1:11:06  lr: 0.000005  loss_cl: 9.7283  loss_pitm: 0.0840  loss_mlm: 0.3070  loss_prd: 0.2821  loss_mrtd: 0.1750  time: 1.2970  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 150/2545]  eta: 1:03:50  lr: 0.000005  loss_cl: 9.6933  loss_pitm: 0.0398  loss_mlm: 0.1727  loss_prd: 0.2947  loss_mrtd: 0.2191  time: 1.3095  data: 0.0001  max mem: 17778
Train Epoch: [17]  [ 200/2545]  eta: 0:59:40  lr: 0.000005  loss_cl: 9.9833  loss_pitm: 0.0245  loss_mlm: 0.6256  loss_prd: 0.0516  loss_mrtd: 0.2102  time: 1.3079  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 250/2545]  eta: 0:56:43  lr: 0.000005  loss_cl: 9.4433  loss_pitm: 0.2229  loss_mlm: 0.2427  loss_prd: 0.2863  loss_mrtd: 0.2770  time: 1.3061  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 300/2545]  eta: 0:54:24  lr: 0.000005  loss_cl: 9.2911  loss_pitm: 0.0060  loss_mlm: 0.1725  loss_prd: 0.3107  loss_mrtd: 0.1639  time: 1.3117  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 350/2545]  eta: 0:52:28  lr: 0.000005  loss_cl: 9.5664  loss_pitm: 0.0088  loss_mlm: 0.5362  loss_prd: 0.0521  loss_mrtd: 0.2395  time: 1.3172  data: 0.0001  max mem: 17778
Train Epoch: [17]  [ 400/2545]  eta: 0:50:43  lr: 0.000005  loss_cl: 9.3793  loss_pitm: 0.0073  loss_mlm: 0.3910  loss_prd: 0.0513  loss_mrtd: 0.2001  time: 1.3003  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 450/2545]  eta: 0:49:09  lr: 0.000005  loss_cl: 9.1677  loss_pitm: 0.0063  loss_mlm: 0.3968  loss_prd: 0.6446  loss_mrtd: 0.2195  time: 1.3186  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 500/2545]  eta: 0:47:39  lr: 0.000005  loss_cl: 9.4692  loss_pitm: 0.1187  loss_mlm: 0.4551  loss_prd: 0.0486  loss_mrtd: 0.2591  time: 1.3223  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 550/2545]  eta: 0:46:15  lr: 0.000005  loss_cl: 9.5205  loss_pitm: 0.0669  loss_mlm: 0.2355  loss_prd: 0.0545  loss_mrtd: 0.1723  time: 1.3194  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 600/2545]  eta: 0:44:53  lr: 0.000005  loss_cl: 9.3205  loss_pitm: 0.0853  loss_mlm: 0.3904  loss_prd: 0.2978  loss_mrtd: 0.1490  time: 1.3173  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 650/2545]  eta: 0:43:34  lr: 0.000005  loss_cl: 9.5700  loss_pitm: 0.0197  loss_mlm: 0.3495  loss_prd: 0.0959  loss_mrtd: 0.2227  time: 1.3144  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 700/2545]  eta: 0:42:15  lr: 0.000005  loss_cl: 9.6913  loss_pitm: 0.0075  loss_mlm: 0.4363  loss_prd: 0.0383  loss_mrtd: 0.2950  time: 1.3079  data: 0.0001  max mem: 17778
Train Epoch: [17]  [ 750/2545]  eta: 0:40:58  lr: 0.000005  loss_cl: 9.3686  loss_pitm: 0.0103  loss_mlm: 0.2949  loss_prd: 0.0680  loss_mrtd: 0.1915  time: 1.3061  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 800/2545]  eta: 0:39:43  lr: 0.000005  loss_cl: 8.9677  loss_pitm: 0.1193  loss_mlm: 0.7395  loss_prd: 0.1363  loss_mrtd: 0.2335  time: 1.3035  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 850/2545]  eta: 0:38:28  lr: 0.000005  loss_cl: 8.6316  loss_pitm: 0.0047  loss_mlm: 0.4995  loss_prd: 0.0549  loss_mrtd: 0.2481  time: 1.3030  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 900/2545]  eta: 0:37:15  lr: 0.000005  loss_cl: 9.2903  loss_pitm: 0.0064  loss_mlm: 0.3685  loss_prd: 0.2685  loss_mrtd: 0.2171  time: 1.3086  data: 0.0002  max mem: 17778
Train Epoch: [17]  [ 950/2545]  eta: 0:36:03  lr: 0.000005  loss_cl: 9.3910  loss_pitm: 0.0115  loss_mlm: 0.4343  loss_prd: 0.0471  loss_mrtd: 0.1230  time: 1.3086  data: 0.0002  max mem: 17778
Train Epoch: [17]  [1000/2545]  eta: 0:34:51  lr: 0.000005  loss_cl: 9.0062  loss_pitm: 0.1564  loss_mlm: 0.3785  loss_prd: 0.2812  loss_mrtd: 0.1672  time: 1.3028  data: 0.0001  max mem: 17778
Train Epoch: [17]  [1050/2545]  eta: 0:33:40  lr: 0.000005  loss_cl: 8.7732  loss_pitm: 0.0284  loss_mlm: 0.2490  loss_prd: 0.1458  loss_mrtd: 0.1783  time: 1.3153  data: 0.0002  max mem: 17778
Train Epoch: [17]  [1100/2545]  eta: 0:32:30  lr: 0.000005  loss_cl: 8.9801  loss_pitm: 0.0634  loss_mlm: 0.8511  loss_prd: 0.0793  loss_mrtd: 0.3053  time: 1.3088  data: 0.0002  max mem: 17778
Train Epoch: [17]  [1150/2545]  eta: 0:31:21  lr: 0.000005  loss_cl: 8.4272  loss_pitm: 0.0188  loss_mlm: 0.5407  loss_prd: 0.1846  loss_mrtd: 0.2226  time: 1.3103  data: 0.0001  max mem: 17778
Train Epoch: [17]  [1200/2545]  eta: 0:30:11  lr: 0.000005  loss_cl: 8.8685  loss_pitm: 0.1183  loss_mlm: 0.5385  loss_prd: 0.2061  loss_mrtd: 0.2856  time: 1.3127  data: 0.0001  max mem: 17778
Train Epoch: [17]  [1250/2545]  eta: 0:29:02  lr: 0.000005  loss_cl: 8.4961  loss_pitm: 0.0831  loss_mlm: 0.6194  loss_prd: 0.0561  loss_mrtd: 0.2568  time: 1.3045  data: 0.0002  max mem: 17778
Train Epoch: [17]  [1300/2545]  eta: 0:27:53  lr: 0.000005  loss_cl: 8.2940  loss_pitm: 0.1144  loss_mlm: 0.4988  loss_prd: 0.3073  loss_mrtd: 0.2600  time: 1.3154  data: 0.0002  max mem: 17778
Train Epoch: [17]  [1350/2545]  eta: 0:26:44  lr: 0.000005  loss_cl: 8.3434  loss_pitm: 0.0406  loss_mlm: 0.5118  loss_prd: 0.0886  loss_mrtd: 0.1918  time: 1.3197  data: 0.0002  max mem: 17778
Train Epoch: [17]  [1400/2545]  eta: 0:25:35  lr: 0.000005  loss_cl: 8.4893  loss_pitm: 0.0231  loss_mlm: 0.4420  loss_prd: 0.0675  loss_mrtd: 0.2360  time: 1.2885  data: 0.0002  max mem: 17778
Train Epoch: [17]  [1450/2545]  eta: 0:24:27  lr: 0.000005  loss_cl: 8.4782  loss_pitm: 0.1288  loss_mlm: 0.4001  loss_prd: 0.4216  loss_mrtd: 0.2099  time: 1.3062  data: 0.0001  max mem: 17778
Train Epoch: [17]  [1500/2545]  eta: 0:23:18  lr: 0.000005  loss_cl: 8.8986  loss_pitm: 0.0134  loss_mlm: 0.7964  loss_prd: 0.0505  loss_mrtd: 0.2823  time: 1.3087  data: 0.0001  max mem: 17778
Train Epoch: [17]  [1550/2545]  eta: 0:22:10  lr: 0.000005  loss_cl: 8.2507  loss_pitm: 0.0543  loss_mlm: 0.4633  loss_prd: 0.2569  loss_mrtd: 0.1547  time: 1.2945  data: 0.0002  max mem: 17778
Train Epoch: [17]  [1600/2545]  eta: 0:21:02  lr: 0.000005  loss_cl: 8.3004  loss_pitm: 0.0839  loss_mlm: 0.6043  loss_prd: 0.2872  loss_mrtd: 0.1961  time: 1.3015  data: 0.0002  max mem: 17778
Train Epoch: [17]  [1650/2545]  eta: 0:19:55  lr: 0.000005  loss_cl: 8.2344  loss_pitm: 0.0168  loss_mlm: 0.4856  loss_prd: 0.0691  loss_mrtd: 0.2230  time: 1.3026  data: 0.0002  max mem: 17778
Train Epoch: [17]  [1700/2545]  eta: 0:18:47  lr: 0.000005  loss_cl: 8.2596  loss_pitm: 0.0318  loss_mlm: 0.5733  loss_prd: 0.7672  loss_mrtd: 0.2772  time: 1.3039  data: 0.0001  max mem: 17778
Train Epoch: [17]  [1750/2545]  eta: 0:17:40  lr: 0.000005  loss_cl: 8.3915  loss_pitm: 0.0210  loss_mlm: 0.7114  loss_prd: 0.0587  loss_mrtd: 0.2598  time: 1.3025  data: 0.0001  max mem: 17778
Train Epoch: [17]  [1800/2545]  eta: 0:16:33  lr: 0.000005  loss_cl: 8.1030  loss_pitm: 0.1079  loss_mlm: 0.3093  loss_prd: 0.0488  loss_mrtd: 0.1615  time: 1.2859  data: 0.0001  max mem: 17778
Train Epoch: [17]  [1850/2545]  eta: 0:15:25  lr: 0.000005  loss_cl: 8.0123  loss_pitm: 0.0131  loss_mlm: 0.5025  loss_prd: 0.0582  loss_mrtd: 0.1432  time: 1.3007  data: 0.0001  max mem: 17778
Train Epoch: [17]  [1900/2545]  eta: 0:14:18  lr: 0.000005  loss_cl: 7.5876  loss_pitm: 0.0050  loss_mlm: 0.2786  loss_prd: 0.0558  loss_mrtd: 0.2561  time: 1.3017  data: 0.0001  max mem: 17778
Train Epoch: [17]  [1950/2545]  eta: 0:13:11  lr: 0.000005  loss_cl: 7.9016  loss_pitm: 0.2394  loss_mlm: 0.7503  loss_prd: 0.4350  loss_mrtd: 0.2562  time: 1.3083  data: 0.0001  max mem: 17778
Train Epoch: [17]  [2000/2545]  eta: 0:12:04  lr: 0.000005  loss_cl: 7.7276  loss_pitm: 0.0345  loss_mlm: 0.2301  loss_prd: 0.0550  loss_mrtd: 0.2299  time: 1.2870  data: 0.0001  max mem: 17778
Train Epoch: [17]  [2050/2545]  eta: 0:10:57  lr: 0.000005  loss_cl: 7.9597  loss_pitm: 0.0066  loss_mlm: 0.3544  loss_prd: 0.0665  loss_mrtd: 0.2181  time: 1.2912  data: 0.0001  max mem: 17778
Train Epoch: [17]  [2100/2545]  eta: 0:09:51  lr: 0.000005  loss_cl: 8.1896  loss_pitm: 0.0073  loss_mlm: 0.5215  loss_prd: 0.2738  loss_mrtd: 0.2858  time: 1.3045  data: 0.0001  max mem: 17778
Train Epoch: [17]  [2150/2545]  eta: 0:08:44  lr: 0.000005  loss_cl: 7.7407  loss_pitm: 0.0158  loss_mlm: 0.5658  loss_prd: 0.2584  loss_mrtd: 0.2226  time: 1.3064  data: 0.0001  max mem: 17778
Train Epoch: [17]  [2200/2545]  eta: 0:07:38  lr: 0.000005  loss_cl: 6.7737  loss_pitm: 0.0059  loss_mlm: 0.4963  loss_prd: 0.2324  loss_mrtd: 0.2704  time: 1.3155  data: 0.0001  max mem: 17778
Train Epoch: [17]  [2250/2545]  eta: 0:06:31  lr: 0.000005  loss_cl: 7.8032  loss_pitm: 0.0586  loss_mlm: 0.2078  loss_prd: 0.2921  loss_mrtd: 0.1941  time: 1.3072  data: 0.0001  max mem: 17778
Train Epoch: [17]  [2300/2545]  eta: 0:05:25  lr: 0.000005  loss_cl: 7.5593  loss_pitm: 0.0722  loss_mlm: 0.5106  loss_prd: 0.0624  loss_mrtd: 0.2808  time: 1.3088  data: 0.0001  max mem: 17778
Train Epoch: [17]  [2350/2545]  eta: 0:04:18  lr: 0.000005  loss_cl: 7.7832  loss_pitm: 0.0080  loss_mlm: 0.3041  loss_prd: 0.2892  loss_mrtd: 0.2461  time: 1.3093  data: 0.0001  max mem: 17778
Train Epoch: [17]  [2400/2545]  eta: 0:03:12  lr: 0.000005  loss_cl: 7.2688  loss_pitm: 0.0032  loss_mlm: 0.3075  loss_prd: 0.0541  loss_mrtd: 0.2853  time: 1.2868  data: 0.0001  max mem: 17778
Train Epoch: [17]  [2450/2545]  eta: 0:02:05  lr: 0.000005  loss_cl: 7.5496  loss_pitm: 0.1139  loss_mlm: 0.4294  loss_prd: 0.2628  loss_mrtd: 0.2280  time: 1.2939  data: 0.0001  max mem: 17778
Train Epoch: [17]  [2500/2545]  eta: 0:00:59  lr: 0.000005  loss_cl: 7.2827  loss_pitm: 0.0331  loss_mlm: 0.5815  loss_prd: 0.0850  loss_mrtd: 0.2912  time: 1.3088  data: 0.0001  max mem: 17778
Train Epoch: [17]  [2544/2545]  eta: 0:00:01  lr: 0.000005  loss_cl: 7.5299  loss_pitm: 0.3231  loss_mlm: 0.2372  loss_prd: 0.5037  loss_mrtd: 0.2101  time: 1.3020  data: 0.0002  max mem: 17778
Train Epoch: [17] Total time: 0:56:11 (1.3246 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.6176  loss_pitm: 0.0507  loss_mlm: 0.4399  loss_prd: 0.2115  loss_mrtd: 0.2227
Train Epoch的聚类: [18]  [   0/5241]  eta: 0:44:34    time: 0.5103  data: 0.4928  max mem: 17778
Train Epoch的聚类: [18]  [5240/5241]  eta: 0:00:00    time: 0.1720  data: 0.0002  max mem: 17778
Train Epoch的聚类: [18] Total time: 0:15:01 (0.1720 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4301
-1 (未归入任何簇) 的数量: 1996

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [18]  [   0/2543]  eta: 1 day, 0:21:58  lr: 0.000004  loss_cl: 10.0840  loss_pitm: 0.1441  loss_mlm: 0.8167  loss_prd: 0.0613  loss_mrtd: 0.2214  time: 34.4940  data: 0.4816  max mem: 17778
Train Epoch: [18]  [  50/2543]  eta: 1:19:58  lr: 0.000004  loss_cl: 9.8528  loss_pitm: 0.0393  loss_mlm: 0.7107  loss_prd: 0.0692  loss_mrtd: 0.2482  time: 1.2791  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 100/2543]  eta: 1:05:44  lr: 0.000004  loss_cl: 9.6919  loss_pitm: 0.0472  loss_mlm: 0.4453  loss_prd: 0.3019  loss_mrtd: 0.2095  time: 1.2926  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 150/2543]  eta: 1:00:30  lr: 0.000004  loss_cl: 9.6251  loss_pitm: 0.0092  loss_mlm: 0.5348  loss_prd: 0.0681  loss_mrtd: 0.2373  time: 1.3319  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 200/2543]  eta: 0:57:09  lr: 0.000004  loss_cl: 9.1734  loss_pitm: 0.0096  loss_mlm: 0.3560  loss_prd: 0.3021  loss_mrtd: 0.2233  time: 1.3059  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 250/2543]  eta: 0:54:41  lr: 0.000004  loss_cl: 9.2516  loss_pitm: 0.0545  loss_mlm: 0.4465  loss_prd: 0.2820  loss_mrtd: 0.1654  time: 1.2925  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 300/2543]  eta: 0:52:43  lr: 0.000004  loss_cl: 9.7795  loss_pitm: 0.0042  loss_mlm: 0.3829  loss_prd: 0.2472  loss_mrtd: 0.2443  time: 1.3073  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 350/2543]  eta: 0:50:59  lr: 0.000004  loss_cl: 9.2214  loss_pitm: 0.1346  loss_mlm: 0.2305  loss_prd: 0.0689  loss_mrtd: 0.1676  time: 1.3041  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 400/2543]  eta: 0:49:24  lr: 0.000004  loss_cl: 8.8535  loss_pitm: 0.0960  loss_mlm: 0.3258  loss_prd: 0.0446  loss_mrtd: 0.2447  time: 1.2990  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 450/2543]  eta: 0:47:56  lr: 0.000004  loss_cl: 10.0422  loss_pitm: 0.0115  loss_mlm: 0.1854  loss_prd: 0.2869  loss_mrtd: 0.1540  time: 1.3012  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 500/2543]  eta: 0:46:33  lr: 0.000004  loss_cl: 9.0532  loss_pitm: 0.0426  loss_mlm: 0.6066  loss_prd: 0.4319  loss_mrtd: 0.2378  time: 1.3104  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 550/2543]  eta: 0:45:14  lr: 0.000004  loss_cl: 9.1078  loss_pitm: 0.0158  loss_mlm: 0.3738  loss_prd: 0.0736  loss_mrtd: 0.2274  time: 1.3075  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 600/2543]  eta: 0:43:56  lr: 0.000004  loss_cl: 8.9580  loss_pitm: 0.1493  loss_mlm: 0.3477  loss_prd: 0.0458  loss_mrtd: 0.2172  time: 1.2950  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 650/2543]  eta: 0:42:40  lr: 0.000004  loss_cl: 9.2980  loss_pitm: 0.0437  loss_mlm: 0.5218  loss_prd: 0.2796  loss_mrtd: 0.2518  time: 1.3072  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 700/2543]  eta: 0:41:26  lr: 0.000004  loss_cl: 9.3935  loss_pitm: 0.0073  loss_mlm: 0.3518  loss_prd: 0.5270  loss_mrtd: 0.1813  time: 1.2902  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 750/2543]  eta: 0:40:13  lr: 0.000004  loss_cl: 8.6787  loss_pitm: 0.0636  loss_mlm: 0.1954  loss_prd: 0.0910  loss_mrtd: 0.1613  time: 1.2982  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 800/2543]  eta: 0:39:01  lr: 0.000004  loss_cl: 8.5752  loss_pitm: 0.0113  loss_mlm: 0.5171  loss_prd: 0.2995  loss_mrtd: 0.2204  time: 1.2920  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 850/2543]  eta: 0:37:50  lr: 0.000004  loss_cl: 8.8024  loss_pitm: 0.0104  loss_mlm: 0.2683  loss_prd: 0.0699  loss_mrtd: 0.2438  time: 1.3021  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 900/2543]  eta: 0:36:39  lr: 0.000004  loss_cl: 8.8478  loss_pitm: 0.0266  loss_mlm: 0.4281  loss_prd: 0.2567  loss_mrtd: 0.1850  time: 1.2998  data: 0.0001  max mem: 17778
Train Epoch: [18]  [ 950/2543]  eta: 0:35:29  lr: 0.000004  loss_cl: 8.5791  loss_pitm: 0.0263  loss_mlm: 0.4065  loss_prd: 0.3590  loss_mrtd: 0.2401  time: 1.3046  data: 0.0001  max mem: 17778
Train Epoch: [18]  [1000/2543]  eta: 0:34:20  lr: 0.000004  loss_cl: 8.3628  loss_pitm: 0.0063  loss_mlm: 0.6993  loss_prd: 0.0531  loss_mrtd: 0.2438  time: 1.3207  data: 0.0001  max mem: 17778
Train Epoch: [18]  [1050/2543]  eta: 0:33:11  lr: 0.000004  loss_cl: 8.6196  loss_pitm: 0.1932  loss_mlm: 0.4774  loss_prd: 0.2981  loss_mrtd: 0.1854  time: 1.3154  data: 0.0001  max mem: 17778
Train Epoch: [18]  [1100/2543]  eta: 0:32:03  lr: 0.000004  loss_cl: 8.6585  loss_pitm: 0.0105  loss_mlm: 0.2658  loss_prd: 0.0435  loss_mrtd: 0.1910  time: 1.3205  data: 0.0001  max mem: 17778
Train Epoch: [18]  [1150/2543]  eta: 0:30:55  lr: 0.000004  loss_cl: 8.3341  loss_pitm: 0.0182  loss_mlm: 0.3509  loss_prd: 0.0560  loss_mrtd: 0.2041  time: 1.3095  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1200/2543]  eta: 0:29:47  lr: 0.000004  loss_cl: 8.3666  loss_pitm: 0.0102  loss_mlm: 0.4191  loss_prd: 0.2823  loss_mrtd: 0.1999  time: 1.3066  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1250/2543]  eta: 0:28:40  lr: 0.000004  loss_cl: 8.7464  loss_pitm: 0.0061  loss_mlm: 0.6051  loss_prd: 0.2360  loss_mrtd: 0.1536  time: 1.3170  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1300/2543]  eta: 0:27:33  lr: 0.000004  loss_cl: 8.2320  loss_pitm: 0.0866  loss_mlm: 0.6297  loss_prd: 0.0386  loss_mrtd: 0.2591  time: 1.3089  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1350/2543]  eta: 0:26:25  lr: 0.000004  loss_cl: 8.5384  loss_pitm: 0.0071  loss_mlm: 0.8330  loss_prd: 0.0410  loss_mrtd: 0.2563  time: 1.3019  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1400/2543]  eta: 0:25:17  lr: 0.000004  loss_cl: 8.1178  loss_pitm: 0.0060  loss_mlm: 0.3931  loss_prd: 0.0424  loss_mrtd: 0.3106  time: 1.2954  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1450/2543]  eta: 0:24:10  lr: 0.000004  loss_cl: 8.1956  loss_pitm: 0.1145  loss_mlm: 0.4520  loss_prd: 0.3024  loss_mrtd: 0.2323  time: 1.3083  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1500/2543]  eta: 0:23:03  lr: 0.000004  loss_cl: 8.0713  loss_pitm: 0.0272  loss_mlm: 0.3800  loss_prd: 0.2879  loss_mrtd: 0.2163  time: 1.3217  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1550/2543]  eta: 0:21:56  lr: 0.000004  loss_cl: 7.7062  loss_pitm: 0.0184  loss_mlm: 0.4347  loss_prd: 0.0889  loss_mrtd: 0.2457  time: 1.3101  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1600/2543]  eta: 0:20:49  lr: 0.000004  loss_cl: 8.2768  loss_pitm: 0.0038  loss_mlm: 0.2366  loss_prd: 0.2871  loss_mrtd: 0.2842  time: 1.2988  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1650/2543]  eta: 0:19:43  lr: 0.000004  loss_cl: 8.3992  loss_pitm: 0.0817  loss_mlm: 0.3142  loss_prd: 0.0634  loss_mrtd: 0.2199  time: 1.3183  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1700/2543]  eta: 0:18:36  lr: 0.000004  loss_cl: 8.3252  loss_pitm: 0.0018  loss_mlm: 0.4404  loss_prd: 0.0656  loss_mrtd: 0.2325  time: 1.3043  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1750/2543]  eta: 0:17:29  lr: 0.000004  loss_cl: 8.2600  loss_pitm: 0.1027  loss_mlm: 0.5388  loss_prd: 0.0336  loss_mrtd: 0.3077  time: 1.3048  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1800/2543]  eta: 0:16:23  lr: 0.000004  loss_cl: 8.2704  loss_pitm: 0.0186  loss_mlm: 0.6202  loss_prd: 0.2217  loss_mrtd: 0.2717  time: 1.3059  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1850/2543]  eta: 0:15:16  lr: 0.000004  loss_cl: 8.2979  loss_pitm: 0.1208  loss_mlm: 0.4727  loss_prd: 0.2708  loss_mrtd: 0.2541  time: 1.3250  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1900/2543]  eta: 0:14:10  lr: 0.000004  loss_cl: 7.7445  loss_pitm: 0.0094  loss_mlm: 0.2551  loss_prd: 0.0546  loss_mrtd: 0.2610  time: 1.3237  data: 0.0001  max mem: 17784
Train Epoch: [18]  [1950/2543]  eta: 0:13:04  lr: 0.000004  loss_cl: 7.7758  loss_pitm: 0.0137  loss_mlm: 0.4076  loss_prd: 0.0520  loss_mrtd: 0.2246  time: 1.3106  data: 0.0001  max mem: 17784
Train Epoch: [18]  [2000/2543]  eta: 0:11:58  lr: 0.000004  loss_cl: 8.1052  loss_pitm: 0.0593  loss_mlm: 0.3529  loss_prd: 0.7712  loss_mrtd: 0.2628  time: 1.3154  data: 0.0001  max mem: 17784
Train Epoch: [18]  [2050/2543]  eta: 0:10:52  lr: 0.000004  loss_cl: 7.7265  loss_pitm: 0.0051  loss_mlm: 0.5998  loss_prd: 0.1197  loss_mrtd: 0.2767  time: 1.3168  data: 0.0001  max mem: 17784
Train Epoch: [18]  [2100/2543]  eta: 0:09:45  lr: 0.000004  loss_cl: 8.0483  loss_pitm: 0.0145  loss_mlm: 0.6776  loss_prd: 0.0519  loss_mrtd: 0.2931  time: 1.2947  data: 0.0001  max mem: 17784
Train Epoch: [18]  [2150/2543]  eta: 0:08:39  lr: 0.000004  loss_cl: 7.9734  loss_pitm: 0.0280  loss_mlm: 0.5585  loss_prd: 0.0706  loss_mrtd: 0.2665  time: 1.3038  data: 0.0001  max mem: 17784
Train Epoch: [18]  [2200/2543]  eta: 0:07:33  lr: 0.000004  loss_cl: 7.8045  loss_pitm: 0.0132  loss_mlm: 0.3811  loss_prd: 0.2626  loss_mrtd: 0.2616  time: 1.2966  data: 0.0001  max mem: 17784
Train Epoch: [18]  [2250/2543]  eta: 0:06:27  lr: 0.000004  loss_cl: 7.6162  loss_pitm: 0.0093  loss_mlm: 0.4167  loss_prd: 0.3323  loss_mrtd: 0.2218  time: 1.3157  data: 0.0001  max mem: 17784
Train Epoch: [18]  [2300/2543]  eta: 0:05:20  lr: 0.000004  loss_cl: 7.1482  loss_pitm: 0.0059  loss_mlm: 0.5022  loss_prd: 0.0453  loss_mrtd: 0.1961  time: 1.3020  data: 0.0001  max mem: 17784
Train Epoch: [18]  [2350/2543]  eta: 0:04:14  lr: 0.000004  loss_cl: 8.0865  loss_pitm: 0.1468  loss_mlm: 0.3983  loss_prd: 0.2446  loss_mrtd: 0.2799  time: 1.3226  data: 0.0001  max mem: 17784
Train Epoch: [18]  [2400/2543]  eta: 0:03:08  lr: 0.000004  loss_cl: 7.4665  loss_pitm: 0.0253  loss_mlm: 0.6357  loss_prd: 0.0669  loss_mrtd: 0.2181  time: 1.3062  data: 0.0001  max mem: 17784
Train Epoch: [18]  [2450/2543]  eta: 0:02:02  lr: 0.000004  loss_cl: 6.8005  loss_pitm: 0.0542  loss_mlm: 0.2242  loss_prd: 0.1518  loss_mrtd: 0.2386  time: 1.3199  data: 0.0001  max mem: 17784
Train Epoch: [18]  [2500/2543]  eta: 0:00:56  lr: 0.000004  loss_cl: 7.4180  loss_pitm: 0.0542  loss_mlm: 0.4528  loss_prd: 0.3217  loss_mrtd: 0.2033  time: 1.3137  data: 0.0001  max mem: 17784
Train Epoch: [18]  [2542/2543]  eta: 0:00:01  lr: 0.000004  loss_cl: 7.0305  loss_pitm: 0.0041  loss_mlm: 0.5070  loss_prd: 0.0475  loss_mrtd: 0.2224  time: 1.3037  data: 0.0002  max mem: 17784
Train Epoch: [18] Total time: 0:55:56 (1.3200 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5190  loss_pitm: 0.0502  loss_mlm: 0.4381  loss_prd: 0.2031  loss_mrtd: 0.2212
Train Epoch的聚类: [19]  [   0/5241]  eta: 0:53:40    time: 0.6145  data: 0.5927  max mem: 17784
Train Epoch的聚类: [19]  [5240/5241]  eta: 0:00:00    time: 0.1718  data: 0.0002  max mem: 17784
Train Epoch的聚类: [19] Total time: 0:15:01 (0.1719 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4242
-1 (未归入任何簇) 的数量: 1844

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [19]  [   0/2549]  eta: 22:59:44  lr: 0.000004  loss_cl: 10.1536  loss_pitm: 0.0987  loss_mlm: 0.5309  loss_prd: 0.2744  loss_mrtd: 0.1667  time: 32.4771  data: 0.5160  max mem: 17784
Train Epoch: [19]  [  50/2549]  eta: 1:18:14  lr: 0.000004  loss_cl: 9.4673  loss_pitm: 0.0695  loss_mlm: 0.4585  loss_prd: 0.0666  loss_mrtd: 0.2590  time: 1.2757  data: 0.0001  max mem: 17789
Train Epoch: [19]  [ 100/2549]  eta: 1:04:43  lr: 0.000004  loss_cl: 9.7015  loss_pitm: 0.0026  loss_mlm: 0.4361  loss_prd: 0.5021  loss_mrtd: 0.2077  time: 1.2970  data: 0.0001  max mem: 17789
Train Epoch: [19]  [ 150/2549]  eta: 0:59:38  lr: 0.000004  loss_cl: 9.2798  loss_pitm: 0.0062  loss_mlm: 0.5137  loss_prd: 0.2904  loss_mrtd: 0.2559  time: 1.3074  data: 0.0001  max mem: 17789
Train Epoch: [19]  [ 200/2549]  eta: 0:56:33  lr: 0.000004  loss_cl: 9.5003  loss_pitm: 0.2393  loss_mlm: 0.1823  loss_prd: 0.0733  loss_mrtd: 0.3213  time: 1.2992  data: 0.0001  max mem: 17789
Train Epoch: [19]  [ 250/2549]  eta: 0:54:14  lr: 0.000004  loss_cl: 8.9498  loss_pitm: 0.2875  loss_mlm: 0.4443  loss_prd: 0.6126  loss_mrtd: 0.2297  time: 1.2891  data: 0.0001  max mem: 17789
Train Epoch: [19]  [ 300/2549]  eta: 0:52:24  lr: 0.000004  loss_cl: 9.1008  loss_pitm: 0.0032  loss_mlm: 0.2801  loss_prd: 0.0528  loss_mrtd: 0.1677  time: 1.3111  data: 0.0001  max mem: 17789
Train Epoch: [19]  [ 350/2549]  eta: 0:50:46  lr: 0.000004  loss_cl: 9.0598  loss_pitm: 0.0024  loss_mlm: 0.3422  loss_prd: 0.2637  loss_mrtd: 0.2027  time: 1.3003  data: 0.0001  max mem: 17789
Train Epoch: [19]  [ 400/2549]  eta: 0:49:15  lr: 0.000004  loss_cl: 9.4812  loss_pitm: 0.1021  loss_mlm: 0.4520  loss_prd: 0.2775  loss_mrtd: 0.2500  time: 1.3092  data: 0.0001  max mem: 17789
Train Epoch: [19]  [ 450/2549]  eta: 0:47:51  lr: 0.000004  loss_cl: 9.4754  loss_pitm: 0.1261  loss_mlm: 0.5816  loss_prd: 0.2551  loss_mrtd: 0.2510  time: 1.3202  data: 0.0001  max mem: 17790
Train Epoch: [19]  [ 500/2549]  eta: 0:46:31  lr: 0.000004  loss_cl: 8.5896  loss_pitm: 0.0050  loss_mlm: 0.6313  loss_prd: 0.0552  loss_mrtd: 0.2654  time: 1.3191  data: 0.0001  max mem: 17790
Train Epoch: [19]  [ 550/2549]  eta: 0:45:14  lr: 0.000004  loss_cl: 9.1243  loss_pitm: 0.0082  loss_mlm: 0.3647  loss_prd: 0.0553  loss_mrtd: 0.1900  time: 1.3185  data: 0.0001  max mem: 17790
Train Epoch: [19]  [ 600/2549]  eta: 0:43:58  lr: 0.000004  loss_cl: 9.3523  loss_pitm: 0.0047  loss_mlm: 0.4014  loss_prd: 0.0428  loss_mrtd: 0.2330  time: 1.3124  data: 0.0001  max mem: 17790
Train Epoch: [19]  [ 650/2549]  eta: 0:42:42  lr: 0.000004  loss_cl: 8.5992  loss_pitm: 0.0195  loss_mlm: 0.2449  loss_prd: 0.0838  loss_mrtd: 0.2230  time: 1.2941  data: 0.0001  max mem: 17790
Train Epoch: [19]  [ 700/2549]  eta: 0:41:28  lr: 0.000004  loss_cl: 9.1824  loss_pitm: 0.0337  loss_mlm: 0.5111  loss_prd: 0.1223  loss_mrtd: 0.2279  time: 1.3105  data: 0.0001  max mem: 17790
Train Epoch: [19]  [ 750/2549]  eta: 0:40:17  lr: 0.000004  loss_cl: 8.8546  loss_pitm: 0.0064  loss_mlm: 0.3359  loss_prd: 0.2136  loss_mrtd: 0.1127  time: 1.2946  data: 0.0001  max mem: 17790
Train Epoch: [19]  [ 800/2549]  eta: 0:39:05  lr: 0.000004  loss_cl: 8.9093  loss_pitm: 0.0117  loss_mlm: 0.4492  loss_prd: 0.6343  loss_mrtd: 0.2354  time: 1.3093  data: 0.0001  max mem: 17790
Train Epoch: [19]  [ 850/2549]  eta: 0:37:54  lr: 0.000004  loss_cl: 8.8741  loss_pitm: 0.0786  loss_mlm: 0.2707  loss_prd: 0.2346  loss_mrtd: 0.1643  time: 1.3154  data: 0.0001  max mem: 17790
Train Epoch: [19]  [ 900/2549]  eta: 0:36:45  lr: 0.000004  loss_cl: 8.4090  loss_pitm: 0.0136  loss_mlm: 0.6042  loss_prd: 0.2645  loss_mrtd: 0.3090  time: 1.3105  data: 0.0001  max mem: 17790
Train Epoch: [19]  [ 950/2549]  eta: 0:35:35  lr: 0.000004  loss_cl: 9.0692  loss_pitm: 0.0044  loss_mlm: 0.2629  loss_prd: 0.0452  loss_mrtd: 0.2467  time: 1.3161  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1000/2549]  eta: 0:34:26  lr: 0.000004  loss_cl: 9.0253  loss_pitm: 0.0096  loss_mlm: 0.3335  loss_prd: 0.0447  loss_mrtd: 0.1860  time: 1.2990  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1050/2549]  eta: 0:33:18  lr: 0.000004  loss_cl: 8.8324  loss_pitm: 0.0106  loss_mlm: 0.5408  loss_prd: 0.2449  loss_mrtd: 0.2301  time: 1.2984  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1100/2549]  eta: 0:32:10  lr: 0.000004  loss_cl: 8.4825  loss_pitm: 0.0103  loss_mlm: 0.6665  loss_prd: 0.0420  loss_mrtd: 0.1676  time: 1.3243  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1150/2549]  eta: 0:31:01  lr: 0.000004  loss_cl: 8.8294  loss_pitm: 0.0035  loss_mlm: 0.6961  loss_prd: 0.5195  loss_mrtd: 0.1969  time: 1.2949  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1200/2549]  eta: 0:29:53  lr: 0.000004  loss_cl: 8.4273  loss_pitm: 0.1180  loss_mlm: 0.4265  loss_prd: 0.3682  loss_mrtd: 0.2086  time: 1.3180  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1250/2549]  eta: 0:28:45  lr: 0.000004  loss_cl: 8.8079  loss_pitm: 0.0149  loss_mlm: 0.2856  loss_prd: 0.0693  loss_mrtd: 0.2435  time: 1.3060  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1300/2549]  eta: 0:27:38  lr: 0.000004  loss_cl: 8.7349  loss_pitm: 0.0175  loss_mlm: 0.6103  loss_prd: 0.2687  loss_mrtd: 0.2331  time: 1.3017  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1350/2549]  eta: 0:26:30  lr: 0.000004  loss_cl: 8.5062  loss_pitm: 0.0302  loss_mlm: 0.2958  loss_prd: 0.0580  loss_mrtd: 0.2463  time: 1.3058  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1400/2549]  eta: 0:25:23  lr: 0.000004  loss_cl: 8.4360  loss_pitm: 0.0333  loss_mlm: 0.2554  loss_prd: 0.0574  loss_mrtd: 0.1653  time: 1.3009  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1450/2549]  eta: 0:24:15  lr: 0.000004  loss_cl: 8.5495  loss_pitm: 0.0062  loss_mlm: 0.5955  loss_prd: 0.0526  loss_mrtd: 0.2431  time: 1.2938  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1500/2549]  eta: 0:23:08  lr: 0.000004  loss_cl: 8.5005  loss_pitm: 0.0992  loss_mlm: 0.4922  loss_prd: 0.0867  loss_mrtd: 0.1475  time: 1.3109  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1550/2549]  eta: 0:22:01  lr: 0.000004  loss_cl: 8.3125  loss_pitm: 0.0223  loss_mlm: 0.4468  loss_prd: 0.2713  loss_mrtd: 0.2831  time: 1.2987  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1600/2549]  eta: 0:20:55  lr: 0.000004  loss_cl: 8.6067  loss_pitm: 0.1305  loss_mlm: 0.1942  loss_prd: 0.0456  loss_mrtd: 0.2712  time: 1.2971  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1650/2549]  eta: 0:19:48  lr: 0.000004  loss_cl: 8.6380  loss_pitm: 0.0037  loss_mlm: 0.4970  loss_prd: 0.0489  loss_mrtd: 0.2217  time: 1.3116  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1700/2549]  eta: 0:18:42  lr: 0.000004  loss_cl: 8.1991  loss_pitm: 0.0610  loss_mlm: 0.4772  loss_prd: 0.1536  loss_mrtd: 0.2837  time: 1.3073  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1750/2549]  eta: 0:17:35  lr: 0.000004  loss_cl: 8.0062  loss_pitm: 0.0051  loss_mlm: 0.4902  loss_prd: 0.4790  loss_mrtd: 0.1853  time: 1.3012  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1800/2549]  eta: 0:16:29  lr: 0.000004  loss_cl: 7.9829  loss_pitm: 0.0075  loss_mlm: 0.4830  loss_prd: 0.0554  loss_mrtd: 0.2014  time: 1.2999  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1850/2549]  eta: 0:15:22  lr: 0.000004  loss_cl: 7.9648  loss_pitm: 0.0253  loss_mlm: 0.2824  loss_prd: 0.3352  loss_mrtd: 0.2059  time: 1.3067  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1900/2549]  eta: 0:14:16  lr: 0.000004  loss_cl: 8.3884  loss_pitm: 0.0088  loss_mlm: 0.4646  loss_prd: 0.0550  loss_mrtd: 0.2276  time: 1.3085  data: 0.0001  max mem: 17790
Train Epoch: [19]  [1950/2549]  eta: 0:13:10  lr: 0.000004  loss_cl: 7.8853  loss_pitm: 0.0502  loss_mlm: 0.3210  loss_prd: 0.5372  loss_mrtd: 0.1710  time: 1.3039  data: 0.0001  max mem: 17790
Train Epoch: [19]  [2000/2549]  eta: 0:12:04  lr: 0.000004  loss_cl: 8.0823  loss_pitm: 0.0098  loss_mlm: 0.5744  loss_prd: 0.5632  loss_mrtd: 0.2645  time: 1.3026  data: 0.0001  max mem: 17790
Train Epoch: [19]  [2050/2549]  eta: 0:10:57  lr: 0.000004  loss_cl: 8.2869  loss_pitm: 0.0141  loss_mlm: 0.4791  loss_prd: 0.0474  loss_mrtd: 0.2027  time: 1.3056  data: 0.0001  max mem: 17790
Train Epoch: [19]  [2100/2549]  eta: 0:09:51  lr: 0.000004  loss_cl: 7.6443  loss_pitm: 0.0938  loss_mlm: 0.3984  loss_prd: 0.0378  loss_mrtd: 0.2258  time: 1.3021  data: 0.0001  max mem: 17790
Train Epoch: [19]  [2150/2549]  eta: 0:08:45  lr: 0.000004  loss_cl: 7.5971  loss_pitm: 0.0168  loss_mlm: 0.4250  loss_prd: 0.0670  loss_mrtd: 0.2004  time: 1.3014  data: 0.0001  max mem: 17790
Train Epoch: [19]  [2200/2549]  eta: 0:07:39  lr: 0.000004  loss_cl: 7.9491  loss_pitm: 0.0651  loss_mlm: 0.3141  loss_prd: 0.2481  loss_mrtd: 0.1336  time: 1.2987  data: 0.0001  max mem: 17790
Train Epoch: [19]  [2250/2549]  eta: 0:06:33  lr: 0.000004  loss_cl: 7.3120  loss_pitm: 0.0171  loss_mlm: 0.4027  loss_prd: 0.0416  loss_mrtd: 0.2770  time: 1.3015  data: 0.0001  max mem: 17790
Train Epoch: [19]  [2300/2549]  eta: 0:05:27  lr: 0.000004  loss_cl: 7.4795  loss_pitm: 0.1503  loss_mlm: 0.2893  loss_prd: 0.0714  loss_mrtd: 0.2192  time: 1.3028  data: 0.0001  max mem: 17790
Train Epoch: [19]  [2350/2549]  eta: 0:04:21  lr: 0.000004  loss_cl: 7.4936  loss_pitm: 0.0750  loss_mlm: 0.4704  loss_prd: 0.1631  loss_mrtd: 0.2148  time: 1.3014  data: 0.0001  max mem: 17790
Train Epoch: [19]  [2400/2549]  eta: 0:03:16  lr: 0.000004  loss_cl: 7.1799  loss_pitm: 0.0047  loss_mlm: 0.5285  loss_prd: 0.0490  loss_mrtd: 0.2576  time: 1.3079  data: 0.0001  max mem: 17790
Train Epoch: [19]  [2450/2549]  eta: 0:02:10  lr: 0.000004  loss_cl: 7.0685  loss_pitm: 0.0039  loss_mlm: 0.2397  loss_prd: 0.0540  loss_mrtd: 0.1822  time: 1.3254  data: 0.0001  max mem: 17790
Train Epoch: [19]  [2500/2549]  eta: 0:01:04  lr: 0.000004  loss_cl: 6.9678  loss_pitm: 0.0124  loss_mlm: 0.5333  loss_prd: 0.2859  loss_mrtd: 0.1698  time: 1.3054  data: 0.0001  max mem: 17790
Train Epoch: [19]  [2548/2549]  eta: 0:00:01  lr: 0.000004  loss_cl: 7.0414  loss_pitm: 0.0162  loss_mlm: 0.5922  loss_prd: 0.1316  loss_mrtd: 0.2421  time: 1.3149  data: 0.0001  max mem: 17790
Train Epoch: [19] Total time: 0:55:54 (1.3161 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5517  loss_pitm: 0.0472  loss_mlm: 0.4346  loss_prd: 0.2087  loss_mrtd: 0.2202
Train Epoch的聚类: [20]  [   0/5241]  eta: 0:51:11    time: 0.5861  data: 0.5660  max mem: 17790
Train Epoch的聚类: [20]  [5240/5241]  eta: 0:00:00    time: 0.1719  data: 0.0002  max mem: 17790
Train Epoch的聚类: [20] Total time: 0:15:01 (0.1720 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4216
-1 (未归入任何簇) 的数量: 1800

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [20]  [   0/2551]  eta: 1 day, 0:02:09  lr: 0.000003  loss_cl: 10.3718  loss_pitm: 0.4336  loss_mlm: 0.3290  loss_prd: 0.1980  loss_mrtd: 0.1519  time: 33.9198  data: 0.5232  max mem: 17790
Train Epoch: [20]  [  50/2551]  eta: 1:19:36  lr: 0.000003  loss_cl: 10.1876  loss_pitm: 0.0672  loss_mlm: 0.3804  loss_prd: 0.2995  loss_mrtd: 0.2326  time: 1.2833  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 100/2551]  eta: 1:05:29  lr: 0.000003  loss_cl: 10.2607  loss_pitm: 0.0481  loss_mlm: 0.3961  loss_prd: 0.4606  loss_mrtd: 0.1977  time: 1.2882  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 150/2551]  eta: 1:00:03  lr: 0.000003  loss_cl: 9.7572  loss_pitm: 0.0930  loss_mlm: 0.3377  loss_prd: 0.0558  loss_mrtd: 0.1709  time: 1.2871  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 200/2551]  eta: 0:56:55  lr: 0.000003  loss_cl: 9.4152  loss_pitm: 0.0386  loss_mlm: 0.2490  loss_prd: 0.3681  loss_mrtd: 0.1684  time: 1.3099  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 250/2551]  eta: 0:54:42  lr: 0.000003  loss_cl: 9.4821  loss_pitm: 0.1056  loss_mlm: 0.1880  loss_prd: 0.3219  loss_mrtd: 0.1944  time: 1.3193  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 300/2551]  eta: 0:52:49  lr: 0.000003  loss_cl: 9.4843  loss_pitm: 0.2235  loss_mlm: 0.4319  loss_prd: 0.2661  loss_mrtd: 0.1976  time: 1.3174  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 350/2551]  eta: 0:51:10  lr: 0.000003  loss_cl: 9.3980  loss_pitm: 0.0227  loss_mlm: 0.2878  loss_prd: 0.0610  loss_mrtd: 0.2001  time: 1.3188  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 400/2551]  eta: 0:49:36  lr: 0.000003  loss_cl: 9.2423  loss_pitm: 0.1133  loss_mlm: 0.5112  loss_prd: 0.3185  loss_mrtd: 0.2158  time: 1.3053  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 450/2551]  eta: 0:48:08  lr: 0.000003  loss_cl: 9.2322  loss_pitm: 0.0247  loss_mlm: 0.2962  loss_prd: 0.2955  loss_mrtd: 0.1299  time: 1.2957  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 500/2551]  eta: 0:46:45  lr: 0.000003  loss_cl: 9.7302  loss_pitm: 0.1001  loss_mlm: 0.4364  loss_prd: 0.0842  loss_mrtd: 0.2729  time: 1.3155  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 550/2551]  eta: 0:45:25  lr: 0.000003  loss_cl: 9.3075  loss_pitm: 0.0038  loss_mlm: 0.3966  loss_prd: 0.0395  loss_mrtd: 0.2835  time: 1.3038  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 600/2551]  eta: 0:44:08  lr: 0.000003  loss_cl: 9.0164  loss_pitm: 0.0060  loss_mlm: 0.3126  loss_prd: 0.2900  loss_mrtd: 0.1467  time: 1.3022  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 650/2551]  eta: 0:42:53  lr: 0.000003  loss_cl: 8.7013  loss_pitm: 0.0027  loss_mlm: 0.7054  loss_prd: 0.2923  loss_mrtd: 0.2347  time: 1.3093  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 700/2551]  eta: 0:41:41  lr: 0.000003  loss_cl: 8.9332  loss_pitm: 0.0248  loss_mlm: 0.4256  loss_prd: 0.3160  loss_mrtd: 0.2009  time: 1.3014  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 750/2551]  eta: 0:40:29  lr: 0.000003  loss_cl: 8.9225  loss_pitm: 0.0096  loss_mlm: 0.4702  loss_prd: 0.0503  loss_mrtd: 0.2551  time: 1.3236  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 800/2551]  eta: 0:39:18  lr: 0.000003  loss_cl: 9.3111  loss_pitm: 0.1104  loss_mlm: 0.1557  loss_prd: 0.0512  loss_mrtd: 0.1682  time: 1.3158  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 850/2551]  eta: 0:38:07  lr: 0.000003  loss_cl: 8.7448  loss_pitm: 0.0073  loss_mlm: 0.4602  loss_prd: 0.0466  loss_mrtd: 0.2094  time: 1.3078  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 900/2551]  eta: 0:36:56  lr: 0.000003  loss_cl: 9.0531  loss_pitm: 0.2445  loss_mlm: 0.1334  loss_prd: 0.2851  loss_mrtd: 0.1939  time: 1.3044  data: 0.0001  max mem: 17790
Train Epoch: [20]  [ 950/2551]  eta: 0:35:46  lr: 0.000003  loss_cl: 8.8577  loss_pitm: 0.1470  loss_mlm: 0.2853  loss_prd: 0.7925  loss_mrtd: 0.2007  time: 1.3035  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1000/2551]  eta: 0:34:35  lr: 0.000003  loss_cl: 9.0059  loss_pitm: 0.0204  loss_mlm: 0.3499  loss_prd: 0.2886  loss_mrtd: 0.2147  time: 1.2968  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1050/2551]  eta: 0:33:26  lr: 0.000003  loss_cl: 9.0593  loss_pitm: 0.0084  loss_mlm: 0.4936  loss_prd: 0.3111  loss_mrtd: 0.2372  time: 1.2976  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1100/2551]  eta: 0:32:17  lr: 0.000003  loss_cl: 8.6847  loss_pitm: 0.0032  loss_mlm: 0.2516  loss_prd: 0.3142  loss_mrtd: 0.1203  time: 1.3056  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1150/2551]  eta: 0:31:08  lr: 0.000003  loss_cl: 8.8451  loss_pitm: 0.0514  loss_mlm: 0.8556  loss_prd: 0.9807  loss_mrtd: 0.2094  time: 1.2924  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1200/2551]  eta: 0:29:59  lr: 0.000003  loss_cl: 8.5076  loss_pitm: 0.0102  loss_mlm: 0.5201  loss_prd: 0.0511  loss_mrtd: 0.1510  time: 1.3018  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1250/2551]  eta: 0:28:52  lr: 0.000003  loss_cl: 8.6375  loss_pitm: 0.0179  loss_mlm: 0.5879  loss_prd: 0.0615  loss_mrtd: 0.1829  time: 1.3235  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1300/2551]  eta: 0:27:44  lr: 0.000003  loss_cl: 8.5300  loss_pitm: 0.0135  loss_mlm: 0.3817  loss_prd: 0.2753  loss_mrtd: 0.2199  time: 1.3032  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1350/2551]  eta: 0:26:37  lr: 0.000003  loss_cl: 8.9089  loss_pitm: 0.1223  loss_mlm: 0.4397  loss_prd: 0.0475  loss_mrtd: 0.2105  time: 1.3207  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1400/2551]  eta: 0:25:30  lr: 0.000003  loss_cl: 8.1096  loss_pitm: 0.0319  loss_mlm: 0.3190  loss_prd: 0.1750  loss_mrtd: 0.1870  time: 1.3072  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1450/2551]  eta: 0:24:23  lr: 0.000003  loss_cl: 8.6071  loss_pitm: 0.0020  loss_mlm: 0.4293  loss_prd: 0.4760  loss_mrtd: 0.2399  time: 1.3125  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1500/2551]  eta: 0:23:16  lr: 0.000003  loss_cl: 8.3373  loss_pitm: 0.0338  loss_mlm: 0.4500  loss_prd: 0.6598  loss_mrtd: 0.1612  time: 1.2985  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1550/2551]  eta: 0:22:08  lr: 0.000003  loss_cl: 8.4847  loss_pitm: 0.0093  loss_mlm: 0.6942  loss_prd: 0.2884  loss_mrtd: 0.2748  time: 1.3001  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1600/2551]  eta: 0:21:01  lr: 0.000003  loss_cl: 8.1569  loss_pitm: 0.0042  loss_mlm: 0.4325  loss_prd: 0.5566  loss_mrtd: 0.1718  time: 1.3012  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1650/2551]  eta: 0:19:55  lr: 0.000003  loss_cl: 7.9718  loss_pitm: 0.0099  loss_mlm: 0.4437  loss_prd: 0.0478  loss_mrtd: 0.2300  time: 1.3126  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1700/2551]  eta: 0:18:48  lr: 0.000003  loss_cl: 8.2666  loss_pitm: 0.0175  loss_mlm: 0.4664  loss_prd: 0.1131  loss_mrtd: 0.2035  time: 1.3102  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1750/2551]  eta: 0:17:41  lr: 0.000003  loss_cl: 8.5854  loss_pitm: 0.0238  loss_mlm: 0.6210  loss_prd: 0.1409  loss_mrtd: 0.2406  time: 1.2901  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1800/2551]  eta: 0:16:34  lr: 0.000003  loss_cl: 8.4036  loss_pitm: 0.0079  loss_mlm: 0.3911  loss_prd: 0.0414  loss_mrtd: 0.1719  time: 1.2934  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1850/2551]  eta: 0:15:28  lr: 0.000003  loss_cl: 7.9305  loss_pitm: 0.0189  loss_mlm: 0.4861  loss_prd: 0.0438  loss_mrtd: 0.2342  time: 1.3051  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1900/2551]  eta: 0:14:21  lr: 0.000003  loss_cl: 7.6999  loss_pitm: 0.0249  loss_mlm: 0.3361  loss_prd: 0.0442  loss_mrtd: 0.1397  time: 1.3160  data: 0.0001  max mem: 17790
Train Epoch: [20]  [1950/2551]  eta: 0:13:15  lr: 0.000003  loss_cl: 7.7483  loss_pitm: 0.0205  loss_mlm: 0.5741  loss_prd: 0.5194  loss_mrtd: 0.2082  time: 1.3163  data: 0.0001  max mem: 17790
Train Epoch: [20]  [2000/2551]  eta: 0:12:08  lr: 0.000003  loss_cl: 7.6382  loss_pitm: 0.0081  loss_mlm: 0.4341  loss_prd: 0.0386  loss_mrtd: 0.2563  time: 1.2933  data: 0.0001  max mem: 17790
Train Epoch: [20]  [2050/2551]  eta: 0:11:02  lr: 0.000003  loss_cl: 8.1144  loss_pitm: 0.0149  loss_mlm: 0.3661  loss_prd: 0.0369  loss_mrtd: 0.2568  time: 1.3039  data: 0.0001  max mem: 17790
Train Epoch: [20]  [2100/2551]  eta: 0:09:56  lr: 0.000003  loss_cl: 7.9518  loss_pitm: 0.0591  loss_mlm: 0.4691  loss_prd: 0.2702  loss_mrtd: 0.2423  time: 1.2942  data: 0.0001  max mem: 17790
Train Epoch: [20]  [2150/2551]  eta: 0:08:50  lr: 0.000003  loss_cl: 7.4281  loss_pitm: 0.0119  loss_mlm: 0.4427  loss_prd: 0.3106  loss_mrtd: 0.2125  time: 1.3122  data: 0.0001  max mem: 17790
Train Epoch: [20]  [2200/2551]  eta: 0:07:43  lr: 0.000003  loss_cl: 7.5429  loss_pitm: 0.0111  loss_mlm: 0.4474  loss_prd: 0.0440  loss_mrtd: 0.2181  time: 1.3113  data: 0.0001  max mem: 17790
Train Epoch: [20]  [2250/2551]  eta: 0:06:37  lr: 0.000003  loss_cl: 7.5365  loss_pitm: 0.1741  loss_mlm: 0.2650  loss_prd: 0.2496  loss_mrtd: 0.1980  time: 1.3117  data: 0.0001  max mem: 17790
Train Epoch: [20]  [2300/2551]  eta: 0:05:31  lr: 0.000003  loss_cl: 7.4385  loss_pitm: 0.0369  loss_mlm: 0.4046  loss_prd: 0.2939  loss_mrtd: 0.1981  time: 1.3145  data: 0.0001  max mem: 17790
Train Epoch: [20]  [2350/2551]  eta: 0:04:25  lr: 0.000003  loss_cl: 7.7233  loss_pitm: 0.0083  loss_mlm: 0.5287  loss_prd: 0.0732  loss_mrtd: 0.2656  time: 1.3037  data: 0.0001  max mem: 17790
Train Epoch: [20]  [2400/2551]  eta: 0:03:19  lr: 0.000003  loss_cl: 7.9477  loss_pitm: 0.0119  loss_mlm: 0.1852  loss_prd: 0.1183  loss_mrtd: 0.1873  time: 1.3001  data: 0.0001  max mem: 17790
Train Epoch: [20]  [2450/2551]  eta: 0:02:13  lr: 0.000003  loss_cl: 7.3605  loss_pitm: 0.0661  loss_mlm: 0.5532  loss_prd: 0.1356  loss_mrtd: 0.2269  time: 1.3035  data: 0.0001  max mem: 17790
Train Epoch: [20]  [2500/2551]  eta: 0:01:07  lr: 0.000003  loss_cl: 7.2662  loss_pitm: 0.1673  loss_mlm: 0.3499  loss_prd: 0.4945  loss_mrtd: 0.2040  time: 1.3070  data: 0.0001  max mem: 17790
Train Epoch: [20]  [2550/2551]  eta: 0:00:01  lr: 0.000003  loss_cl: 7.1559  loss_pitm: 0.0033  loss_mlm: 0.3276  loss_prd: 0.0534  loss_mrtd: 0.1780  time: 1.3145  data: 0.0002  max mem: 17790
Train Epoch: [20] Total time: 0:56:07 (1.3200 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5917  loss_pitm: 0.0468  loss_mlm: 0.4280  loss_prd: 0.2012  loss_mrtd: 0.2196
Train Epoch的聚类: [21]  [   0/5241]  eta: 0:46:07    time: 0.5280  data: 0.5048  max mem: 17790
Train Epoch的聚类: [21]  [5240/5241]  eta: 0:00:00    time: 0.1718  data: 0.0002  max mem: 17790
Train Epoch的聚类: [21] Total time: 0:15:00 (0.1719 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4172
-1 (未归入任何簇) 的数量: 1771

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [21]  [   0/2552]  eta: 23:46:02  lr: 0.000003  loss_cl: 10.2864  loss_pitm: 0.0098  loss_mlm: 0.4765  loss_prd: 0.2896  loss_mrtd: 0.2105  time: 33.5277  data: 0.5745  max mem: 17790
Train Epoch: [21]  [  50/2552]  eta: 1:19:27  lr: 0.000003  loss_cl: 9.4859  loss_pitm: 0.0082  loss_mlm: 0.4859  loss_prd: 0.0513  loss_mrtd: 0.2141  time: 1.2790  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 100/2552]  eta: 1:05:31  lr: 0.000003  loss_cl: 9.9909  loss_pitm: 0.0059  loss_mlm: 0.5106  loss_prd: 0.3400  loss_mrtd: 0.2387  time: 1.2945  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 150/2552]  eta: 1:00:11  lr: 0.000003  loss_cl: 9.7773  loss_pitm: 0.3364  loss_mlm: 0.5356  loss_prd: 0.3055  loss_mrtd: 0.2628  time: 1.3053  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 200/2552]  eta: 0:57:01  lr: 0.000003  loss_cl: 9.4145  loss_pitm: 0.0266  loss_mlm: 0.2887  loss_prd: 0.0821  loss_mrtd: 0.1738  time: 1.2934  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 250/2552]  eta: 0:54:38  lr: 0.000003  loss_cl: 9.3430  loss_pitm: 0.1816  loss_mlm: 0.3176  loss_prd: 0.0823  loss_mrtd: 0.1881  time: 1.3022  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 300/2552]  eta: 0:52:46  lr: 0.000003  loss_cl: 9.5069  loss_pitm: 0.0036  loss_mlm: 0.3495  loss_prd: 0.0524  loss_mrtd: 0.2298  time: 1.3247  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 350/2552]  eta: 0:51:08  lr: 0.000003  loss_cl: 9.2801  loss_pitm: 0.0964  loss_mlm: 0.1978  loss_prd: 0.6301  loss_mrtd: 0.2234  time: 1.3185  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 400/2552]  eta: 0:49:40  lr: 0.000003  loss_cl: 9.8133  loss_pitm: 0.0032  loss_mlm: 0.5182  loss_prd: 0.1822  loss_mrtd: 0.2603  time: 1.3355  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 450/2552]  eta: 0:48:16  lr: 0.000003  loss_cl: 9.1811  loss_pitm: 0.0989  loss_mlm: 0.2495  loss_prd: 0.0624  loss_mrtd: 0.1949  time: 1.3141  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 500/2552]  eta: 0:46:54  lr: 0.000003  loss_cl: 9.2481  loss_pitm: 0.0081  loss_mlm: 0.4527  loss_prd: 0.2510  loss_mrtd: 0.1816  time: 1.3177  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 550/2552]  eta: 0:45:37  lr: 0.000003  loss_cl: 9.1328  loss_pitm: 0.1488  loss_mlm: 0.3955  loss_prd: 0.0527  loss_mrtd: 0.2847  time: 1.3116  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 600/2552]  eta: 0:44:18  lr: 0.000003  loss_cl: 8.9571  loss_pitm: 0.0183  loss_mlm: 0.5163  loss_prd: 0.5235  loss_mrtd: 0.1986  time: 1.3064  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 650/2552]  eta: 0:43:01  lr: 0.000003  loss_cl: 9.3581  loss_pitm: 0.0107  loss_mlm: 0.3979  loss_prd: 0.2846  loss_mrtd: 0.1724  time: 1.3076  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 700/2552]  eta: 0:41:48  lr: 0.000003  loss_cl: 9.0379  loss_pitm: 0.0061  loss_mlm: 0.3594  loss_prd: 0.0542  loss_mrtd: 0.2169  time: 1.3232  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 750/2552]  eta: 0:40:34  lr: 0.000003  loss_cl: 9.0274  loss_pitm: 0.0292  loss_mlm: 0.1090  loss_prd: 0.4777  loss_mrtd: 0.2133  time: 1.3099  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 800/2552]  eta: 0:39:22  lr: 0.000003  loss_cl: 9.1607  loss_pitm: 0.0194  loss_mlm: 0.4484  loss_prd: 0.2967  loss_mrtd: 0.1771  time: 1.3170  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 850/2552]  eta: 0:38:11  lr: 0.000003  loss_cl: 9.0025  loss_pitm: 0.0084  loss_mlm: 0.2301  loss_prd: 0.2869  loss_mrtd: 0.1882  time: 1.3235  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 900/2552]  eta: 0:37:00  lr: 0.000003  loss_cl: 9.0478  loss_pitm: 0.1020  loss_mlm: 0.1723  loss_prd: 0.0427  loss_mrtd: 0.1648  time: 1.2957  data: 0.0001  max mem: 17790
Train Epoch: [21]  [ 950/2552]  eta: 0:35:49  lr: 0.000003  loss_cl: 8.2607  loss_pitm: 0.0101  loss_mlm: 0.4501  loss_prd: 0.1378  loss_mrtd: 0.2517  time: 1.3097  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1000/2552]  eta: 0:34:40  lr: 0.000003  loss_cl: 8.5847  loss_pitm: 0.0100  loss_mlm: 0.5217  loss_prd: 0.2429  loss_mrtd: 0.1952  time: 1.3074  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1050/2552]  eta: 0:33:32  lr: 0.000003  loss_cl: 9.0975  loss_pitm: 0.0575  loss_mlm: 0.3613  loss_prd: 0.3312  loss_mrtd: 0.2358  time: 1.3121  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1100/2552]  eta: 0:32:22  lr: 0.000003  loss_cl: 8.7209  loss_pitm: 0.0071  loss_mlm: 0.8243  loss_prd: 0.3122  loss_mrtd: 0.2188  time: 1.3021  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1150/2552]  eta: 0:31:13  lr: 0.000003  loss_cl: 8.4712  loss_pitm: 0.0064  loss_mlm: 0.6510  loss_prd: 0.2848  loss_mrtd: 0.2309  time: 1.3030  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1200/2552]  eta: 0:30:04  lr: 0.000003  loss_cl: 8.7476  loss_pitm: 0.0597  loss_mlm: 0.3423  loss_prd: 0.2476  loss_mrtd: 0.1989  time: 1.3071  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1250/2552]  eta: 0:28:56  lr: 0.000003  loss_cl: 8.2425  loss_pitm: 0.0713  loss_mlm: 0.5072  loss_prd: 0.3205  loss_mrtd: 0.2405  time: 1.3137  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1300/2552]  eta: 0:27:47  lr: 0.000003  loss_cl: 8.6174  loss_pitm: 0.1109  loss_mlm: 0.5964  loss_prd: 0.4039  loss_mrtd: 0.2591  time: 1.2996  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1350/2552]  eta: 0:26:40  lr: 0.000003  loss_cl: 8.3815  loss_pitm: 0.0189  loss_mlm: 0.2008  loss_prd: 1.0236  loss_mrtd: 0.2306  time: 1.3151  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1400/2552]  eta: 0:25:32  lr: 0.000003  loss_cl: 8.7973  loss_pitm: 0.0041  loss_mlm: 0.4909  loss_prd: 0.0532  loss_mrtd: 0.2352  time: 1.3037  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1450/2552]  eta: 0:24:25  lr: 0.000003  loss_cl: 8.3300  loss_pitm: 0.0184  loss_mlm: 0.4093  loss_prd: 0.2723  loss_mrtd: 0.2641  time: 1.3000  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1500/2552]  eta: 0:23:17  lr: 0.000003  loss_cl: 8.7000  loss_pitm: 0.0051  loss_mlm: 0.2725  loss_prd: 0.0575  loss_mrtd: 0.1980  time: 1.2933  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1550/2552]  eta: 0:22:10  lr: 0.000003  loss_cl: 8.2889  loss_pitm: 0.0120  loss_mlm: 0.3961  loss_prd: 0.5608  loss_mrtd: 0.2279  time: 1.3112  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1600/2552]  eta: 0:21:03  lr: 0.000003  loss_cl: 8.1372  loss_pitm: 0.0198  loss_mlm: 0.2137  loss_prd: 0.7921  loss_mrtd: 0.2094  time: 1.3092  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1650/2552]  eta: 0:19:56  lr: 0.000003  loss_cl: 8.2127  loss_pitm: 0.2443  loss_mlm: 0.5339  loss_prd: 0.0517  loss_mrtd: 0.3319  time: 1.2971  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1700/2552]  eta: 0:18:49  lr: 0.000003  loss_cl: 8.1484  loss_pitm: 0.0047  loss_mlm: 0.4661  loss_prd: 0.0547  loss_mrtd: 0.2076  time: 1.3030  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1750/2552]  eta: 0:17:43  lr: 0.000003  loss_cl: 8.2917  loss_pitm: 0.0215  loss_mlm: 0.9339  loss_prd: 0.1653  loss_mrtd: 0.2670  time: 1.3254  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1800/2552]  eta: 0:16:36  lr: 0.000003  loss_cl: 8.1260  loss_pitm: 0.0794  loss_mlm: 0.5978  loss_prd: 0.7592  loss_mrtd: 0.2140  time: 1.2977  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1850/2552]  eta: 0:15:29  lr: 0.000003  loss_cl: 8.4921  loss_pitm: 0.0085  loss_mlm: 0.4658  loss_prd: 0.0479  loss_mrtd: 0.2394  time: 1.3041  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1900/2552]  eta: 0:14:23  lr: 0.000003  loss_cl: 8.1562  loss_pitm: 0.0081  loss_mlm: 0.4388  loss_prd: 0.2536  loss_mrtd: 0.2451  time: 1.2977  data: 0.0001  max mem: 17790
Train Epoch: [21]  [1950/2552]  eta: 0:13:16  lr: 0.000003  loss_cl: 8.3218  loss_pitm: 0.0052  loss_mlm: 0.4790  loss_prd: 0.0499  loss_mrtd: 0.2292  time: 1.2941  data: 0.0001  max mem: 17794
Train Epoch: [21]  [2000/2552]  eta: 0:12:10  lr: 0.000003  loss_cl: 7.6501  loss_pitm: 0.0497  loss_mlm: 0.3709  loss_prd: 0.6075  loss_mrtd: 0.1923  time: 1.3000  data: 0.0001  max mem: 17794
Train Epoch: [21]  [2050/2552]  eta: 0:11:03  lr: 0.000003  loss_cl: 7.9203  loss_pitm: 0.1027  loss_mlm: 0.4322  loss_prd: 1.0045  loss_mrtd: 0.2378  time: 1.3030  data: 0.0001  max mem: 17794
Train Epoch: [21]  [2100/2552]  eta: 0:09:57  lr: 0.000003  loss_cl: 7.6703  loss_pitm: 0.0319  loss_mlm: 0.4823  loss_prd: 0.0684  loss_mrtd: 0.1498  time: 1.3162  data: 0.0001  max mem: 17794
Train Epoch: [21]  [2150/2552]  eta: 0:08:51  lr: 0.000003  loss_cl: 7.8381  loss_pitm: 0.1286  loss_mlm: 0.1773  loss_prd: 0.0454  loss_mrtd: 0.2364  time: 1.3033  data: 0.0001  max mem: 17794
Train Epoch: [21]  [2200/2552]  eta: 0:07:45  lr: 0.000003  loss_cl: 7.7443  loss_pitm: 0.0124  loss_mlm: 0.3645  loss_prd: 0.0603  loss_mrtd: 0.1507  time: 1.3230  data: 0.0001  max mem: 17794
Train Epoch: [21]  [2250/2552]  eta: 0:06:38  lr: 0.000003  loss_cl: 7.9820  loss_pitm: 0.0268  loss_mlm: 0.3445  loss_prd: 0.3086  loss_mrtd: 0.2021  time: 1.2958  data: 0.0001  max mem: 17794
Train Epoch: [21]  [2300/2552]  eta: 0:05:32  lr: 0.000003  loss_cl: 7.9705  loss_pitm: 0.0092  loss_mlm: 0.4303  loss_prd: 0.2183  loss_mrtd: 0.2884  time: 1.3026  data: 0.0001  max mem: 17794
Train Epoch: [21]  [2350/2552]  eta: 0:04:26  lr: 0.000003  loss_cl: 7.8864  loss_pitm: 0.0305  loss_mlm: 0.4349  loss_prd: 0.2192  loss_mrtd: 0.1844  time: 1.3020  data: 0.0001  max mem: 17794
Train Epoch: [21]  [2400/2552]  eta: 0:03:20  lr: 0.000003  loss_cl: 7.3568  loss_pitm: 0.1297  loss_mlm: 0.5957  loss_prd: 0.0895  loss_mrtd: 0.2623  time: 1.3043  data: 0.0001  max mem: 17794
Train Epoch: [21]  [2450/2552]  eta: 0:02:14  lr: 0.000003  loss_cl: 7.3387  loss_pitm: 0.0069  loss_mlm: 0.5131  loss_prd: 0.5126  loss_mrtd: 0.2136  time: 1.2971  data: 0.0001  max mem: 17794
Train Epoch: [21]  [2500/2552]  eta: 0:01:08  lr: 0.000003  loss_cl: 7.0372  loss_pitm: 0.0030  loss_mlm: 0.5167  loss_prd: 0.0446  loss_mrtd: 0.2834  time: 1.3005  data: 0.0001  max mem: 17794
Train Epoch: [21]  [2550/2552]  eta: 0:00:02  lr: 0.000003  loss_cl: 7.0883  loss_pitm: 0.0392  loss_mlm: 0.5655  loss_prd: 0.3138  loss_mrtd: 0.1869  time: 1.3040  data: 0.0002  max mem: 17794
Train Epoch: [21]  [2551/2552]  eta: 0:00:01  lr: 0.000003  loss_cl: 7.5116  loss_pitm: 0.0386  loss_mlm: 0.4262  loss_prd: 0.5574  loss_mrtd: 0.2520  time: 1.3019  data: 0.0002  max mem: 17794
Train Epoch: [21] Total time: 0:56:06 (1.3190 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5425  loss_pitm: 0.0468  loss_mlm: 0.4243  loss_prd: 0.1990  loss_mrtd: 0.2190
Train Epoch的聚类: [22]  [   0/5241]  eta: 0:45:50    time: 0.5248  data: 0.5070  max mem: 17794
Train Epoch的聚类: [22]  [5240/5241]  eta: 0:00:00    time: 0.1720  data: 0.0002  max mem: 17794
Train Epoch的聚类: [22] Total time: 0:15:01 (0.1719 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4144
-1 (未归入任何簇) 的数量: 1853

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [22]  [   0/2548]  eta: 1 day, 1:14:01  lr: 0.000002  loss_cl: 9.8849  loss_pitm: 0.0160  loss_mlm: 0.4412  loss_prd: 0.0413  loss_mrtd: 0.2338  time: 35.6522  data: 0.5852  max mem: 17794
Train Epoch: [22]  [  50/2548]  eta: 1:21:17  lr: 0.000002  loss_cl: 9.6372  loss_pitm: 0.0874  loss_mlm: 0.2353  loss_prd: 0.0409  loss_mrtd: 0.2082  time: 1.2999  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 100/2548]  eta: 1:06:29  lr: 0.000002  loss_cl: 9.8204  loss_pitm: 0.0309  loss_mlm: 0.6874  loss_prd: 0.2991  loss_mrtd: 0.2133  time: 1.3094  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 150/2548]  eta: 1:00:56  lr: 0.000002  loss_cl: 9.5102  loss_pitm: 0.0404  loss_mlm: 0.3762  loss_prd: 0.0738  loss_mrtd: 0.2013  time: 1.3097  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 200/2548]  eta: 0:57:28  lr: 0.000002  loss_cl: 9.5923  loss_pitm: 0.0427  loss_mlm: 0.5201  loss_prd: 0.0907  loss_mrtd: 0.2277  time: 1.2982  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 250/2548]  eta: 0:54:57  lr: 0.000002  loss_cl: 8.9901  loss_pitm: 0.0740  loss_mlm: 0.5670  loss_prd: 0.2928  loss_mrtd: 0.2387  time: 1.2933  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 300/2548]  eta: 0:52:56  lr: 0.000002  loss_cl: 9.6674  loss_pitm: 0.0055  loss_mlm: 0.4250  loss_prd: 0.0549  loss_mrtd: 0.1792  time: 1.3031  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 350/2548]  eta: 0:51:09  lr: 0.000002  loss_cl: 9.5779  loss_pitm: 0.0415  loss_mlm: 0.4902  loss_prd: 0.2846  loss_mrtd: 0.1646  time: 1.3019  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 400/2548]  eta: 0:49:32  lr: 0.000002  loss_cl: 9.6889  loss_pitm: 0.0041  loss_mlm: 0.3771  loss_prd: 0.0558  loss_mrtd: 0.2205  time: 1.2930  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 450/2548]  eta: 0:48:05  lr: 0.000002  loss_cl: 9.2120  loss_pitm: 0.0126  loss_mlm: 0.5118  loss_prd: 0.2332  loss_mrtd: 0.1687  time: 1.3047  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 500/2548]  eta: 0:46:44  lr: 0.000002  loss_cl: 9.1992  loss_pitm: 0.1846  loss_mlm: 0.5276  loss_prd: 0.2399  loss_mrtd: 0.2260  time: 1.3131  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 550/2548]  eta: 0:45:26  lr: 0.000002  loss_cl: 8.7812  loss_pitm: 0.0643  loss_mlm: 0.2529  loss_prd: 0.0533  loss_mrtd: 0.2129  time: 1.3151  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 600/2548]  eta: 0:44:08  lr: 0.000002  loss_cl: 9.1997  loss_pitm: 0.0476  loss_mlm: 0.3770  loss_prd: 0.3280  loss_mrtd: 0.1945  time: 1.2956  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 650/2548]  eta: 0:42:52  lr: 0.000002  loss_cl: 9.0175  loss_pitm: 0.0099  loss_mlm: 0.5840  loss_prd: 0.2723  loss_mrtd: 0.2794  time: 1.3059  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 700/2548]  eta: 0:41:38  lr: 0.000002  loss_cl: 9.2075  loss_pitm: 0.0329  loss_mlm: 0.3745  loss_prd: 0.0893  loss_mrtd: 0.1928  time: 1.3113  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 750/2548]  eta: 0:40:26  lr: 0.000002  loss_cl: 9.2007  loss_pitm: 0.0612  loss_mlm: 0.2833  loss_prd: 0.0569  loss_mrtd: 0.2167  time: 1.3080  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 800/2548]  eta: 0:39:15  lr: 0.000002  loss_cl: 9.5280  loss_pitm: 0.0052  loss_mlm: 0.3428  loss_prd: 0.2454  loss_mrtd: 0.2055  time: 1.3078  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 850/2548]  eta: 0:38:04  lr: 0.000002  loss_cl: 8.9929  loss_pitm: 0.0878  loss_mlm: 0.3043  loss_prd: 0.3015  loss_mrtd: 0.1328  time: 1.3200  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 900/2548]  eta: 0:36:54  lr: 0.000002  loss_cl: 8.9317  loss_pitm: 0.0412  loss_mlm: 0.4555  loss_prd: 0.7857  loss_mrtd: 0.2126  time: 1.3155  data: 0.0001  max mem: 17794
Train Epoch: [22]  [ 950/2548]  eta: 0:35:45  lr: 0.000002  loss_cl: 8.8664  loss_pitm: 0.1394  loss_mlm: 0.4509  loss_prd: 0.2761  loss_mrtd: 0.2212  time: 1.3091  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1000/2548]  eta: 0:34:35  lr: 0.000002  loss_cl: 9.2908  loss_pitm: 0.0586  loss_mlm: 0.6355  loss_prd: 0.2688  loss_mrtd: 0.2306  time: 1.2993  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1050/2548]  eta: 0:33:25  lr: 0.000002  loss_cl: 8.9837  loss_pitm: 0.0409  loss_mlm: 0.6121  loss_prd: 0.2855  loss_mrtd: 0.2264  time: 1.2938  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1100/2548]  eta: 0:32:16  lr: 0.000002  loss_cl: 8.8817  loss_pitm: 0.0891  loss_mlm: 0.3719  loss_prd: 0.3134  loss_mrtd: 0.1706  time: 1.3220  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1150/2548]  eta: 0:31:07  lr: 0.000002  loss_cl: 8.6765  loss_pitm: 0.1077  loss_mlm: 0.3811  loss_prd: 0.0669  loss_mrtd: 0.2048  time: 1.2908  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1200/2548]  eta: 0:29:58  lr: 0.000002  loss_cl: 8.6078  loss_pitm: 0.0183  loss_mlm: 0.5797  loss_prd: 0.3524  loss_mrtd: 0.2548  time: 1.3038  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1250/2548]  eta: 0:28:50  lr: 0.000002  loss_cl: 8.7039  loss_pitm: 0.0268  loss_mlm: 0.3570  loss_prd: 0.4344  loss_mrtd: 0.2205  time: 1.3262  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1300/2548]  eta: 0:27:42  lr: 0.000002  loss_cl: 8.2434  loss_pitm: 0.1397  loss_mlm: 0.6286  loss_prd: 0.3114  loss_mrtd: 0.2413  time: 1.2954  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1350/2548]  eta: 0:26:34  lr: 0.000002  loss_cl: 8.2388  loss_pitm: 0.0535  loss_mlm: 0.4538  loss_prd: 0.0529  loss_mrtd: 0.2064  time: 1.3034  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1400/2548]  eta: 0:25:26  lr: 0.000002  loss_cl: 8.4443  loss_pitm: 0.0132  loss_mlm: 0.3923  loss_prd: 0.1526  loss_mrtd: 0.2450  time: 1.2947  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1450/2548]  eta: 0:24:19  lr: 0.000002  loss_cl: 8.5955  loss_pitm: 0.1264  loss_mlm: 0.4865  loss_prd: 0.0425  loss_mrtd: 0.2088  time: 1.2994  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1500/2548]  eta: 0:23:11  lr: 0.000002  loss_cl: 8.2937  loss_pitm: 0.0057  loss_mlm: 0.4264  loss_prd: 0.2984  loss_mrtd: 0.1716  time: 1.3080  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1550/2548]  eta: 0:22:04  lr: 0.000002  loss_cl: 8.6960  loss_pitm: 0.0085  loss_mlm: 0.2126  loss_prd: 0.2634  loss_mrtd: 0.1471  time: 1.3050  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1600/2548]  eta: 0:20:57  lr: 0.000002  loss_cl: 8.3250  loss_pitm: 0.0032  loss_mlm: 0.6176  loss_prd: 0.3575  loss_mrtd: 0.1926  time: 1.2988  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1650/2548]  eta: 0:19:50  lr: 0.000002  loss_cl: 8.1833  loss_pitm: 0.1306  loss_mlm: 0.3362  loss_prd: 0.1048  loss_mrtd: 0.2582  time: 1.2923  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1700/2548]  eta: 0:18:43  lr: 0.000002  loss_cl: 8.0568  loss_pitm: 0.0237  loss_mlm: 0.5544  loss_prd: 0.1736  loss_mrtd: 0.2301  time: 1.2954  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1750/2548]  eta: 0:17:37  lr: 0.000002  loss_cl: 8.0914  loss_pitm: 0.0091  loss_mlm: 0.5088  loss_prd: 0.0442  loss_mrtd: 0.2273  time: 1.2993  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1800/2548]  eta: 0:16:30  lr: 0.000002  loss_cl: 8.0646  loss_pitm: 0.1197  loss_mlm: 0.5133  loss_prd: 0.3468  loss_mrtd: 0.1415  time: 1.3012  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1850/2548]  eta: 0:15:23  lr: 0.000002  loss_cl: 8.3591  loss_pitm: 0.0016  loss_mlm: 0.3282  loss_prd: 0.0494  loss_mrtd: 0.2380  time: 1.2955  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1900/2548]  eta: 0:14:17  lr: 0.000002  loss_cl: 8.0035  loss_pitm: 0.0115  loss_mlm: 0.2308  loss_prd: 0.0615  loss_mrtd: 0.1392  time: 1.3070  data: 0.0001  max mem: 17794
Train Epoch: [22]  [1950/2548]  eta: 0:13:10  lr: 0.000002  loss_cl: 8.0555  loss_pitm: 0.0307  loss_mlm: 0.3588  loss_prd: 0.0434  loss_mrtd: 0.1760  time: 1.3015  data: 0.0001  max mem: 17794
Train Epoch: [22]  [2000/2548]  eta: 0:12:04  lr: 0.000002  loss_cl: 7.8539  loss_pitm: 0.0149  loss_mlm: 0.3569  loss_prd: 0.2306  loss_mrtd: 0.2762  time: 1.2984  data: 0.0001  max mem: 17794
Train Epoch: [22]  [2050/2548]  eta: 0:10:57  lr: 0.000002  loss_cl: 7.8824  loss_pitm: 0.0318  loss_mlm: 0.2861  loss_prd: 0.0491  loss_mrtd: 0.1979  time: 1.3039  data: 0.0001  max mem: 17794
Train Epoch: [22]  [2100/2548]  eta: 0:09:51  lr: 0.000002  loss_cl: 7.7924  loss_pitm: 0.0040  loss_mlm: 0.3836  loss_prd: 0.0433  loss_mrtd: 0.2491  time: 1.3062  data: 0.0001  max mem: 17794
Train Epoch: [22]  [2150/2548]  eta: 0:08:45  lr: 0.000002  loss_cl: 7.5229  loss_pitm: 0.0220  loss_mlm: 0.6665  loss_prd: 0.2428  loss_mrtd: 0.1837  time: 1.3107  data: 0.0001  max mem: 17794
Train Epoch: [22]  [2200/2548]  eta: 0:07:39  lr: 0.000002  loss_cl: 7.4163  loss_pitm: 0.0087  loss_mlm: 0.2424  loss_prd: 0.5316  loss_mrtd: 0.2146  time: 1.2891  data: 0.0001  max mem: 17794
Train Epoch: [22]  [2250/2548]  eta: 0:06:33  lr: 0.000002  loss_cl: 7.8608  loss_pitm: 0.3578  loss_mlm: 0.3514  loss_prd: 0.0754  loss_mrtd: 0.2303  time: 1.3087  data: 0.0001  max mem: 17794
Train Epoch: [22]  [2300/2548]  eta: 0:05:27  lr: 0.000002  loss_cl: 7.2557  loss_pitm: 0.0023  loss_mlm: 0.3475  loss_prd: 0.0427  loss_mrtd: 0.1930  time: 1.3049  data: 0.0001  max mem: 17794
Train Epoch: [22]  [2350/2548]  eta: 0:04:21  lr: 0.000002  loss_cl: 7.6697  loss_pitm: 0.0039  loss_mlm: 0.4401  loss_prd: 0.0453  loss_mrtd: 0.3024  time: 1.3068  data: 0.0001  max mem: 17794
Train Epoch: [22]  [2400/2548]  eta: 0:03:15  lr: 0.000002  loss_cl: 7.3667  loss_pitm: 0.0222  loss_mlm: 0.2535  loss_prd: 0.1370  loss_mrtd: 0.1976  time: 1.3270  data: 0.0001  max mem: 17794
Train Epoch: [22]  [2450/2548]  eta: 0:02:09  lr: 0.000002  loss_cl: 7.3242  loss_pitm: 0.0128  loss_mlm: 0.3085  loss_prd: 0.0498  loss_mrtd: 0.1704  time: 1.3162  data: 0.0001  max mem: 17794
Train Epoch: [22]  [2500/2548]  eta: 0:01:03  lr: 0.000002  loss_cl: 7.2857  loss_pitm: 0.0217  loss_mlm: 0.4894  loss_prd: 0.5414  loss_mrtd: 0.2470  time: 1.3097  data: 0.0001  max mem: 17794
Train Epoch: [22]  [2547/2548]  eta: 0:00:01  lr: 0.000002  loss_cl: 7.2665  loss_pitm: 0.0947  loss_mlm: 0.4138  loss_prd: 0.0452  loss_mrtd: 0.1493  time: 1.3085  data: 0.0002  max mem: 17794
Train Epoch: [22] Total time: 0:56:00 (1.3188 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5627  loss_pitm: 0.0448  loss_mlm: 0.4223  loss_prd: 0.1988  loss_mrtd: 0.2177
Train Epoch的聚类: [23]  [   0/5241]  eta: 0:48:44    time: 0.5580  data: 0.5410  max mem: 17794
Train Epoch的聚类: [23]  [5240/5241]  eta: 0:00:00    time: 0.1720  data: 0.0002  max mem: 17794
Train Epoch的聚类: [23] Total time: 0:15:01 (0.1719 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4145
-1 (未归入任何簇) 的数量: 1741

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [23]  [   0/2553]  eta: 1 day, 10:26:25  lr: 0.000002  loss_cl: 10.4742  loss_pitm: 0.0066  loss_mlm: 0.5423  loss_prd: 0.2475  loss_mrtd: 0.1827  time: 48.5645  data: 0.5541  max mem: 17794
Train Epoch: [23]  [  50/2553]  eta: 1:31:30  lr: 0.000002  loss_cl: 10.5594  loss_pitm: 0.0047  loss_mlm: 0.5568  loss_prd: 0.3245  loss_mrtd: 0.2582  time: 1.2637  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 100/2553]  eta: 1:11:14  lr: 0.000002  loss_cl: 9.9702  loss_pitm: 0.0058  loss_mlm: 0.3817  loss_prd: 0.2663  loss_mrtd: 0.2254  time: 1.2874  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 150/2553]  eta: 1:03:54  lr: 0.000002  loss_cl: 9.7402  loss_pitm: 0.0504  loss_mlm: 0.4924  loss_prd: 0.0408  loss_mrtd: 0.2718  time: 1.2997  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 200/2553]  eta: 0:59:39  lr: 0.000002  loss_cl: 9.6984  loss_pitm: 0.0538  loss_mlm: 0.3825  loss_prd: 0.2937  loss_mrtd: 0.2440  time: 1.2965  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 250/2553]  eta: 0:56:42  lr: 0.000002  loss_cl: 9.1640  loss_pitm: 0.0027  loss_mlm: 0.4186  loss_prd: 0.2241  loss_mrtd: 0.2615  time: 1.3018  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 300/2553]  eta: 0:54:25  lr: 0.000002  loss_cl: 9.5677  loss_pitm: 0.0152  loss_mlm: 0.4178  loss_prd: 0.7642  loss_mrtd: 0.1460  time: 1.3023  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 350/2553]  eta: 0:52:28  lr: 0.000002  loss_cl: 9.5067  loss_pitm: 0.0122  loss_mlm: 0.4432  loss_prd: 0.0656  loss_mrtd: 0.2378  time: 1.3101  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 400/2553]  eta: 0:50:42  lr: 0.000002  loss_cl: 9.2461  loss_pitm: 0.0738  loss_mlm: 0.5442  loss_prd: 0.0367  loss_mrtd: 0.1722  time: 1.3012  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 450/2553]  eta: 0:49:06  lr: 0.000002  loss_cl: 9.3486  loss_pitm: 0.0043  loss_mlm: 0.4170  loss_prd: 0.0690  loss_mrtd: 0.2347  time: 1.3105  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 500/2553]  eta: 0:47:36  lr: 0.000002  loss_cl: 8.6338  loss_pitm: 0.0895  loss_mlm: 0.3570  loss_prd: 0.2810  loss_mrtd: 0.1941  time: 1.3050  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 550/2553]  eta: 0:46:09  lr: 0.000002  loss_cl: 9.5113  loss_pitm: 0.0056  loss_mlm: 0.3676  loss_prd: 0.4549  loss_mrtd: 0.2145  time: 1.3040  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 600/2553]  eta: 0:44:48  lr: 0.000002  loss_cl: 9.1716  loss_pitm: 0.0977  loss_mlm: 0.3002  loss_prd: 0.4465  loss_mrtd: 0.1607  time: 1.3001  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 650/2553]  eta: 0:43:27  lr: 0.000002  loss_cl: 9.1943  loss_pitm: 0.0231  loss_mlm: 0.2839  loss_prd: 0.2998  loss_mrtd: 0.1615  time: 1.2948  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 700/2553]  eta: 0:42:09  lr: 0.000002  loss_cl: 8.9171  loss_pitm: 0.0236  loss_mlm: 0.4222  loss_prd: 0.0447  loss_mrtd: 0.2194  time: 1.2976  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 750/2553]  eta: 0:40:53  lr: 0.000002  loss_cl: 9.2338  loss_pitm: 0.0042  loss_mlm: 0.5824  loss_prd: 0.3385  loss_mrtd: 0.2875  time: 1.2978  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 800/2553]  eta: 0:39:38  lr: 0.000002  loss_cl: 9.3593  loss_pitm: 0.0081  loss_mlm: 0.2457  loss_prd: 0.2868  loss_mrtd: 0.2021  time: 1.3022  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 850/2553]  eta: 0:38:24  lr: 0.000002  loss_cl: 8.9464  loss_pitm: 0.0031  loss_mlm: 0.4276  loss_prd: 0.1423  loss_mrtd: 0.2665  time: 1.2926  data: 0.0001  max mem: 17794
Train Epoch: [23]  [ 900/2553]  eta: 0:37:12  lr: 0.000002  loss_cl: 8.7800  loss_pitm: 0.0045  loss_mlm: 0.3446  loss_prd: 0.0430  loss_mrtd: 0.2801  time: 1.3013  data: 0.0002  max mem: 17794
Train Epoch: [23]  [ 950/2553]  eta: 0:36:03  lr: 0.000002  loss_cl: 8.9400  loss_pitm: 0.0040  loss_mlm: 0.6848  loss_prd: 0.0481  loss_mrtd: 0.3294  time: 1.3379  data: 0.0003  max mem: 17794
Train Epoch: [23]  [1000/2553]  eta: 0:34:54  lr: 0.000002  loss_cl: 9.1314  loss_pitm: 0.0054  loss_mlm: 0.2936  loss_prd: 0.0467  loss_mrtd: 0.1745  time: 1.3256  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1050/2553]  eta: 0:33:45  lr: 0.000002  loss_cl: 8.8672  loss_pitm: 0.0214  loss_mlm: 0.4025  loss_prd: 0.0544  loss_mrtd: 0.2171  time: 1.3393  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1100/2553]  eta: 0:32:36  lr: 0.000002  loss_cl: 8.7502  loss_pitm: 0.0017  loss_mlm: 0.4590  loss_prd: 0.0484  loss_mrtd: 0.2407  time: 1.3248  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1150/2553]  eta: 0:31:28  lr: 0.000002  loss_cl: 9.5020  loss_pitm: 0.0115  loss_mlm: 0.2056  loss_prd: 0.0627  loss_mrtd: 0.2411  time: 1.3345  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1200/2553]  eta: 0:30:20  lr: 0.000002  loss_cl: 9.0113  loss_pitm: 0.0921  loss_mlm: 0.4083  loss_prd: 0.2822  loss_mrtd: 0.1597  time: 1.3451  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1250/2553]  eta: 0:29:12  lr: 0.000002  loss_cl: 8.4931  loss_pitm: 0.0534  loss_mlm: 0.7778  loss_prd: 0.0468  loss_mrtd: 0.2567  time: 1.3264  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1300/2553]  eta: 0:28:04  lr: 0.000002  loss_cl: 8.5519  loss_pitm: 0.0400  loss_mlm: 0.4668  loss_prd: 0.0372  loss_mrtd: 0.2103  time: 1.3389  data: 0.0003  max mem: 17794
Train Epoch: [23]  [1350/2553]  eta: 0:26:56  lr: 0.000002  loss_cl: 8.8165  loss_pitm: 0.0134  loss_mlm: 0.3211  loss_prd: 0.0629  loss_mrtd: 0.2559  time: 1.3493  data: 0.0003  max mem: 17794
Train Epoch: [23]  [1400/2553]  eta: 0:25:49  lr: 0.000002  loss_cl: 8.5289  loss_pitm: 0.1303  loss_mlm: 0.3074  loss_prd: 0.3067  loss_mrtd: 0.1910  time: 1.3332  data: 0.0003  max mem: 17794
Train Epoch: [23]  [1450/2553]  eta: 0:24:42  lr: 0.000002  loss_cl: 8.3088  loss_pitm: 0.0095  loss_mlm: 0.2799  loss_prd: 0.2243  loss_mrtd: 0.1642  time: 1.3549  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1500/2553]  eta: 0:23:34  lr: 0.000002  loss_cl: 8.4226  loss_pitm: 0.0728  loss_mlm: 0.8135  loss_prd: 0.0503  loss_mrtd: 0.2068  time: 1.3273  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1550/2553]  eta: 0:22:26  lr: 0.000002  loss_cl: 8.4498  loss_pitm: 0.0560  loss_mlm: 0.3408  loss_prd: 0.0488  loss_mrtd: 0.1682  time: 1.3249  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1600/2553]  eta: 0:21:18  lr: 0.000002  loss_cl: 8.0343  loss_pitm: 0.0061  loss_mlm: 0.3852  loss_prd: 0.2716  loss_mrtd: 0.1769  time: 1.3208  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1650/2553]  eta: 0:20:11  lr: 0.000002  loss_cl: 8.4860  loss_pitm: 0.0038  loss_mlm: 0.3871  loss_prd: 0.2210  loss_mrtd: 0.2410  time: 1.3318  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1700/2553]  eta: 0:19:04  lr: 0.000002  loss_cl: 8.2113  loss_pitm: 0.0026  loss_mlm: 0.4532  loss_prd: 0.4831  loss_mrtd: 0.2546  time: 1.3174  data: 0.0003  max mem: 17794
Train Epoch: [23]  [1750/2553]  eta: 0:17:56  lr: 0.000002  loss_cl: 8.4633  loss_pitm: 0.0542  loss_mlm: 0.2074  loss_prd: 0.0498  loss_mrtd: 0.1686  time: 1.3264  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1800/2553]  eta: 0:16:49  lr: 0.000002  loss_cl: 7.6543  loss_pitm: 0.0201  loss_mlm: 0.4951  loss_prd: 0.2316  loss_mrtd: 0.1942  time: 1.3177  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1850/2553]  eta: 0:15:42  lr: 0.000002  loss_cl: 8.0596  loss_pitm: 0.0117  loss_mlm: 0.3413  loss_prd: 0.0418  loss_mrtd: 0.2243  time: 1.3384  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1900/2553]  eta: 0:14:34  lr: 0.000002  loss_cl: 8.2420  loss_pitm: 0.0657  loss_mlm: 0.3815  loss_prd: 0.5236  loss_mrtd: 0.2082  time: 1.3276  data: 0.0002  max mem: 17794
Train Epoch: [23]  [1950/2553]  eta: 0:13:27  lr: 0.000002  loss_cl: 8.1036  loss_pitm: 0.0416  loss_mlm: 0.6126  loss_prd: 0.2969  loss_mrtd: 0.2763  time: 1.3316  data: 0.0002  max mem: 17794
Train Epoch: [23]  [2000/2553]  eta: 0:12:20  lr: 0.000002  loss_cl: 7.7998  loss_pitm: 0.0111  loss_mlm: 0.7426  loss_prd: 0.3392  loss_mrtd: 0.2317  time: 1.3262  data: 0.0003  max mem: 17794
Train Epoch: [23]  [2050/2553]  eta: 0:11:13  lr: 0.000002  loss_cl: 8.0737  loss_pitm: 0.0937  loss_mlm: 0.4505  loss_prd: 0.3634  loss_mrtd: 0.2706  time: 1.3169  data: 0.0002  max mem: 17794
Train Epoch: [23]  [2100/2553]  eta: 0:10:06  lr: 0.000002  loss_cl: 8.0137  loss_pitm: 0.0162  loss_mlm: 0.2731  loss_prd: 0.2968  loss_mrtd: 0.1287  time: 1.3106  data: 0.0001  max mem: 17794
Train Epoch: [23]  [2150/2553]  eta: 0:08:59  lr: 0.000002  loss_cl: 8.0481  loss_pitm: 0.0528  loss_mlm: 0.1742  loss_prd: 0.1042  loss_mrtd: 0.2335  time: 1.3281  data: 0.0002  max mem: 17794
Train Epoch: [23]  [2200/2553]  eta: 0:07:52  lr: 0.000002  loss_cl: 7.6258  loss_pitm: 0.0094  loss_mlm: 0.4479  loss_prd: 0.0423  loss_mrtd: 0.1981  time: 1.3111  data: 0.0003  max mem: 17794
Train Epoch: [23]  [2250/2553]  eta: 0:06:45  lr: 0.000002  loss_cl: 7.5018  loss_pitm: 0.0109  loss_mlm: 0.5078  loss_prd: 0.0494  loss_mrtd: 0.2194  time: 1.3242  data: 0.0002  max mem: 17794
Train Epoch: [23]  [2300/2553]  eta: 0:05:38  lr: 0.000002  loss_cl: 7.7448  loss_pitm: 0.0170  loss_mlm: 0.3038  loss_prd: 0.4654  loss_mrtd: 0.1270  time: 1.3223  data: 0.0002  max mem: 17794
Train Epoch: [23]  [2350/2553]  eta: 0:04:31  lr: 0.000002  loss_cl: 7.3443  loss_pitm: 0.0085  loss_mlm: 0.5311  loss_prd: 0.0409  loss_mrtd: 0.2618  time: 1.3250  data: 0.0002  max mem: 17794
Train Epoch: [23]  [2400/2553]  eta: 0:03:24  lr: 0.000002  loss_cl: 7.7314  loss_pitm: 0.1170  loss_mlm: 0.4444  loss_prd: 0.5678  loss_mrtd: 0.2288  time: 1.3285  data: 0.0002  max mem: 17794
Train Epoch: [23]  [2450/2553]  eta: 0:02:17  lr: 0.000002  loss_cl: 7.3471  loss_pitm: 0.0048  loss_mlm: 0.4363  loss_prd: 0.1518  loss_mrtd: 0.1471  time: 1.3143  data: 0.0002  max mem: 17794
Train Epoch: [23]  [2500/2553]  eta: 0:01:10  lr: 0.000002  loss_cl: 7.1164  loss_pitm: 0.0110  loss_mlm: 0.3583  loss_prd: 0.3001  loss_mrtd: 0.2262  time: 1.3204  data: 0.0002  max mem: 17794
Train Epoch: [23]  [2550/2553]  eta: 0:00:04  lr: 0.000002  loss_cl: 6.7917  loss_pitm: 0.0041  loss_mlm: 0.3309  loss_prd: 0.0430  loss_mrtd: 0.1844  time: 1.3158  data: 0.0002  max mem: 17794
Train Epoch: [23]  [2552/2553]  eta: 0:00:01  lr: 0.000002  loss_cl: 6.5785  loss_pitm: 0.0144  loss_mlm: 0.5053  loss_prd: 0.0391  loss_mrtd: 0.2957  time: 1.3141  data: 0.0002  max mem: 17794
Train Epoch: [23] Total time: 0:56:49 (1.3355 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5775  loss_pitm: 0.0446  loss_mlm: 0.4185  loss_prd: 0.1892  loss_mrtd: 0.2165
Train Epoch的聚类: [24]  [   0/5241]  eta: 1:14:23    time: 0.8517  data: 0.8293  max mem: 17794
Train Epoch的聚类: [24]  [5240/5241]  eta: 0:00:00    time: 0.1717  data: 0.0003  max mem: 17794
Train Epoch的聚类: [24] Total time: 0:15:00 (0.1717 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4120
-1 (未归入任何簇) 的数量: 1646

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [24]  [   0/2556]  eta: 21:54:22  lr: 0.000002  loss_cl: 10.1595  loss_pitm: 0.0213  loss_mlm: 0.2954  loss_prd: 0.0670  loss_mrtd: 0.1839  time: 30.8540  data: 0.5410  max mem: 17794
Train Epoch: [24]  [  50/2556]  eta: 1:18:13  lr: 0.000002  loss_cl: 9.8547  loss_pitm: 0.0148  loss_mlm: 0.5989  loss_prd: 0.0490  loss_mrtd: 0.1928  time: 1.3104  data: 0.0002  max mem: 17794
Train Epoch: [24]  [ 100/2556]  eta: 1:05:16  lr: 0.000002  loss_cl: 9.6778  loss_pitm: 0.0544  loss_mlm: 0.6666  loss_prd: 0.2968  loss_mrtd: 0.2174  time: 1.3130  data: 0.0002  max mem: 17794
Train Epoch: [24]  [ 150/2556]  eta: 1:00:18  lr: 0.000002  loss_cl: 9.9233  loss_pitm: 0.1543  loss_mlm: 0.5538  loss_prd: 0.0451  loss_mrtd: 0.1693  time: 1.3217  data: 0.0002  max mem: 17794
Train Epoch: [24]  [ 200/2556]  eta: 0:57:26  lr: 0.000002  loss_cl: 9.2493  loss_pitm: 0.0046  loss_mlm: 0.2474  loss_prd: 0.2948  loss_mrtd: 0.1910  time: 1.3420  data: 0.0002  max mem: 17794
Train Epoch: [24]  [ 250/2556]  eta: 0:55:13  lr: 0.000002  loss_cl: 9.1723  loss_pitm: 0.0024  loss_mlm: 0.3071  loss_prd: 0.5908  loss_mrtd: 0.2269  time: 1.3263  data: 0.0003  max mem: 17794
Train Epoch: [24]  [ 300/2556]  eta: 0:53:22  lr: 0.000002  loss_cl: 8.8098  loss_pitm: 0.0047  loss_mlm: 0.4235  loss_prd: 0.3115  loss_mrtd: 0.2422  time: 1.3381  data: 0.0003  max mem: 17794
Train Epoch: [24]  [ 350/2556]  eta: 0:51:43  lr: 0.000002  loss_cl: 9.0524  loss_pitm: 0.0200  loss_mlm: 0.4236  loss_prd: 0.0634  loss_mrtd: 0.1917  time: 1.3267  data: 0.0003  max mem: 17794
Train Epoch: [24]  [ 400/2556]  eta: 0:50:12  lr: 0.000002  loss_cl: 9.4200  loss_pitm: 0.0030  loss_mlm: 0.8490  loss_prd: 0.0442  loss_mrtd: 0.1950  time: 1.3082  data: 0.0001  max mem: 17794
Train Epoch: [24]  [ 450/2556]  eta: 0:48:47  lr: 0.000002  loss_cl: 9.7528  loss_pitm: 0.0867  loss_mlm: 0.4154  loss_prd: 0.0486  loss_mrtd: 0.2211  time: 1.3201  data: 0.0003  max mem: 17794
Train Epoch: [24]  [ 500/2556]  eta: 0:47:25  lr: 0.000002  loss_cl: 9.3520  loss_pitm: 0.0037  loss_mlm: 0.5265  loss_prd: 0.0490  loss_mrtd: 0.2409  time: 1.3196  data: 0.0002  max mem: 17794
Train Epoch: [24]  [ 550/2556]  eta: 0:46:07  lr: 0.000002  loss_cl: 9.1143  loss_pitm: 0.0046  loss_mlm: 0.7602  loss_prd: 0.7004  loss_mrtd: 0.2940  time: 1.3380  data: 0.0002  max mem: 17794
Train Epoch: [24]  [ 600/2556]  eta: 0:44:50  lr: 0.000002  loss_cl: 9.2947  loss_pitm: 0.0022  loss_mlm: 0.2308  loss_prd: 0.4776  loss_mrtd: 0.1642  time: 1.3430  data: 0.0002  max mem: 17794
Train Epoch: [24]  [ 650/2556]  eta: 0:43:34  lr: 0.000002  loss_cl: 8.9991  loss_pitm: 0.0153  loss_mlm: 0.4477  loss_prd: 0.5486  loss_mrtd: 0.1929  time: 1.3229  data: 0.0002  max mem: 17794
Train Epoch: [24]  [ 700/2556]  eta: 0:42:21  lr: 0.000002  loss_cl: 8.9704  loss_pitm: 0.0197  loss_mlm: 0.5285  loss_prd: 0.3869  loss_mrtd: 0.1845  time: 1.3539  data: 0.0003  max mem: 17794
Train Epoch: [24]  [ 750/2556]  eta: 0:41:07  lr: 0.000002  loss_cl: 9.4001  loss_pitm: 0.0053  loss_mlm: 0.2038  loss_prd: 0.2573  loss_mrtd: 0.2365  time: 1.3264  data: 0.0002  max mem: 17794
Train Epoch: [24]  [ 800/2556]  eta: 0:39:54  lr: 0.000002  loss_cl: 9.4498  loss_pitm: 0.0085  loss_mlm: 0.2974  loss_prd: 0.0452  loss_mrtd: 0.2252  time: 1.3202  data: 0.0002  max mem: 17794
Train Epoch: [24]  [ 850/2556]  eta: 0:38:42  lr: 0.000002  loss_cl: 9.0729  loss_pitm: 0.0493  loss_mlm: 0.4297  loss_prd: 0.2766  loss_mrtd: 0.2519  time: 1.3345  data: 0.0003  max mem: 17794
Train Epoch: [24]  [ 900/2556]  eta: 0:37:31  lr: 0.000002  loss_cl: 9.2466  loss_pitm: 0.1615  loss_mlm: 0.7775  loss_prd: 0.0309  loss_mrtd: 0.2572  time: 1.3242  data: 0.0003  max mem: 17794
Train Epoch: [24]  [ 950/2556]  eta: 0:36:20  lr: 0.000002  loss_cl: 9.0205  loss_pitm: 0.0111  loss_mlm: 0.4163  loss_prd: 0.2983  loss_mrtd: 0.2635  time: 1.3280  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1000/2556]  eta: 0:35:10  lr: 0.000002  loss_cl: 8.8365  loss_pitm: 0.0156  loss_mlm: 0.5159  loss_prd: 0.2908  loss_mrtd: 0.2010  time: 1.3369  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1050/2556]  eta: 0:34:00  lr: 0.000002  loss_cl: 8.9398  loss_pitm: 0.1196  loss_mlm: 0.3551  loss_prd: 0.3126  loss_mrtd: 0.1901  time: 1.3097  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1100/2556]  eta: 0:32:50  lr: 0.000002  loss_cl: 8.7490  loss_pitm: 0.0121  loss_mlm: 0.3419  loss_prd: 0.5186  loss_mrtd: 0.2019  time: 1.3303  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1150/2556]  eta: 0:31:40  lr: 0.000002  loss_cl: 8.8333  loss_pitm: 0.0217  loss_mlm: 0.3710  loss_prd: 0.0803  loss_mrtd: 0.2835  time: 1.3168  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1200/2556]  eta: 0:30:31  lr: 0.000002  loss_cl: 8.7010  loss_pitm: 0.1486  loss_mlm: 0.4007  loss_prd: 0.0428  loss_mrtd: 0.1947  time: 1.3237  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1250/2556]  eta: 0:29:22  lr: 0.000002  loss_cl: 8.9440  loss_pitm: 0.0126  loss_mlm: 0.3605  loss_prd: 0.0566  loss_mrtd: 0.2068  time: 1.3244  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1300/2556]  eta: 0:28:13  lr: 0.000002  loss_cl: 8.6208  loss_pitm: 0.0205  loss_mlm: 0.2240  loss_prd: 0.1669  loss_mrtd: 0.2197  time: 1.3245  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1350/2556]  eta: 0:27:05  lr: 0.000002  loss_cl: 8.6799  loss_pitm: 0.0502  loss_mlm: 0.6848  loss_prd: 0.1600  loss_mrtd: 0.2295  time: 1.3364  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1400/2556]  eta: 0:25:57  lr: 0.000002  loss_cl: 8.3252  loss_pitm: 0.0091  loss_mlm: 0.6204  loss_prd: 0.0416  loss_mrtd: 0.2603  time: 1.3275  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1450/2556]  eta: 0:24:48  lr: 0.000002  loss_cl: 8.5202  loss_pitm: 0.0367  loss_mlm: 0.3004  loss_prd: 0.2861  loss_mrtd: 0.2856  time: 1.3225  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1500/2556]  eta: 0:23:40  lr: 0.000002  loss_cl: 8.1013  loss_pitm: 0.0370  loss_mlm: 0.4408  loss_prd: 0.3543  loss_mrtd: 0.2721  time: 1.3274  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1550/2556]  eta: 0:22:32  lr: 0.000002  loss_cl: 8.4283  loss_pitm: 0.0221  loss_mlm: 0.4245  loss_prd: 0.0585  loss_mrtd: 0.2274  time: 1.3250  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1600/2556]  eta: 0:21:25  lr: 0.000002  loss_cl: 8.2143  loss_pitm: 0.0590  loss_mlm: 0.3496  loss_prd: 0.0488  loss_mrtd: 0.2133  time: 1.3461  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1650/2556]  eta: 0:20:17  lr: 0.000002  loss_cl: 7.7858  loss_pitm: 0.0360  loss_mlm: 0.5349  loss_prd: 0.0580  loss_mrtd: 0.2765  time: 1.3246  data: 0.0002  max mem: 17794
Train Epoch: [24]  [1700/2556]  eta: 0:19:09  lr: 0.000002  loss_cl: 8.2011  loss_pitm: 0.0033  loss_mlm: 0.2133  loss_prd: 0.0432  loss_mrtd: 0.2082  time: 1.3208  data: 0.0002  max mem: 17795
Train Epoch: [24]  [1750/2556]  eta: 0:18:01  lr: 0.000002  loss_cl: 8.0725  loss_pitm: 0.0679  loss_mlm: 0.4605  loss_prd: 0.0580  loss_mrtd: 0.2965  time: 1.3254  data: 0.0002  max mem: 17795
Train Epoch: [24]  [1800/2556]  eta: 0:16:54  lr: 0.000002  loss_cl: 7.7684  loss_pitm: 0.0026  loss_mlm: 0.4219  loss_prd: 0.0522  loss_mrtd: 0.2483  time: 1.3176  data: 0.0002  max mem: 17795
Train Epoch: [24]  [1850/2556]  eta: 0:15:46  lr: 0.000002  loss_cl: 8.0479  loss_pitm: 0.0243  loss_mlm: 0.3960  loss_prd: 0.0526  loss_mrtd: 0.2141  time: 1.3313  data: 0.0002  max mem: 17795
Train Epoch: [24]  [1900/2556]  eta: 0:14:39  lr: 0.000002  loss_cl: 8.3664  loss_pitm: 0.0077  loss_mlm: 0.7302  loss_prd: 0.0476  loss_mrtd: 0.2147  time: 1.3221  data: 0.0002  max mem: 17795
Train Epoch: [24]  [1950/2556]  eta: 0:13:32  lr: 0.000002  loss_cl: 8.2172  loss_pitm: 0.0236  loss_mlm: 0.6285  loss_prd: 0.2589  loss_mrtd: 0.2320  time: 1.3320  data: 0.0002  max mem: 17795
Train Epoch: [24]  [2000/2556]  eta: 0:12:24  lr: 0.000002  loss_cl: 7.7946  loss_pitm: 0.1250  loss_mlm: 0.2918  loss_prd: 0.1604  loss_mrtd: 0.2577  time: 1.3115  data: 0.0002  max mem: 17795
Train Epoch: [24]  [2050/2556]  eta: 0:11:17  lr: 0.000002  loss_cl: 7.6765  loss_pitm: 0.0129  loss_mlm: 0.3157  loss_prd: 0.2857  loss_mrtd: 0.2046  time: 1.3211  data: 0.0002  max mem: 17795
Train Epoch: [24]  [2100/2556]  eta: 0:10:10  lr: 0.000002  loss_cl: 7.6213  loss_pitm: 0.1004  loss_mlm: 0.6586  loss_prd: 0.2772  loss_mrtd: 0.2303  time: 1.3181  data: 0.0002  max mem: 17795
Train Epoch: [24]  [2150/2556]  eta: 0:09:03  lr: 0.000002  loss_cl: 7.5569  loss_pitm: 0.1074  loss_mlm: 0.4732  loss_prd: 0.3818  loss_mrtd: 0.2455  time: 1.3226  data: 0.0002  max mem: 17795
Train Epoch: [24]  [2200/2556]  eta: 0:07:56  lr: 0.000002  loss_cl: 7.8151  loss_pitm: 0.0021  loss_mlm: 0.1872  loss_prd: 0.0568  loss_mrtd: 0.1898  time: 1.3184  data: 0.0002  max mem: 17795
Train Epoch: [24]  [2250/2556]  eta: 0:06:49  lr: 0.000002  loss_cl: 7.2998  loss_pitm: 0.3013  loss_mlm: 0.5143  loss_prd: 0.0476  loss_mrtd: 0.2108  time: 1.3195  data: 0.0002  max mem: 17795
Train Epoch: [24]  [2300/2556]  eta: 0:05:42  lr: 0.000002  loss_cl: 7.5753  loss_pitm: 0.0473  loss_mlm: 0.3650  loss_prd: 0.3259  loss_mrtd: 0.2146  time: 1.3200  data: 0.0002  max mem: 17795
Train Epoch: [24]  [2350/2556]  eta: 0:04:35  lr: 0.000002  loss_cl: 7.3262  loss_pitm: 0.0434  loss_mlm: 0.2721  loss_prd: 0.3433  loss_mrtd: 0.1870  time: 1.3145  data: 0.0002  max mem: 17795
Train Epoch: [24]  [2400/2556]  eta: 0:03:28  lr: 0.000002  loss_cl: 7.7609  loss_pitm: 0.0054  loss_mlm: 0.4538  loss_prd: 0.0477  loss_mrtd: 0.2231  time: 1.3332  data: 0.0002  max mem: 17795
Train Epoch: [24]  [2450/2556]  eta: 0:02:21  lr: 0.000002  loss_cl: 7.5519  loss_pitm: 0.0650  loss_mlm: 0.3497  loss_prd: 0.0640  loss_mrtd: 0.1589  time: 1.3084  data: 0.0002  max mem: 17795
Train Epoch: [24]  [2500/2556]  eta: 0:01:14  lr: 0.000002  loss_cl: 6.9746  loss_pitm: 0.0528  loss_mlm: 0.3982  loss_prd: 0.1002  loss_mrtd: 0.2203  time: 1.3282  data: 0.0002  max mem: 17795
Train Epoch: [24]  [2550/2556]  eta: 0:00:08  lr: 0.000002  loss_cl: 7.0149  loss_pitm: 0.2939  loss_mlm: 0.3591  loss_prd: 0.0521  loss_mrtd: 0.2102  time: 1.3184  data: 0.0003  max mem: 17795
Train Epoch: [24]  [2555/2556]  eta: 0:00:01  lr: 0.000002  loss_cl: 7.0786  loss_pitm: 0.0121  loss_mlm: 0.2278  loss_prd: 0.2688  loss_mrtd: 0.2051  time: 1.3158  data: 0.0002  max mem: 17795
Train Epoch: [24] Total time: 0:56:55 (1.3363 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5155  loss_pitm: 0.0447  loss_mlm: 0.4177  loss_prd: 0.1920  loss_mrtd: 0.2166
Computing features for evaluation...
Evaluation:  [   0/3079]  eta: 0:35:30    time: 0.6920  data: 0.0070  max mem: 17795
Evaluation:  [  50/3079]  eta: 0:19:20    time: 0.3781  data: 0.0000  max mem: 17795
Evaluation:  [ 100/3079]  eta: 0:18:57    time: 0.3863  data: 0.0000  max mem: 17795
Evaluation:  [ 150/3079]  eta: 0:18:36    time: 0.3832  data: 0.0000  max mem: 17795
Evaluation:  [ 200/3079]  eta: 0:18:18    time: 0.3884  data: 0.0000  max mem: 17795
Evaluation:  [ 250/3079]  eta: 0:17:56    time: 0.3789  data: 0.0000  max mem: 17795
Evaluation:  [ 300/3079]  eta: 0:17:39    time: 0.3879  data: 0.0000  max mem: 17795
Evaluation:  [ 350/3079]  eta: 0:17:18    time: 0.3754  data: 0.0000  max mem: 17795
Evaluation:  [ 400/3079]  eta: 0:17:00    time: 0.3846  data: 0.0000  max mem: 17795
Evaluation:  [ 450/3079]  eta: 0:16:41    time: 0.3836  data: 0.0000  max mem: 17795
Evaluation:  [ 500/3079]  eta: 0:16:25    time: 0.3871  data: 0.0000  max mem: 17795
Evaluation:  [ 550/3079]  eta: 0:16:10    time: 0.3975  data: 0.0000  max mem: 17795
Evaluation:  [ 600/3079]  eta: 0:15:52    time: 0.3887  data: 0.0000  max mem: 17795
Evaluation:  [ 650/3079]  eta: 0:15:34    time: 0.3915  data: 0.0000  max mem: 17795
Evaluation:  [ 700/3079]  eta: 0:15:15    time: 0.3891  data: 0.0000  max mem: 17795
Evaluation:  [ 750/3079]  eta: 0:14:57    time: 0.3866  data: 0.0000  max mem: 17795
Evaluation:  [ 800/3079]  eta: 0:14:39    time: 0.3930  data: 0.0000  max mem: 17795
Evaluation:  [ 850/3079]  eta: 0:14:21    time: 0.3902  data: 0.0000  max mem: 17795
Evaluation:  [ 900/3079]  eta: 0:14:01    time: 0.3892  data: 0.0000  max mem: 17795
Evaluation:  [ 950/3079]  eta: 0:13:44    time: 0.4017  data: 0.0000  max mem: 17795
Evaluation:  [1000/3079]  eta: 0:13:25    time: 0.3818  data: 0.0000  max mem: 17795
Evaluation:  [1050/3079]  eta: 0:13:06    time: 0.3870  data: 0.0000  max mem: 17795
Evaluation:  [1100/3079]  eta: 0:12:47    time: 0.3915  data: 0.0000  max mem: 17795
Evaluation:  [1150/3079]  eta: 0:12:28    time: 0.3850  data: 0.0000  max mem: 17795
Evaluation:  [1200/3079]  eta: 0:12:08    time: 0.3897  data: 0.0000  max mem: 17795
Evaluation:  [1250/3079]  eta: 0:11:49    time: 0.3892  data: 0.0000  max mem: 17795
Evaluation:  [1300/3079]  eta: 0:11:30    time: 0.3886  data: 0.0000  max mem: 17795
Evaluation:  [1350/3079]  eta: 0:11:11    time: 0.3884  data: 0.0000  max mem: 17795
Evaluation:  [1400/3079]  eta: 0:10:52    time: 0.3941  data: 0.0000  max mem: 17795
Evaluation:  [1450/3079]  eta: 0:10:32    time: 0.3851  data: 0.0000  max mem: 17795
Evaluation:  [1500/3079]  eta: 0:10:13    time: 0.3932  data: 0.0000  max mem: 17795
Evaluation:  [1550/3079]  eta: 0:09:54    time: 0.3891  data: 0.0000  max mem: 17795
Evaluation:  [1600/3079]  eta: 0:09:34    time: 0.3878  data: 0.0000  max mem: 17795
Evaluation:  [1650/3079]  eta: 0:09:15    time: 0.4023  data: 0.0000  max mem: 17795
Evaluation:  [1700/3079]  eta: 0:08:56    time: 0.3910  data: 0.0000  max mem: 17795
Evaluation:  [1750/3079]  eta: 0:08:36    time: 0.3832  data: 0.0000  max mem: 17795
Evaluation:  [1800/3079]  eta: 0:08:17    time: 0.3921  data: 0.0000  max mem: 17795
Evaluation:  [1850/3079]  eta: 0:07:58    time: 0.3911  data: 0.0000  max mem: 17795
Evaluation:  [1900/3079]  eta: 0:07:39    time: 0.3800  data: 0.0000  max mem: 17795
Evaluation:  [1950/3079]  eta: 0:07:19    time: 0.3862  data: 0.0000  max mem: 17795
Evaluation:  [2000/3079]  eta: 0:07:00    time: 0.3948  data: 0.0000  max mem: 17795
Evaluation:  [2050/3079]  eta: 0:06:40    time: 0.3810  data: 0.0000  max mem: 17795
Evaluation:  [2100/3079]  eta: 0:06:21    time: 0.3866  data: 0.0000  max mem: 17795
Evaluation:  [2150/3079]  eta: 0:06:01    time: 0.3950  data: 0.0000  max mem: 17795
Evaluation:  [2200/3079]  eta: 0:05:42    time: 0.3836  data: 0.0000  max mem: 17795
Evaluation:  [2250/3079]  eta: 0:05:22    time: 0.3933  data: 0.0000  max mem: 17795
Evaluation:  [2300/3079]  eta: 0:05:03    time: 0.3888  data: 0.0000  max mem: 17795
Evaluation:  [2350/3079]  eta: 0:04:43    time: 0.3934  data: 0.0000  max mem: 17795
Evaluation:  [2400/3079]  eta: 0:04:24    time: 0.3861  data: 0.0000  max mem: 17795
Evaluation:  [2450/3079]  eta: 0:04:04    time: 0.4030  data: 0.0000  max mem: 17795
Evaluation:  [2500/3079]  eta: 0:03:45    time: 0.3834  data: 0.0000  max mem: 17795
Evaluation:  [2550/3079]  eta: 0:03:25    time: 0.3950  data: 0.0000  max mem: 17795
Evaluation:  [2600/3079]  eta: 0:03:06    time: 0.3933  data: 0.0000  max mem: 17795
Evaluation:  [2650/3079]  eta: 0:02:46    time: 0.3894  data: 0.0000  max mem: 17795
Evaluation:  [2700/3079]  eta: 0:02:27    time: 0.3873  data: 0.0000  max mem: 17795
Evaluation:  [2750/3079]  eta: 0:02:08    time: 0.3973  data: 0.0000  max mem: 17795
Evaluation:  [2800/3079]  eta: 0:01:48    time: 0.3862  data: 0.0000  max mem: 17795
Evaluation:  [2850/3079]  eta: 0:01:29    time: 0.3869  data: 0.0000  max mem: 17795
Evaluation:  [2900/3079]  eta: 0:01:09    time: 0.4011  data: 0.0000  max mem: 17795
Evaluation:  [2950/3079]  eta: 0:00:50    time: 0.3750  data: 0.0000  max mem: 17795
Evaluation:  [3000/3079]  eta: 0:00:30    time: 0.3936  data: 0.0000  max mem: 17795
Evaluation:  [3050/3079]  eta: 0:00:11    time: 0.3900  data: 0.0000  max mem: 17795
Evaluation:  [3078/3079]  eta: 0:00:00    time: 0.3832  data: 0.0000  max mem: 17795
Evaluation: Total time: 0:20:00 (0.3899 s / it)
Evaluation time 0:21:30
Test: {'r1': 75.60103607177734, 'r5': 89.32748413085938, 'r10': 93.04743194580078, 'r_mean': 85.99198404947917, 'mAP': 67.65877532958984} 

Train Epoch的聚类: [25]  [   0/5241]  eta: 0:47:02    time: 0.5385  data: 0.5196  max mem: 17795
Train Epoch的聚类: [25]  [5240/5241]  eta: 0:00:00    time: 0.1718  data: 0.0002  max mem: 17795
Train Epoch的聚类: [25] Total time: 0:14:58 (0.1715 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4141
-1 (未归入任何簇) 的数量: 1699

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [25]  [   0/2554]  eta: 2 days, 10:01:47  lr: 0.000002  loss_cl: 9.4970  loss_pitm: 0.1928  loss_mlm: 0.4956  loss_prd: 0.0490  loss_mrtd: 0.2277  time: 81.7964  data: 0.7486  max mem: 17795
Train Epoch: [25]  [  50/2554]  eta: 1:59:44  lr: 0.000002  loss_cl: 10.0345  loss_pitm: 0.0565  loss_mlm: 0.4163  loss_prd: 0.3070  loss_mrtd: 0.2218  time: 1.3057  data: 0.0002  max mem: 18305
Train Epoch: [25]  [ 100/2554]  eta: 1:25:43  lr: 0.000002  loss_cl: 9.4403  loss_pitm: 0.0019  loss_mlm: 0.6004  loss_prd: 0.0459  loss_mrtd: 0.2811  time: 1.3272  data: 0.0002  max mem: 18400
Train Epoch: [25]  [ 150/2554]  eta: 1:13:41  lr: 0.000002  loss_cl: 9.4530  loss_pitm: 0.0325  loss_mlm: 0.5672  loss_prd: 0.0498  loss_mrtd: 0.2566  time: 1.3188  data: 0.0002  max mem: 18400
Train Epoch: [25]  [ 200/2554]  eta: 1:07:06  lr: 0.000002  loss_cl: 9.6018  loss_pitm: 0.0143  loss_mlm: 0.4604  loss_prd: 0.0449  loss_mrtd: 0.3242  time: 1.3335  data: 0.0002  max mem: 18400
Train Epoch: [25]  [ 250/2554]  eta: 1:02:43  lr: 0.000002  loss_cl: 9.5065  loss_pitm: 0.0775  loss_mlm: 0.5604  loss_prd: 0.0418  loss_mrtd: 0.2309  time: 1.3154  data: 0.0003  max mem: 18400
Train Epoch: [25]  [ 300/2554]  eta: 0:59:25  lr: 0.000002  loss_cl: 9.1616  loss_pitm: 0.0090  loss_mlm: 0.2382  loss_prd: 0.0535  loss_mrtd: 0.2357  time: 1.3230  data: 0.0002  max mem: 18400
Train Epoch: [25]  [ 350/2554]  eta: 0:56:43  lr: 0.000002  loss_cl: 9.2730  loss_pitm: 0.0129  loss_mlm: 0.6477  loss_prd: 0.0719  loss_mrtd: 0.2375  time: 1.3343  data: 0.0002  max mem: 18400
Train Epoch: [25]  [ 400/2554]  eta: 0:54:28  lr: 0.000002  loss_cl: 9.1750  loss_pitm: 0.1056  loss_mlm: 0.4184  loss_prd: 0.0685  loss_mrtd: 0.1893  time: 1.3423  data: 0.0002  max mem: 18400
Train Epoch: [25]  [ 450/2554]  eta: 0:52:26  lr: 0.000002  loss_cl: 8.9622  loss_pitm: 0.0252  loss_mlm: 0.4772  loss_prd: 0.0575  loss_mrtd: 0.2219  time: 1.3259  data: 0.0002  max mem: 18523
Train Epoch: [25]  [ 500/2554]  eta: 0:50:36  lr: 0.000002  loss_cl: 9.0366  loss_pitm: 0.0049  loss_mlm: 0.5826  loss_prd: 0.5175  loss_mrtd: 0.2112  time: 1.3294  data: 0.0002  max mem: 18523
Train Epoch: [25]  [ 550/2554]  eta: 0:48:54  lr: 0.000002  loss_cl: 9.1243  loss_pitm: 0.1662  loss_mlm: 0.5181  loss_prd: 0.3596  loss_mrtd: 0.2410  time: 1.3136  data: 0.0002  max mem: 18523
Train Epoch: [25]  [ 600/2554]  eta: 0:47:16  lr: 0.000002  loss_cl: 9.2495  loss_pitm: 0.0033  loss_mlm: 0.2642  loss_prd: 0.0710  loss_mrtd: 0.2193  time: 1.3162  data: 0.0002  max mem: 18589
Train Epoch: [25]  [ 650/2554]  eta: 0:45:45  lr: 0.000002  loss_cl: 8.8337  loss_pitm: 0.0179  loss_mlm: 0.4089  loss_prd: 0.7331  loss_mrtd: 0.1988  time: 1.3222  data: 0.0003  max mem: 18589
Train Epoch: [25]  [ 700/2554]  eta: 0:44:16  lr: 0.000002  loss_cl: 9.1938  loss_pitm: 0.0453  loss_mlm: 0.2788  loss_prd: 0.0693  loss_mrtd: 0.2109  time: 1.3190  data: 0.0002  max mem: 18589
Train Epoch: [25]  [ 750/2554]  eta: 0:42:50  lr: 0.000002  loss_cl: 9.3693  loss_pitm: 0.0025  loss_mlm: 0.5029  loss_prd: 0.0623  loss_mrtd: 0.3004  time: 1.3154  data: 0.0002  max mem: 18589
Train Epoch: [25]  [ 800/2554]  eta: 0:41:29  lr: 0.000002  loss_cl: 8.9775  loss_pitm: 0.0677  loss_mlm: 0.3866  loss_prd: 0.0494  loss_mrtd: 0.2327  time: 1.3316  data: 0.0002  max mem: 18589
Train Epoch: [25]  [ 850/2554]  eta: 0:40:08  lr: 0.000002  loss_cl: 9.1019  loss_pitm: 0.0053  loss_mlm: 0.3838  loss_prd: 0.2430  loss_mrtd: 0.2027  time: 1.3245  data: 0.0003  max mem: 18589
Train Epoch: [25]  [ 900/2554]  eta: 0:38:49  lr: 0.000002  loss_cl: 8.4399  loss_pitm: 0.0304  loss_mlm: 0.3146  loss_prd: 0.2927  loss_mrtd: 0.1075  time: 1.3151  data: 0.0002  max mem: 18589
Train Epoch: [25]  [ 950/2554]  eta: 0:37:31  lr: 0.000002  loss_cl: 8.8408  loss_pitm: 0.0346  loss_mlm: 0.5329  loss_prd: 0.5753  loss_mrtd: 0.2244  time: 1.3269  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1000/2554]  eta: 0:36:15  lr: 0.000002  loss_cl: 8.8150  loss_pitm: 0.0045  loss_mlm: 0.4979  loss_prd: 0.3364  loss_mrtd: 0.1433  time: 1.3022  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1050/2554]  eta: 0:34:59  lr: 0.000002  loss_cl: 8.9390  loss_pitm: 0.1534  loss_mlm: 0.4790  loss_prd: 0.0462  loss_mrtd: 0.1680  time: 1.3250  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1100/2554]  eta: 0:33:45  lr: 0.000002  loss_cl: 9.4330  loss_pitm: 0.0857  loss_mlm: 0.2827  loss_prd: 0.0742  loss_mrtd: 0.1924  time: 1.3473  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1150/2554]  eta: 0:32:31  lr: 0.000002  loss_cl: 8.6596  loss_pitm: 0.1063  loss_mlm: 0.5669  loss_prd: 0.5320  loss_mrtd: 0.2156  time: 1.3287  data: 0.0003  max mem: 18589
Train Epoch: [25]  [1200/2554]  eta: 0:31:18  lr: 0.000002  loss_cl: 8.3454  loss_pitm: 0.0161  loss_mlm: 0.5605  loss_prd: 0.4653  loss_mrtd: 0.2827  time: 1.3218  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1250/2554]  eta: 0:30:05  lr: 0.000002  loss_cl: 8.6233  loss_pitm: 0.0148  loss_mlm: 0.3269  loss_prd: 0.0823  loss_mrtd: 0.2400  time: 1.3203  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1300/2554]  eta: 0:28:53  lr: 0.000002  loss_cl: 8.0091  loss_pitm: 0.0176  loss_mlm: 0.5034  loss_prd: 0.0426  loss_mrtd: 0.2428  time: 1.3191  data: 0.0003  max mem: 18589
Train Epoch: [25]  [1350/2554]  eta: 0:27:41  lr: 0.000002  loss_cl: 8.8416  loss_pitm: 0.0145  loss_mlm: 0.2836  loss_prd: 0.3076  loss_mrtd: 0.1430  time: 1.3115  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1400/2554]  eta: 0:26:30  lr: 0.000002  loss_cl: 8.6765  loss_pitm: 0.0058  loss_mlm: 0.4319  loss_prd: 0.3126  loss_mrtd: 0.2012  time: 1.3255  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1450/2554]  eta: 0:25:18  lr: 0.000002  loss_cl: 8.6569  loss_pitm: 0.0830  loss_mlm: 0.3394  loss_prd: 0.3213  loss_mrtd: 0.2182  time: 1.3287  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1500/2554]  eta: 0:24:08  lr: 0.000002  loss_cl: 8.3285  loss_pitm: 0.0046  loss_mlm: 0.6078  loss_prd: 0.5015  loss_mrtd: 0.2572  time: 1.3478  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1550/2554]  eta: 0:22:58  lr: 0.000002  loss_cl: 7.8375  loss_pitm: 0.0133  loss_mlm: 0.3612  loss_prd: 0.1217  loss_mrtd: 0.2195  time: 1.3254  data: 0.0003  max mem: 18589
Train Epoch: [25]  [1600/2554]  eta: 0:21:48  lr: 0.000002  loss_cl: 7.9841  loss_pitm: 0.0073  loss_mlm: 0.4904  loss_prd: 0.2725  loss_mrtd: 0.1607  time: 1.3377  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1650/2554]  eta: 0:20:38  lr: 0.000002  loss_cl: 8.5560  loss_pitm: 0.2589  loss_mlm: 0.2758  loss_prd: 0.1709  loss_mrtd: 0.2011  time: 1.3449  data: 0.0003  max mem: 18589
Train Epoch: [25]  [1700/2554]  eta: 0:19:28  lr: 0.000002  loss_cl: 8.3704  loss_pitm: 0.1426  loss_mlm: 0.3416  loss_prd: 0.2943  loss_mrtd: 0.2184  time: 1.3255  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1750/2554]  eta: 0:18:19  lr: 0.000002  loss_cl: 8.1201  loss_pitm: 0.0102  loss_mlm: 0.4071  loss_prd: 0.8006  loss_mrtd: 0.1598  time: 1.3287  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1800/2554]  eta: 0:17:10  lr: 0.000002  loss_cl: 8.4220  loss_pitm: 0.0316  loss_mlm: 0.3749  loss_prd: 0.1545  loss_mrtd: 0.2052  time: 1.3297  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1850/2554]  eta: 0:16:01  lr: 0.000002  loss_cl: 7.8460  loss_pitm: 0.0114  loss_mlm: 0.5021  loss_prd: 0.0467  loss_mrtd: 0.1737  time: 1.3519  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1900/2554]  eta: 0:14:52  lr: 0.000002  loss_cl: 7.9975  loss_pitm: 0.0366  loss_mlm: 0.4114  loss_prd: 0.1177  loss_mrtd: 0.2649  time: 1.3305  data: 0.0002  max mem: 18589
Train Epoch: [25]  [1950/2554]  eta: 0:13:43  lr: 0.000002  loss_cl: 7.9865  loss_pitm: 0.1166  loss_mlm: 0.3802  loss_prd: 0.4084  loss_mrtd: 0.2330  time: 1.3390  data: 0.0003  max mem: 18589
Train Epoch: [25]  [2000/2554]  eta: 0:12:34  lr: 0.000002  loss_cl: 7.9231  loss_pitm: 0.0442  loss_mlm: 0.5059  loss_prd: 0.2938  loss_mrtd: 0.2765  time: 1.3193  data: 0.0002  max mem: 18589
Train Epoch: [25]  [2050/2554]  eta: 0:11:26  lr: 0.000002  loss_cl: 7.6024  loss_pitm: 0.0303  loss_mlm: 0.3414  loss_prd: 0.2230  loss_mrtd: 0.2050  time: 1.3160  data: 0.0002  max mem: 18589
Train Epoch: [25]  [2100/2554]  eta: 0:10:17  lr: 0.000002  loss_cl: 7.4837  loss_pitm: 0.0064  loss_mlm: 0.3687  loss_prd: 0.2969  loss_mrtd: 0.1796  time: 1.3373  data: 0.0002  max mem: 18589
Train Epoch: [25]  [2150/2554]  eta: 0:09:09  lr: 0.000002  loss_cl: 7.5073  loss_pitm: 0.0886  loss_mlm: 0.4924  loss_prd: 0.0340  loss_mrtd: 0.2078  time: 1.3071  data: 0.0003  max mem: 18589
Train Epoch: [25]  [2200/2554]  eta: 0:08:01  lr: 0.000002  loss_cl: 7.7676  loss_pitm: 0.0109  loss_mlm: 0.4162  loss_prd: 0.0401  loss_mrtd: 0.2995  time: 1.3332  data: 0.0003  max mem: 18589
Train Epoch: [25]  [2250/2554]  eta: 0:06:52  lr: 0.000002  loss_cl: 7.3102  loss_pitm: 0.0020  loss_mlm: 0.3432  loss_prd: 0.0509  loss_mrtd: 0.2493  time: 1.3333  data: 0.0003  max mem: 18589
Train Epoch: [25]  [2300/2554]  eta: 0:05:44  lr: 0.000002  loss_cl: 7.3662  loss_pitm: 0.0801  loss_mlm: 0.3286  loss_prd: 0.0460  loss_mrtd: 0.1561  time: 1.3267  data: 0.0002  max mem: 18589
Train Epoch: [25]  [2350/2554]  eta: 0:04:36  lr: 0.000002  loss_cl: 7.0839  loss_pitm: 0.0014  loss_mlm: 0.3689  loss_prd: 0.5092  loss_mrtd: 0.1806  time: 1.3140  data: 0.0002  max mem: 18589
Train Epoch: [25]  [2400/2554]  eta: 0:03:28  lr: 0.000002  loss_cl: 7.2778  loss_pitm: 0.0068  loss_mlm: 0.3841  loss_prd: 0.0479  loss_mrtd: 0.1735  time: 1.3336  data: 0.0003  max mem: 18589
Train Epoch: [25]  [2450/2554]  eta: 0:02:20  lr: 0.000002  loss_cl: 7.5296  loss_pitm: 0.0790  loss_mlm: 0.3262  loss_prd: 0.0419  loss_mrtd: 0.1930  time: 1.3249  data: 0.0002  max mem: 18589
Train Epoch: [25]  [2500/2554]  eta: 0:01:13  lr: 0.000002  loss_cl: 7.1561  loss_pitm: 0.1411  loss_mlm: 0.5767  loss_prd: 0.0471  loss_mrtd: 0.2481  time: 1.3251  data: 0.0003  max mem: 18589
Train Epoch: [25]  [2550/2554]  eta: 0:00:05  lr: 0.000002  loss_cl: 7.3111  loss_pitm: 0.0229  loss_mlm: 0.3997  loss_prd: 0.2705  loss_mrtd: 0.2498  time: 1.2984  data: 0.0002  max mem: 18589
Train Epoch: [25]  [2553/2554]  eta: 0:00:01  lr: 0.000002  loss_cl: 7.2341  loss_pitm: 0.0474  loss_mlm: 0.2794  loss_prd: 0.0771  loss_mrtd: 0.1745  time: 1.2990  data: 0.0002  max mem: 18589
Train Epoch: [25] Total time: 0:57:37 (1.3538 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5500  loss_pitm: 0.0418  loss_mlm: 0.4192  loss_prd: 0.1950  loss_mrtd: 0.2162
Computing features for evaluation...
Evaluation:  [   0/3079]  eta: 0:20:25    time: 0.3980  data: 0.0077  max mem: 18589
Evaluation:  [  50/3079]  eta: 0:19:23    time: 0.3788  data: 0.0000  max mem: 18589
Evaluation:  [ 100/3079]  eta: 0:19:16    time: 0.3971  data: 0.0000  max mem: 18589
Evaluation:  [ 150/3079]  eta: 0:18:57    time: 0.3923  data: 0.0000  max mem: 18589
Evaluation:  [ 200/3079]  eta: 0:18:41    time: 0.3964  data: 0.0000  max mem: 18589
Evaluation:  [ 250/3079]  eta: 0:18:18    time: 0.3858  data: 0.0000  max mem: 18589
Evaluation:  [ 300/3079]  eta: 0:18:00    time: 0.3996  data: 0.0000  max mem: 18589
Evaluation:  [ 350/3079]  eta: 0:17:42    time: 0.3892  data: 0.0000  max mem: 18589
Evaluation:  [ 400/3079]  eta: 0:17:23    time: 0.3952  data: 0.0000  max mem: 18589
Evaluation:  [ 450/3079]  eta: 0:17:03    time: 0.3884  data: 0.0000  max mem: 18589
Evaluation:  [ 500/3079]  eta: 0:16:44    time: 0.3981  data: 0.0000  max mem: 18589
Evaluation:  [ 550/3079]  eta: 0:16:25    time: 0.3952  data: 0.0000  max mem: 18589
Evaluation:  [ 600/3079]  eta: 0:16:07    time: 0.4000  data: 0.0000  max mem: 18589
Evaluation:  [ 650/3079]  eta: 0:15:50    time: 0.4157  data: 0.0000  max mem: 18589
Evaluation:  [ 700/3079]  eta: 0:15:31    time: 0.3900  data: 0.0000  max mem: 18589
Evaluation:  [ 750/3079]  eta: 0:15:10    time: 0.3763  data: 0.0000  max mem: 18589
Evaluation:  [ 800/3079]  eta: 0:14:49    time: 0.3778  data: 0.0000  max mem: 18589
Evaluation:  [ 850/3079]  eta: 0:14:29    time: 0.3853  data: 0.0000  max mem: 18589
Evaluation:  [ 900/3079]  eta: 0:14:10    time: 0.3975  data: 0.0000  max mem: 18589
Evaluation:  [ 950/3079]  eta: 0:13:51    time: 0.4010  data: 0.0000  max mem: 18589
Evaluation:  [1000/3079]  eta: 0:13:32    time: 0.3935  data: 0.0000  max mem: 18589
Evaluation:  [1050/3079]  eta: 0:13:13    time: 0.3939  data: 0.0000  max mem: 18589
Evaluation:  [1100/3079]  eta: 0:12:54    time: 0.4039  data: 0.0000  max mem: 18589
Evaluation:  [1150/3079]  eta: 0:12:35    time: 0.3885  data: 0.0000  max mem: 18589
Evaluation:  [1200/3079]  eta: 0:12:15    time: 0.3904  data: 0.0000  max mem: 18589
Evaluation:  [1250/3079]  eta: 0:11:56    time: 0.3957  data: 0.0000  max mem: 18589
Evaluation:  [1300/3079]  eta: 0:11:36    time: 0.3875  data: 0.0000  max mem: 18589
Evaluation:  [1350/3079]  eta: 0:11:17    time: 0.3915  data: 0.0000  max mem: 18589
Evaluation:  [1400/3079]  eta: 0:10:57    time: 0.3978  data: 0.0000  max mem: 18589
Evaluation:  [1450/3079]  eta: 0:10:39    time: 0.3882  data: 0.0000  max mem: 18589
Evaluation:  [1500/3079]  eta: 0:10:20    time: 0.3963  data: 0.0000  max mem: 18589
Evaluation:  [1550/3079]  eta: 0:10:01    time: 0.3957  data: 0.0000  max mem: 18589
Evaluation:  [1600/3079]  eta: 0:09:41    time: 0.3900  data: 0.0000  max mem: 18589
Evaluation:  [1650/3079]  eta: 0:09:21    time: 0.3908  data: 0.0000  max mem: 18589
Evaluation:  [1700/3079]  eta: 0:09:02    time: 0.3996  data: 0.0000  max mem: 18589
Evaluation:  [1750/3079]  eta: 0:08:42    time: 0.3957  data: 0.0000  max mem: 18589
Evaluation:  [1800/3079]  eta: 0:08:22    time: 0.4017  data: 0.0000  max mem: 18589
Evaluation:  [1850/3079]  eta: 0:08:03    time: 0.3939  data: 0.0000  max mem: 18589
Evaluation:  [1900/3079]  eta: 0:07:43    time: 0.3930  data: 0.0000  max mem: 18589
Evaluation:  [1950/3079]  eta: 0:07:23    time: 0.3862  data: 0.0000  max mem: 18589
Evaluation:  [2000/3079]  eta: 0:07:04    time: 0.3981  data: 0.0000  max mem: 18589
Evaluation:  [2050/3079]  eta: 0:06:44    time: 0.3887  data: 0.0000  max mem: 18589
Evaluation:  [2100/3079]  eta: 0:06:24    time: 0.3889  data: 0.0000  max mem: 18589
Evaluation:  [2150/3079]  eta: 0:06:04    time: 0.3943  data: 0.0000  max mem: 18589
Evaluation:  [2200/3079]  eta: 0:05:45    time: 0.3873  data: 0.0000  max mem: 18589
Evaluation:  [2250/3079]  eta: 0:05:25    time: 0.3875  data: 0.0000  max mem: 18589
Evaluation:  [2300/3079]  eta: 0:05:05    time: 0.3910  data: 0.0000  max mem: 18589
Evaluation:  [2350/3079]  eta: 0:04:46    time: 0.3873  data: 0.0000  max mem: 18589
Evaluation:  [2400/3079]  eta: 0:04:26    time: 0.3836  data: 0.0000  max mem: 18589
Evaluation:  [2450/3079]  eta: 0:04:06    time: 0.3917  data: 0.0000  max mem: 18589
Evaluation:  [2500/3079]  eta: 0:03:47    time: 0.3908  data: 0.0000  max mem: 18589
Evaluation:  [2550/3079]  eta: 0:03:27    time: 0.3902  data: 0.0000  max mem: 18589
Evaluation:  [2600/3079]  eta: 0:03:08    time: 0.3942  data: 0.0000  max mem: 18589
Evaluation:  [2650/3079]  eta: 0:02:48    time: 0.3872  data: 0.0000  max mem: 18589
Evaluation:  [2700/3079]  eta: 0:02:28    time: 0.3884  data: 0.0000  max mem: 18589
Evaluation:  [2750/3079]  eta: 0:02:09    time: 0.3994  data: 0.0000  max mem: 18589
Evaluation:  [2800/3079]  eta: 0:01:49    time: 0.3869  data: 0.0000  max mem: 18589
Evaluation:  [2850/3079]  eta: 0:01:29    time: 0.3840  data: 0.0000  max mem: 18589
Evaluation:  [2900/3079]  eta: 0:01:10    time: 0.3883  data: 0.0000  max mem: 18589
Evaluation:  [2950/3079]  eta: 0:00:50    time: 0.3948  data: 0.0000  max mem: 18589
Evaluation:  [3000/3079]  eta: 0:00:31    time: 0.3843  data: 0.0000  max mem: 18589
Evaluation:  [3050/3079]  eta: 0:00:11    time: 0.3882  data: 0.0000  max mem: 18589
Evaluation:  [3078/3079]  eta: 0:00:00    time: 0.3912  data: 0.0000  max mem: 18589
Evaluation: Total time: 0:20:09 (0.3927 s / it)
Evaluation time 0:21:21
Test: {'r1': 75.666015625, 'r5': 89.44119262695312, 'r10': 92.90123748779297, 'r_mean': 86.00281524658203, 'mAP': 67.56903076171875} 

Train Epoch的聚类: [26]  [   0/5241]  eta: 1:00:08    time: 0.6885  data: 0.6610  max mem: 18589
Train Epoch的聚类: [26]  [5240/5241]  eta: 0:00:00    time: 0.1720  data: 0.0002  max mem: 18589
Train Epoch的聚类: [26] Total time: 0:14:59 (0.1717 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4097
-1 (未归入任何簇) 的数量: 1618

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [26]  [   0/2558]  eta: 21:56:16  lr: 0.000001  loss_cl: 10.3775  loss_pitm: 0.0082  loss_mlm: 0.4884  loss_prd: 0.2935  loss_mrtd: 0.2439  time: 30.8744  data: 0.5490  max mem: 18589
Train Epoch: [26]  [  50/2558]  eta: 1:17:21  lr: 0.000001  loss_cl: 9.4357  loss_pitm: 0.0518  loss_mlm: 0.4667  loss_prd: 0.2310  loss_mrtd: 0.1592  time: 1.2770  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 100/2558]  eta: 1:04:33  lr: 0.000001  loss_cl: 9.6285  loss_pitm: 0.0736  loss_mlm: 0.2613  loss_prd: 0.5420  loss_mrtd: 0.2188  time: 1.2975  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 150/2558]  eta: 0:59:33  lr: 0.000001  loss_cl: 10.1316  loss_pitm: 0.0123  loss_mlm: 0.5528  loss_prd: 0.2533  loss_mrtd: 0.2481  time: 1.3084  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 200/2558]  eta: 0:56:26  lr: 0.000001  loss_cl: 9.3558  loss_pitm: 0.0070  loss_mlm: 0.5316  loss_prd: 0.2737  loss_mrtd: 0.2301  time: 1.2989  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 250/2558]  eta: 0:54:17  lr: 0.000001  loss_cl: 9.7072  loss_pitm: 0.0015  loss_mlm: 0.3525  loss_prd: 0.0580  loss_mrtd: 0.1588  time: 1.3128  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 300/2558]  eta: 0:52:28  lr: 0.000001  loss_cl: 9.4212  loss_pitm: 0.0756  loss_mlm: 0.2636  loss_prd: 0.0454  loss_mrtd: 0.2514  time: 1.3159  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 350/2558]  eta: 0:50:57  lr: 0.000001  loss_cl: 9.2255  loss_pitm: 0.1639  loss_mlm: 0.0996  loss_prd: 0.5534  loss_mrtd: 0.1785  time: 1.3222  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 400/2558]  eta: 0:49:27  lr: 0.000001  loss_cl: 8.9512  loss_pitm: 0.0041  loss_mlm: 0.4683  loss_prd: 0.2564  loss_mrtd: 0.2026  time: 1.3050  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 450/2558]  eta: 0:48:03  lr: 0.000001  loss_cl: 9.2788  loss_pitm: 0.0025  loss_mlm: 0.3603  loss_prd: 0.0452  loss_mrtd: 0.1856  time: 1.3105  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 500/2558]  eta: 0:46:45  lr: 0.000001  loss_cl: 9.3210  loss_pitm: 0.0443  loss_mlm: 0.1709  loss_prd: 0.0555  loss_mrtd: 0.1908  time: 1.3113  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 550/2558]  eta: 0:45:27  lr: 0.000001  loss_cl: 9.5256  loss_pitm: 0.0069  loss_mlm: 0.4227  loss_prd: 0.7196  loss_mrtd: 0.2076  time: 1.3136  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 600/2558]  eta: 0:44:11  lr: 0.000001  loss_cl: 9.0928  loss_pitm: 0.0029  loss_mlm: 0.2630  loss_prd: 0.0412  loss_mrtd: 0.1900  time: 1.3050  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 650/2558]  eta: 0:42:56  lr: 0.000001  loss_cl: 9.3320  loss_pitm: 0.0422  loss_mlm: 0.2780  loss_prd: 0.0453  loss_mrtd: 0.1866  time: 1.3072  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 700/2558]  eta: 0:41:43  lr: 0.000001  loss_cl: 9.2906  loss_pitm: 0.0367  loss_mlm: 0.2735  loss_prd: 0.0443  loss_mrtd: 0.2001  time: 1.3050  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 750/2558]  eta: 0:40:31  lr: 0.000001  loss_cl: 9.4797  loss_pitm: 0.0069  loss_mlm: 0.4547  loss_prd: 0.0396  loss_mrtd: 0.1871  time: 1.3153  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 800/2558]  eta: 0:39:20  lr: 0.000001  loss_cl: 8.7723  loss_pitm: 0.0067  loss_mlm: 0.4237  loss_prd: 0.0576  loss_mrtd: 0.2149  time: 1.3040  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 850/2558]  eta: 0:38:10  lr: 0.000001  loss_cl: 9.1785  loss_pitm: 0.0250  loss_mlm: 0.2761  loss_prd: 0.0489  loss_mrtd: 0.2356  time: 1.3118  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 900/2558]  eta: 0:37:01  lr: 0.000001  loss_cl: 8.8938  loss_pitm: 0.2178  loss_mlm: 0.5750  loss_prd: 0.2338  loss_mrtd: 0.2888  time: 1.3281  data: 0.0001  max mem: 18589
Train Epoch: [26]  [ 950/2558]  eta: 0:35:53  lr: 0.000001  loss_cl: 8.6990  loss_pitm: 0.0040  loss_mlm: 0.3131  loss_prd: 0.3240  loss_mrtd: 0.2200  time: 1.3233  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1000/2558]  eta: 0:34:44  lr: 0.000001  loss_cl: 8.9336  loss_pitm: 0.1139  loss_mlm: 0.5823  loss_prd: 0.4781  loss_mrtd: 0.2944  time: 1.3046  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1050/2558]  eta: 0:33:34  lr: 0.000001  loss_cl: 8.9141  loss_pitm: 0.1199  loss_mlm: 0.6315  loss_prd: 0.0469  loss_mrtd: 0.2388  time: 1.3115  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1100/2558]  eta: 0:32:26  lr: 0.000001  loss_cl: 8.9766  loss_pitm: 0.1556  loss_mlm: 0.3593  loss_prd: 0.0440  loss_mrtd: 0.1996  time: 1.3078  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1150/2558]  eta: 0:31:18  lr: 0.000001  loss_cl: 8.7533  loss_pitm: 0.0084  loss_mlm: 0.2884  loss_prd: 0.0478  loss_mrtd: 0.2095  time: 1.3097  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1200/2558]  eta: 0:30:10  lr: 0.000001  loss_cl: 9.0501  loss_pitm: 0.0119  loss_mlm: 0.3341  loss_prd: 0.0392  loss_mrtd: 0.2875  time: 1.3118  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1250/2558]  eta: 0:29:02  lr: 0.000001  loss_cl: 8.4781  loss_pitm: 0.0717  loss_mlm: 0.7342  loss_prd: 0.0504  loss_mrtd: 0.2050  time: 1.3087  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1300/2558]  eta: 0:27:54  lr: 0.000001  loss_cl: 8.2914  loss_pitm: 0.0089  loss_mlm: 0.1987  loss_prd: 0.0425  loss_mrtd: 0.2030  time: 1.2991  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1350/2558]  eta: 0:26:46  lr: 0.000001  loss_cl: 8.3284  loss_pitm: 0.0241  loss_mlm: 0.3953  loss_prd: 0.4885  loss_mrtd: 0.2510  time: 1.3001  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1400/2558]  eta: 0:25:39  lr: 0.000001  loss_cl: 8.9106  loss_pitm: 0.0050  loss_mlm: 0.4029  loss_prd: 0.2511  loss_mrtd: 0.2296  time: 1.3117  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1450/2558]  eta: 0:24:31  lr: 0.000001  loss_cl: 8.4248  loss_pitm: 0.0039  loss_mlm: 0.4230  loss_prd: 0.0536  loss_mrtd: 0.2158  time: 1.2967  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1500/2558]  eta: 0:23:24  lr: 0.000001  loss_cl: 8.3933  loss_pitm: 0.0109  loss_mlm: 0.3659  loss_prd: 0.0623  loss_mrtd: 0.2082  time: 1.3114  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1550/2558]  eta: 0:22:17  lr: 0.000001  loss_cl: 8.4073  loss_pitm: 0.0032  loss_mlm: 0.5868  loss_prd: 0.5678  loss_mrtd: 0.2786  time: 1.3076  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1600/2558]  eta: 0:21:10  lr: 0.000001  loss_cl: 8.4609  loss_pitm: 0.0090  loss_mlm: 0.1800  loss_prd: 0.4395  loss_mrtd: 0.2087  time: 1.3269  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1650/2558]  eta: 0:20:04  lr: 0.000001  loss_cl: 8.4883  loss_pitm: 0.0165  loss_mlm: 0.3214  loss_prd: 0.0797  loss_mrtd: 0.2469  time: 1.3322  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1700/2558]  eta: 0:18:57  lr: 0.000001  loss_cl: 8.2447  loss_pitm: 0.1376  loss_mlm: 0.4823  loss_prd: 0.0393  loss_mrtd: 0.2600  time: 1.2924  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1750/2558]  eta: 0:17:50  lr: 0.000001  loss_cl: 7.8217  loss_pitm: 0.0770  loss_mlm: 0.3411  loss_prd: 0.0422  loss_mrtd: 0.2262  time: 1.3015  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1800/2558]  eta: 0:16:44  lr: 0.000001  loss_cl: 8.2279  loss_pitm: 0.0135  loss_mlm: 0.5329  loss_prd: 0.0495  loss_mrtd: 0.3575  time: 1.3090  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1850/2558]  eta: 0:15:37  lr: 0.000001  loss_cl: 7.6882  loss_pitm: 0.0110  loss_mlm: 0.5128  loss_prd: 0.1919  loss_mrtd: 0.2331  time: 1.3229  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1900/2558]  eta: 0:14:31  lr: 0.000001  loss_cl: 7.7979  loss_pitm: 0.0514  loss_mlm: 0.4248  loss_prd: 0.1573  loss_mrtd: 0.1952  time: 1.3185  data: 0.0001  max mem: 18589
Train Epoch: [26]  [1950/2558]  eta: 0:13:24  lr: 0.000001  loss_cl: 8.1128  loss_pitm: 0.0075  loss_mlm: 0.4941  loss_prd: 0.3162  loss_mrtd: 0.2623  time: 1.3087  data: 0.0001  max mem: 18589
Train Epoch: [26]  [2000/2558]  eta: 0:12:18  lr: 0.000001  loss_cl: 7.7778  loss_pitm: 0.0851  loss_mlm: 0.3695  loss_prd: 0.0376  loss_mrtd: 0.2175  time: 1.3152  data: 0.0001  max mem: 18589
Train Epoch: [26]  [2050/2558]  eta: 0:11:12  lr: 0.000001  loss_cl: 8.3625  loss_pitm: 0.1194  loss_mlm: 0.3883  loss_prd: 0.4619  loss_mrtd: 0.2077  time: 1.2967  data: 0.0001  max mem: 18589
Train Epoch: [26]  [2100/2558]  eta: 0:10:05  lr: 0.000001  loss_cl: 7.8073  loss_pitm: 0.0257  loss_mlm: 0.3101  loss_prd: 0.5072  loss_mrtd: 0.1696  time: 1.3073  data: 0.0001  max mem: 18589
Train Epoch: [26]  [2150/2558]  eta: 0:08:59  lr: 0.000001  loss_cl: 7.2100  loss_pitm: 0.0087  loss_mlm: 0.5068  loss_prd: 0.0412  loss_mrtd: 0.1857  time: 1.2920  data: 0.0001  max mem: 18589
Train Epoch: [26]  [2200/2558]  eta: 0:07:53  lr: 0.000001  loss_cl: 7.6106  loss_pitm: 0.0025  loss_mlm: 0.2414  loss_prd: 0.0380  loss_mrtd: 0.1526  time: 1.3024  data: 0.0001  max mem: 18589
Train Epoch: [26]  [2250/2558]  eta: 0:06:46  lr: 0.000001  loss_cl: 7.4243  loss_pitm: 0.0062  loss_mlm: 0.3261  loss_prd: 0.2522  loss_mrtd: 0.2203  time: 1.3222  data: 0.0001  max mem: 18589
Train Epoch: [26]  [2300/2558]  eta: 0:05:40  lr: 0.000001  loss_cl: 7.7032  loss_pitm: 0.0692  loss_mlm: 0.5588  loss_prd: 0.0496  loss_mrtd: 0.2603  time: 1.2980  data: 0.0001  max mem: 18589
Train Epoch: [26]  [2350/2558]  eta: 0:04:34  lr: 0.000001  loss_cl: 7.6964  loss_pitm: 0.0126  loss_mlm: 0.3083  loss_prd: 0.2953  loss_mrtd: 0.1496  time: 1.3053  data: 0.0001  max mem: 18589
Train Epoch: [26]  [2400/2558]  eta: 0:03:28  lr: 0.000001  loss_cl: 7.5717  loss_pitm: 0.0037  loss_mlm: 0.5938  loss_prd: 0.0453  loss_mrtd: 0.2673  time: 1.3168  data: 0.0001  max mem: 18589
Train Epoch: [26]  [2450/2558]  eta: 0:02:22  lr: 0.000001  loss_cl: 7.4020  loss_pitm: 0.0032  loss_mlm: 0.5211  loss_prd: 0.0476  loss_mrtd: 0.2201  time: 1.3132  data: 0.0001  max mem: 18589
Train Epoch: [26]  [2500/2558]  eta: 0:01:16  lr: 0.000001  loss_cl: 7.1817  loss_pitm: 0.0125  loss_mlm: 0.2526  loss_prd: 0.2763  loss_mrtd: 0.2585  time: 1.3259  data: 0.0001  max mem: 18589
Train Epoch: [26]  [2550/2558]  eta: 0:00:10  lr: 0.000001  loss_cl: 7.1464  loss_pitm: 0.0642  loss_mlm: 0.3426  loss_prd: 0.7491  loss_mrtd: 0.2533  time: 1.3048  data: 0.0002  max mem: 18589
Train Epoch: [26]  [2557/2558]  eta: 0:00:01  lr: 0.000001  loss_cl: 7.6282  loss_pitm: 0.0058  loss_mlm: 0.5756  loss_prd: 0.3322  loss_mrtd: 0.2379  time: 1.2988  data: 0.0002  max mem: 18589
Train Epoch: [26] Total time: 0:56:16 (1.3200 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5727  loss_pitm: 0.0436  loss_mlm: 0.4116  loss_prd: 0.1898  loss_mrtd: 0.2154
Computing features for evaluation...
Evaluation:  [   0/3079]  eta: 0:19:18    time: 0.3763  data: 0.0056  max mem: 18589
Evaluation:  [  50/3079]  eta: 0:18:26    time: 0.3648  data: 0.0000  max mem: 18589
Evaluation:  [ 100/3079]  eta: 0:18:08    time: 0.3685  data: 0.0000  max mem: 18589
Evaluation:  [ 150/3079]  eta: 0:17:56    time: 0.3731  data: 0.0000  max mem: 18589
Evaluation:  [ 200/3079]  eta: 0:17:41    time: 0.3690  data: 0.0000  max mem: 18589
Evaluation:  [ 250/3079]  eta: 0:17:22    time: 0.3688  data: 0.0000  max mem: 18589
Evaluation:  [ 300/3079]  eta: 0:17:04    time: 0.3722  data: 0.0000  max mem: 18589
Evaluation:  [ 350/3079]  eta: 0:16:47    time: 0.3712  data: 0.0000  max mem: 18589
Evaluation:  [ 400/3079]  eta: 0:16:29    time: 0.3691  data: 0.0000  max mem: 18589
Evaluation:  [ 450/3079]  eta: 0:16:11    time: 0.3736  data: 0.0000  max mem: 18589
Evaluation:  [ 500/3079]  eta: 0:15:53    time: 0.3707  data: 0.0000  max mem: 18589
Evaluation:  [ 550/3079]  eta: 0:15:34    time: 0.3688  data: 0.0000  max mem: 18589
Evaluation:  [ 600/3079]  eta: 0:15:16    time: 0.3731  data: 0.0000  max mem: 18589
Evaluation:  [ 650/3079]  eta: 0:14:58    time: 0.3744  data: 0.0000  max mem: 18589
Evaluation:  [ 700/3079]  eta: 0:14:39    time: 0.3663  data: 0.0000  max mem: 18589
Evaluation:  [ 750/3079]  eta: 0:14:20    time: 0.3705  data: 0.0000  max mem: 18589
Evaluation:  [ 800/3079]  eta: 0:14:02    time: 0.3745  data: 0.0000  max mem: 18589
Evaluation:  [ 850/3079]  eta: 0:13:44    time: 0.3692  data: 0.0000  max mem: 18589
Evaluation:  [ 900/3079]  eta: 0:13:25    time: 0.3711  data: 0.0000  max mem: 18589
Evaluation:  [ 950/3079]  eta: 0:13:07    time: 0.3741  data: 0.0000  max mem: 18589
Evaluation:  [1000/3079]  eta: 0:12:48    time: 0.3657  data: 0.0000  max mem: 18589
Evaluation:  [1050/3079]  eta: 0:12:30    time: 0.3698  data: 0.0000  max mem: 18589
Evaluation:  [1100/3079]  eta: 0:12:12    time: 0.3743  data: 0.0000  max mem: 18589
Evaluation:  [1150/3079]  eta: 0:11:53    time: 0.3680  data: 0.0000  max mem: 18589
Evaluation:  [1200/3079]  eta: 0:11:34    time: 0.3677  data: 0.0000  max mem: 18589
Evaluation:  [1250/3079]  eta: 0:11:16    time: 0.3698  data: 0.0000  max mem: 18589
Evaluation:  [1300/3079]  eta: 0:10:58    time: 0.3700  data: 0.0000  max mem: 18589
Evaluation:  [1350/3079]  eta: 0:10:39    time: 0.3672  data: 0.0000  max mem: 18589
Evaluation:  [1400/3079]  eta: 0:10:21    time: 0.3718  data: 0.0000  max mem: 18589
Evaluation:  [1450/3079]  eta: 0:10:02    time: 0.3715  data: 0.0000  max mem: 18589
Evaluation:  [1500/3079]  eta: 0:09:44    time: 0.3678  data: 0.0000  max mem: 18589
Evaluation:  [1550/3079]  eta: 0:09:25    time: 0.3702  data: 0.0000  max mem: 18589
Evaluation:  [1600/3079]  eta: 0:09:07    time: 0.3716  data: 0.0000  max mem: 18589
Evaluation:  [1650/3079]  eta: 0:08:48    time: 0.3677  data: 0.0000  max mem: 18589
Evaluation:  [1700/3079]  eta: 0:08:30    time: 0.3718  data: 0.0000  max mem: 18589
Evaluation:  [1750/3079]  eta: 0:08:11    time: 0.3719  data: 0.0000  max mem: 18589
Evaluation:  [1800/3079]  eta: 0:07:53    time: 0.3690  data: 0.0000  max mem: 18589
Evaluation:  [1850/3079]  eta: 0:07:34    time: 0.3728  data: 0.0000  max mem: 18589
Evaluation:  [1900/3079]  eta: 0:07:16    time: 0.3718  data: 0.0000  max mem: 18589
Evaluation:  [1950/3079]  eta: 0:06:57    time: 0.3668  data: 0.0000  max mem: 18589
Evaluation:  [2000/3079]  eta: 0:06:39    time: 0.3716  data: 0.0000  max mem: 18589
Evaluation:  [2050/3079]  eta: 0:06:20    time: 0.3733  data: 0.0000  max mem: 18589
Evaluation:  [2100/3079]  eta: 0:06:02    time: 0.3680  data: 0.0000  max mem: 18589
Evaluation:  [2150/3079]  eta: 0:05:43    time: 0.3722  data: 0.0000  max mem: 18589
Evaluation:  [2200/3079]  eta: 0:05:25    time: 0.3731  data: 0.0000  max mem: 18589
Evaluation:  [2250/3079]  eta: 0:05:06    time: 0.3697  data: 0.0000  max mem: 18589
Evaluation:  [2300/3079]  eta: 0:04:48    time: 0.3691  data: 0.0000  max mem: 18589
Evaluation:  [2350/3079]  eta: 0:04:29    time: 0.3751  data: 0.0000  max mem: 18589
Evaluation:  [2400/3079]  eta: 0:04:11    time: 0.3673  data: 0.0000  max mem: 18589
Evaluation:  [2450/3079]  eta: 0:03:52    time: 0.3692  data: 0.0000  max mem: 18589
Evaluation:  [2500/3079]  eta: 0:03:34    time: 0.3753  data: 0.0000  max mem: 18589
Evaluation:  [2550/3079]  eta: 0:03:15    time: 0.3694  data: 0.0000  max mem: 18589
Evaluation:  [2600/3079]  eta: 0:02:57    time: 0.3693  data: 0.0000  max mem: 18589
Evaluation:  [2650/3079]  eta: 0:02:38    time: 0.3708  data: 0.0000  max mem: 18589
Evaluation:  [2700/3079]  eta: 0:02:20    time: 0.3716  data: 0.0000  max mem: 18589
Evaluation:  [2750/3079]  eta: 0:02:01    time: 0.3665  data: 0.0000  max mem: 18589
Evaluation:  [2800/3079]  eta: 0:01:43    time: 0.3713  data: 0.0000  max mem: 18589
Evaluation:  [2850/3079]  eta: 0:01:24    time: 0.3701  data: 0.0000  max mem: 18589
Evaluation:  [2900/3079]  eta: 0:01:06    time: 0.3657  data: 0.0000  max mem: 18589
Evaluation:  [2950/3079]  eta: 0:00:47    time: 0.3728  data: 0.0000  max mem: 18589
Evaluation:  [3000/3079]  eta: 0:00:29    time: 0.3758  data: 0.0000  max mem: 18589
Evaluation:  [3050/3079]  eta: 0:00:10    time: 0.3659  data: 0.0000  max mem: 18589
Evaluation:  [3078/3079]  eta: 0:00:00    time: 0.3695  data: 0.0000  max mem: 18589
Evaluation: Total time: 0:18:59 (0.3702 s / it)
Evaluation time 0:20:41
Test: {'r1': 75.32488250732422, 'r5': 89.48992919921875, 'r10': 93.06367492675781, 'r_mean': 85.9594955444336, 'mAP': 67.6772232055664} 

Train Epoch的聚类: [27]  [   0/5241]  eta: 0:44:35    time: 0.5105  data: 0.4905  max mem: 18589
Train Epoch的聚类: [27]  [5240/5241]  eta: 0:00:00    time: 0.1720  data: 0.0002  max mem: 18589
Train Epoch的聚类: [27] Total time: 0:14:59 (0.1716 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4131
-1 (未归入任何簇) 的数量: 1503

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [27]  [   0/2562]  eta: 20:36:19  lr: 0.000001  loss_cl: 10.0722  loss_pitm: 0.0115  loss_mlm: 0.3104  loss_prd: 0.2547  loss_mrtd: 0.1771  time: 28.9536  data: 0.5236  max mem: 18589
Train Epoch: [27]  [  50/2562]  eta: 1:15:43  lr: 0.000001  loss_cl: 10.1011  loss_pitm: 0.2822  loss_mlm: 0.4824  loss_prd: 0.2704  loss_mrtd: 0.2796  time: 1.2638  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 100/2562]  eta: 1:03:37  lr: 0.000001  loss_cl: 9.5938  loss_pitm: 0.0356  loss_mlm: 0.2857  loss_prd: 0.4314  loss_mrtd: 0.2194  time: 1.2917  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 150/2562]  eta: 0:59:00  lr: 0.000001  loss_cl: 9.8998  loss_pitm: 0.2793  loss_mlm: 0.4691  loss_prd: 0.0509  loss_mrtd: 0.2408  time: 1.2953  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 200/2562]  eta: 0:56:08  lr: 0.000001  loss_cl: 9.1586  loss_pitm: 0.0223  loss_mlm: 0.5931  loss_prd: 0.1122  loss_mrtd: 0.2626  time: 1.2923  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 250/2562]  eta: 0:54:02  lr: 0.000001  loss_cl: 9.5532  loss_pitm: 0.0061  loss_mlm: 0.4320  loss_prd: 0.2670  loss_mrtd: 0.1737  time: 1.3021  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 300/2562]  eta: 0:52:12  lr: 0.000001  loss_cl: 9.5485  loss_pitm: 0.0146  loss_mlm: 0.2231  loss_prd: 0.2712  loss_mrtd: 0.1268  time: 1.2966  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 350/2562]  eta: 0:50:36  lr: 0.000001  loss_cl: 8.8665  loss_pitm: 0.0311  loss_mlm: 0.5623  loss_prd: 0.1064  loss_mrtd: 0.2112  time: 1.2919  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 400/2562]  eta: 0:49:08  lr: 0.000001  loss_cl: 9.0540  loss_pitm: 0.0294  loss_mlm: 0.3936  loss_prd: 0.1389  loss_mrtd: 0.1691  time: 1.2989  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 450/2562]  eta: 0:47:45  lr: 0.000001  loss_cl: 9.4191  loss_pitm: 0.0212  loss_mlm: 0.6740  loss_prd: 0.5300  loss_mrtd: 0.2029  time: 1.2919  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 500/2562]  eta: 0:46:26  lr: 0.000001  loss_cl: 8.9238  loss_pitm: 0.0462  loss_mlm: 0.3077  loss_prd: 0.0646  loss_mrtd: 0.2213  time: 1.3024  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 550/2562]  eta: 0:45:11  lr: 0.000001  loss_cl: 9.3638  loss_pitm: 0.0082  loss_mlm: 0.3042  loss_prd: 0.4411  loss_mrtd: 0.2006  time: 1.3084  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 600/2562]  eta: 0:43:56  lr: 0.000001  loss_cl: 9.0465  loss_pitm: 0.0556  loss_mlm: 0.2202  loss_prd: 0.0443  loss_mrtd: 0.2325  time: 1.2906  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 650/2562]  eta: 0:42:44  lr: 0.000001  loss_cl: 9.2358  loss_pitm: 0.0126  loss_mlm: 0.4422  loss_prd: 0.5445  loss_mrtd: 0.2860  time: 1.3160  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 700/2562]  eta: 0:41:30  lr: 0.000001  loss_cl: 9.0728  loss_pitm: 0.0158  loss_mlm: 0.4882  loss_prd: 0.5309  loss_mrtd: 0.2228  time: 1.2979  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 750/2562]  eta: 0:40:20  lr: 0.000001  loss_cl: 8.3003  loss_pitm: 0.0021  loss_mlm: 0.5630  loss_prd: 0.2777  loss_mrtd: 0.2859  time: 1.3059  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 800/2562]  eta: 0:39:11  lr: 0.000001  loss_cl: 9.2346  loss_pitm: 0.1302  loss_mlm: 0.3538  loss_prd: 0.0738  loss_mrtd: 0.2077  time: 1.3144  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 850/2562]  eta: 0:38:01  lr: 0.000001  loss_cl: 8.6769  loss_pitm: 0.1553  loss_mlm: 0.3679  loss_prd: 0.0296  loss_mrtd: 0.2557  time: 1.2990  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 900/2562]  eta: 0:36:52  lr: 0.000001  loss_cl: 9.0236  loss_pitm: 0.1260  loss_mlm: 0.4351  loss_prd: 0.2814  loss_mrtd: 0.2238  time: 1.3018  data: 0.0001  max mem: 18589
Train Epoch: [27]  [ 950/2562]  eta: 0:35:43  lr: 0.000001  loss_cl: 8.8796  loss_pitm: 0.0326  loss_mlm: 0.2262  loss_prd: 0.1276  loss_mrtd: 0.2139  time: 1.2925  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1000/2562]  eta: 0:34:35  lr: 0.000001  loss_cl: 8.9414  loss_pitm: 0.0045  loss_mlm: 0.4179  loss_prd: 0.0504  loss_mrtd: 0.2094  time: 1.3075  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1050/2562]  eta: 0:33:27  lr: 0.000001  loss_cl: 8.1974  loss_pitm: 0.1736  loss_mlm: 0.4539  loss_prd: 0.2674  loss_mrtd: 0.2204  time: 1.2993  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1100/2562]  eta: 0:32:20  lr: 0.000001  loss_cl: 8.8272  loss_pitm: 0.0558  loss_mlm: 0.3194  loss_prd: 0.0393  loss_mrtd: 0.1924  time: 1.3108  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1150/2562]  eta: 0:31:12  lr: 0.000001  loss_cl: 8.3114  loss_pitm: 0.0099  loss_mlm: 0.2506  loss_prd: 0.1577  loss_mrtd: 0.1689  time: 1.3035  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1200/2562]  eta: 0:30:04  lr: 0.000001  loss_cl: 9.2064  loss_pitm: 0.0899  loss_mlm: 0.3592  loss_prd: 0.0531  loss_mrtd: 0.2156  time: 1.3143  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1250/2562]  eta: 0:28:57  lr: 0.000001  loss_cl: 8.6990  loss_pitm: 0.0958  loss_mlm: 0.2455  loss_prd: 0.0519  loss_mrtd: 0.2188  time: 1.3048  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1300/2562]  eta: 0:27:50  lr: 0.000001  loss_cl: 8.7833  loss_pitm: 0.0078  loss_mlm: 0.4064  loss_prd: 0.2691  loss_mrtd: 0.2198  time: 1.3076  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1350/2562]  eta: 0:26:43  lr: 0.000001  loss_cl: 8.8478  loss_pitm: 0.0144  loss_mlm: 0.9013  loss_prd: 0.0392  loss_mrtd: 0.1955  time: 1.3065  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1400/2562]  eta: 0:25:36  lr: 0.000001  loss_cl: 8.4223  loss_pitm: 0.0031  loss_mlm: 0.2182  loss_prd: 0.5166  loss_mrtd: 0.2182  time: 1.3203  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1450/2562]  eta: 0:24:30  lr: 0.000001  loss_cl: 8.5312  loss_pitm: 0.0352  loss_mlm: 0.2438  loss_prd: 0.2838  loss_mrtd: 0.1899  time: 1.3203  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1500/2562]  eta: 0:23:23  lr: 0.000001  loss_cl: 8.1551  loss_pitm: 0.0030  loss_mlm: 0.3895  loss_prd: 0.0483  loss_mrtd: 0.1584  time: 1.3018  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1550/2562]  eta: 0:22:16  lr: 0.000001  loss_cl: 8.2894  loss_pitm: 0.0168  loss_mlm: 0.3649  loss_prd: 0.0929  loss_mrtd: 0.2291  time: 1.3087  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1600/2562]  eta: 0:21:10  lr: 0.000001  loss_cl: 7.9987  loss_pitm: 0.0391  loss_mlm: 0.3132  loss_prd: 0.0434  loss_mrtd: 0.2176  time: 1.3050  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1650/2562]  eta: 0:20:03  lr: 0.000001  loss_cl: 8.5545  loss_pitm: 0.1389  loss_mlm: 0.6199  loss_prd: 0.2425  loss_mrtd: 0.2223  time: 1.2967  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1700/2562]  eta: 0:18:56  lr: 0.000001  loss_cl: 7.8694  loss_pitm: 0.0104  loss_mlm: 0.3666  loss_prd: 0.0497  loss_mrtd: 0.1795  time: 1.3031  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1750/2562]  eta: 0:17:50  lr: 0.000001  loss_cl: 8.1544  loss_pitm: 0.2020  loss_mlm: 0.4615  loss_prd: 0.2990  loss_mrtd: 0.2748  time: 1.3023  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1800/2562]  eta: 0:16:44  lr: 0.000001  loss_cl: 7.9693  loss_pitm: 0.1238  loss_mlm: 0.5183  loss_prd: 0.0454  loss_mrtd: 0.1812  time: 1.3003  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1850/2562]  eta: 0:15:38  lr: 0.000001  loss_cl: 8.2187  loss_pitm: 0.0176  loss_mlm: 0.3256  loss_prd: 0.0598  loss_mrtd: 0.3025  time: 1.3142  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1900/2562]  eta: 0:14:32  lr: 0.000001  loss_cl: 7.8838  loss_pitm: 0.0168  loss_mlm: 0.4622  loss_prd: 0.0671  loss_mrtd: 0.2617  time: 1.3166  data: 0.0001  max mem: 18589
Train Epoch: [27]  [1950/2562]  eta: 0:13:26  lr: 0.000001  loss_cl: 8.3509  loss_pitm: 0.0058  loss_mlm: 0.3052  loss_prd: 0.2711  loss_mrtd: 0.2230  time: 1.3000  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2000/2562]  eta: 0:12:20  lr: 0.000001  loss_cl: 8.3223  loss_pitm: 0.0072  loss_mlm: 0.3923  loss_prd: 0.0464  loss_mrtd: 0.2477  time: 1.3126  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2050/2562]  eta: 0:11:14  lr: 0.000001  loss_cl: 7.3992  loss_pitm: 0.0074  loss_mlm: 0.5024  loss_prd: 0.0458  loss_mrtd: 0.2162  time: 1.3064  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2100/2562]  eta: 0:10:08  lr: 0.000001  loss_cl: 7.6447  loss_pitm: 0.1965  loss_mlm: 0.3125  loss_prd: 0.0405  loss_mrtd: 0.1554  time: 1.3021  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2150/2562]  eta: 0:09:02  lr: 0.000001  loss_cl: 7.8983  loss_pitm: 0.0349  loss_mlm: 0.7078  loss_prd: 0.2789  loss_mrtd: 0.2541  time: 1.3173  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2200/2562]  eta: 0:07:56  lr: 0.000001  loss_cl: 7.5597  loss_pitm: 0.0067  loss_mlm: 0.4649  loss_prd: 0.0498  loss_mrtd: 0.1961  time: 1.3170  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2250/2562]  eta: 0:06:50  lr: 0.000001  loss_cl: 7.7555  loss_pitm: 0.0175  loss_mlm: 0.5992  loss_prd: 0.0539  loss_mrtd: 0.2915  time: 1.2940  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2300/2562]  eta: 0:05:44  lr: 0.000001  loss_cl: 7.4710  loss_pitm: 0.0855  loss_mlm: 0.4330  loss_prd: 0.0549  loss_mrtd: 0.2289  time: 1.3069  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2350/2562]  eta: 0:04:38  lr: 0.000001  loss_cl: 7.8908  loss_pitm: 0.0801  loss_mlm: 0.5970  loss_prd: 0.2845  loss_mrtd: 0.2225  time: 1.3175  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2400/2562]  eta: 0:03:33  lr: 0.000001  loss_cl: 7.2487  loss_pitm: 0.0032  loss_mlm: 0.3946  loss_prd: 0.2690  loss_mrtd: 0.2347  time: 1.2922  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2450/2562]  eta: 0:02:27  lr: 0.000001  loss_cl: 7.5091  loss_pitm: 0.0198  loss_mlm: 0.5125  loss_prd: 0.3032  loss_mrtd: 0.1942  time: 1.2962  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2500/2562]  eta: 0:01:21  lr: 0.000001  loss_cl: 7.2778  loss_pitm: 0.0164  loss_mlm: 0.3232  loss_prd: 0.0501  loss_mrtd: 0.1573  time: 1.3033  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2550/2562]  eta: 0:00:15  lr: 0.000001  loss_cl: 7.1091  loss_pitm: 0.0022  loss_mlm: 0.5427  loss_prd: 0.0400  loss_mrtd: 0.2087  time: 1.3091  data: 0.0001  max mem: 18589
Train Epoch: [27]  [2561/2562]  eta: 0:00:01  lr: 0.000001  loss_cl: 7.0101  loss_pitm: 0.0055  loss_mlm: 0.4318  loss_prd: 0.3070  loss_mrtd: 0.2541  time: 1.2938  data: 0.0001  max mem: 18589
Train Epoch: [27] Total time: 0:56:08 (1.3148 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5572  loss_pitm: 0.0430  loss_mlm: 0.4126  loss_prd: 0.1919  loss_mrtd: 0.2141
Computing features for evaluation...
Evaluation:  [   0/3079]  eta: 0:19:25    time: 0.3786  data: 0.0072  max mem: 18589
Evaluation:  [  50/3079]  eta: 0:18:11    time: 0.3604  data: 0.0000  max mem: 18589
Evaluation:  [ 100/3079]  eta: 0:17:58    time: 0.3657  data: 0.0000  max mem: 18589
Evaluation:  [ 150/3079]  eta: 0:17:48    time: 0.3675  data: 0.0000  max mem: 18589
Evaluation:  [ 200/3079]  eta: 0:17:30    time: 0.3633  data: 0.0000  max mem: 18589
Evaluation:  [ 250/3079]  eta: 0:17:09    time: 0.3607  data: 0.0000  max mem: 18589
Evaluation:  [ 300/3079]  eta: 0:16:51    time: 0.3670  data: 0.0000  max mem: 18589
Evaluation:  [ 350/3079]  eta: 0:16:34    time: 0.3687  data: 0.0000  max mem: 18589
Evaluation:  [ 400/3079]  eta: 0:16:16    time: 0.3651  data: 0.0000  max mem: 18589
Evaluation:  [ 450/3079]  eta: 0:15:58    time: 0.3669  data: 0.0000  max mem: 18589
Evaluation:  [ 500/3079]  eta: 0:15:40    time: 0.3689  data: 0.0000  max mem: 18589
Evaluation:  [ 550/3079]  eta: 0:15:22    time: 0.3652  data: 0.0000  max mem: 18589
Evaluation:  [ 600/3079]  eta: 0:15:04    time: 0.3664  data: 0.0000  max mem: 18589
Evaluation:  [ 650/3079]  eta: 0:14:46    time: 0.3698  data: 0.0000  max mem: 18589
Evaluation:  [ 700/3079]  eta: 0:14:28    time: 0.3638  data: 0.0000  max mem: 18589
Evaluation:  [ 750/3079]  eta: 0:14:09    time: 0.3630  data: 0.0000  max mem: 18589
Evaluation:  [ 800/3079]  eta: 0:13:51    time: 0.3675  data: 0.0000  max mem: 18589
Evaluation:  [ 850/3079]  eta: 0:13:34    time: 0.3694  data: 0.0000  max mem: 18589
Evaluation:  [ 900/3079]  eta: 0:13:15    time: 0.3634  data: 0.0000  max mem: 18589
Evaluation:  [ 950/3079]  eta: 0:12:57    time: 0.3675  data: 0.0000  max mem: 18589
Evaluation:  [1000/3079]  eta: 0:12:39    time: 0.3625  data: 0.0000  max mem: 18589
Evaluation:  [1050/3079]  eta: 0:12:20    time: 0.3601  data: 0.0000  max mem: 18589
Evaluation:  [1100/3079]  eta: 0:12:02    time: 0.3674  data: 0.0000  max mem: 18589
Evaluation:  [1150/3079]  eta: 0:11:44    time: 0.3686  data: 0.0000  max mem: 18589
Evaluation:  [1200/3079]  eta: 0:11:26    time: 0.3618  data: 0.0000  max mem: 18589
Evaluation:  [1250/3079]  eta: 0:11:07    time: 0.3632  data: 0.0000  max mem: 18589
Evaluation:  [1300/3079]  eta: 0:10:49    time: 0.3678  data: 0.0000  max mem: 18589
Evaluation:  [1350/3079]  eta: 0:10:31    time: 0.3633  data: 0.0000  max mem: 18589
Evaluation:  [1400/3079]  eta: 0:10:13    time: 0.3660  data: 0.0000  max mem: 18589
Evaluation:  [1450/3079]  eta: 0:09:55    time: 0.3685  data: 0.0000  max mem: 18589
Evaluation:  [1500/3079]  eta: 0:09:36    time: 0.3647  data: 0.0000  max mem: 18589
Evaluation:  [1550/3079]  eta: 0:09:18    time: 0.3637  data: 0.0000  max mem: 18589
Evaluation:  [1600/3079]  eta: 0:09:00    time: 0.3673  data: 0.0000  max mem: 18589
Evaluation:  [1650/3079]  eta: 0:08:42    time: 0.3664  data: 0.0000  max mem: 18589
Evaluation:  [1700/3079]  eta: 0:08:23    time: 0.3657  data: 0.0000  max mem: 18589
Evaluation:  [1750/3079]  eta: 0:08:05    time: 0.3683  data: 0.0000  max mem: 18589
Evaluation:  [1800/3079]  eta: 0:07:47    time: 0.3667  data: 0.0000  max mem: 18589
Evaluation:  [1850/3079]  eta: 0:07:29    time: 0.3635  data: 0.0000  max mem: 18589
Evaluation:  [1900/3079]  eta: 0:07:10    time: 0.3669  data: 0.0000  max mem: 18589
Evaluation:  [1950/3079]  eta: 0:06:52    time: 0.3654  data: 0.0000  max mem: 18589
Evaluation:  [2000/3079]  eta: 0:06:34    time: 0.3629  data: 0.0000  max mem: 18589
Evaluation:  [2050/3079]  eta: 0:06:15    time: 0.3674  data: 0.0000  max mem: 18589
Evaluation:  [2100/3079]  eta: 0:05:57    time: 0.3680  data: 0.0000  max mem: 18589
Evaluation:  [2150/3079]  eta: 0:05:39    time: 0.3638  data: 0.0000  max mem: 18589
Evaluation:  [2200/3079]  eta: 0:05:21    time: 0.3664  data: 0.0000  max mem: 18589
Evaluation:  [2250/3079]  eta: 0:05:02    time: 0.3699  data: 0.0000  max mem: 18589
Evaluation:  [2300/3079]  eta: 0:04:44    time: 0.3626  data: 0.0000  max mem: 18589
Evaluation:  [2350/3079]  eta: 0:04:26    time: 0.3669  data: 0.0000  max mem: 18589
Evaluation:  [2400/3079]  eta: 0:04:08    time: 0.3679  data: 0.0000  max mem: 18589
Evaluation:  [2450/3079]  eta: 0:03:49    time: 0.3648  data: 0.0000  max mem: 18589
Evaluation:  [2500/3079]  eta: 0:03:31    time: 0.3667  data: 0.0000  max mem: 18589
Evaluation:  [2550/3079]  eta: 0:03:13    time: 0.3688  data: 0.0000  max mem: 18589
Evaluation:  [2600/3079]  eta: 0:02:55    time: 0.3662  data: 0.0000  max mem: 18589
Evaluation:  [2650/3079]  eta: 0:02:36    time: 0.3641  data: 0.0000  max mem: 18589
Evaluation:  [2700/3079]  eta: 0:02:18    time: 0.3682  data: 0.0000  max mem: 18589
Evaluation:  [2750/3079]  eta: 0:02:00    time: 0.3654  data: 0.0000  max mem: 18589
Evaluation:  [2800/3079]  eta: 0:01:41    time: 0.3651  data: 0.0000  max mem: 18589
Evaluation:  [2850/3079]  eta: 0:01:23    time: 0.3670  data: 0.0000  max mem: 18589
Evaluation:  [2900/3079]  eta: 0:01:05    time: 0.3661  data: 0.0000  max mem: 18589
Evaluation:  [2950/3079]  eta: 0:00:47    time: 0.3631  data: 0.0000  max mem: 18589
Evaluation:  [3000/3079]  eta: 0:00:28    time: 0.3708  data: 0.0000  max mem: 18589
Evaluation:  [3050/3079]  eta: 0:00:10    time: 0.3654  data: 0.0000  max mem: 18589
Evaluation:  [3078/3079]  eta: 0:00:00    time: 0.3632  data: 0.0000  max mem: 18589
Evaluation: Total time: 0:18:45 (0.3656 s / it)
Evaluation time 0:20:40
Test: {'r1': 75.60103607177734, 'r5': 89.21377563476562, 'r10': 93.1773910522461, 'r_mean': 85.99740091959636, 'mAP': 67.68988037109375} 

Train Epoch的聚类: [28]  [   0/5241]  eta: 0:50:20    time: 0.5764  data: 0.5526  max mem: 18589
Train Epoch的聚类: [28]  [5240/5241]  eta: 0:00:00    time: 0.1715  data: 0.0002  max mem: 18589
Train Epoch的聚类: [28] Total time: 0:14:57 (0.1713 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4098
-1 (未归入任何簇) 的数量: 1574

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [28]  [   0/2559]  eta: 1 day, 2:48:39  lr: 0.000001  loss_cl: 10.4580  loss_pitm: 0.0055  loss_mlm: 0.2688  loss_prd: 0.2615  loss_mrtd: 0.2312  time: 37.7176  data: 0.5497  max mem: 18589
Train Epoch: [28]  [  50/2559]  eta: 1:22:42  lr: 0.000001  loss_cl: 10.1507  loss_pitm: 0.0087  loss_mlm: 0.3450  loss_prd: 0.0494  loss_mrtd: 0.2351  time: 1.2747  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 100/2559]  eta: 1:07:00  lr: 0.000001  loss_cl: 9.3801  loss_pitm: 0.0232  loss_mlm: 0.4060  loss_prd: 0.2664  loss_mrtd: 0.1883  time: 1.2925  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 150/2559]  eta: 1:01:17  lr: 0.000001  loss_cl: 9.5986  loss_pitm: 0.0054  loss_mlm: 0.1585  loss_prd: 0.1979  loss_mrtd: 0.1384  time: 1.3163  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 200/2559]  eta: 0:57:50  lr: 0.000001  loss_cl: 9.5450  loss_pitm: 0.0215  loss_mlm: 0.3756  loss_prd: 0.0681  loss_mrtd: 0.2359  time: 1.3046  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 250/2559]  eta: 0:55:19  lr: 0.000001  loss_cl: 9.0368  loss_pitm: 0.0220  loss_mlm: 0.2728  loss_prd: 0.0389  loss_mrtd: 0.2701  time: 1.3057  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 300/2559]  eta: 0:53:15  lr: 0.000001  loss_cl: 9.2502  loss_pitm: 0.0027  loss_mlm: 0.2876  loss_prd: 0.0467  loss_mrtd: 0.2580  time: 1.3013  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 350/2559]  eta: 0:51:30  lr: 0.000001  loss_cl: 9.9348  loss_pitm: 0.0389  loss_mlm: 0.4179  loss_prd: 0.2865  loss_mrtd: 0.2291  time: 1.3108  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 400/2559]  eta: 0:49:58  lr: 0.000001  loss_cl: 9.8425  loss_pitm: 0.0034  loss_mlm: 0.2358  loss_prd: 0.0507  loss_mrtd: 0.2165  time: 1.3205  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 450/2559]  eta: 0:48:29  lr: 0.000001  loss_cl: 9.6142  loss_pitm: 0.0047  loss_mlm: 0.6245  loss_prd: 0.0446  loss_mrtd: 0.2040  time: 1.2955  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 500/2559]  eta: 0:47:04  lr: 0.000001  loss_cl: 8.8578  loss_pitm: 0.0092  loss_mlm: 0.3528  loss_prd: 0.2530  loss_mrtd: 0.1765  time: 1.2965  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 550/2559]  eta: 0:45:41  lr: 0.000001  loss_cl: 9.7298  loss_pitm: 0.0302  loss_mlm: 0.3857  loss_prd: 0.0933  loss_mrtd: 0.2337  time: 1.2961  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 600/2559]  eta: 0:44:22  lr: 0.000001  loss_cl: 9.3064  loss_pitm: 0.0054  loss_mlm: 0.2851  loss_prd: 0.2948  loss_mrtd: 0.2024  time: 1.3058  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 650/2559]  eta: 0:43:06  lr: 0.000001  loss_cl: 9.2544  loss_pitm: 0.0085  loss_mlm: 0.3401  loss_prd: 0.0501  loss_mrtd: 0.2109  time: 1.2971  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 700/2559]  eta: 0:41:51  lr: 0.000001  loss_cl: 9.2828  loss_pitm: 0.0074  loss_mlm: 0.2025  loss_prd: 0.0387  loss_mrtd: 0.1654  time: 1.3018  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 750/2559]  eta: 0:40:37  lr: 0.000001  loss_cl: 9.4331  loss_pitm: 0.0058  loss_mlm: 0.3900  loss_prd: 0.0681  loss_mrtd: 0.1731  time: 1.2927  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 800/2559]  eta: 0:39:24  lr: 0.000001  loss_cl: 8.7095  loss_pitm: 0.0061  loss_mlm: 0.5029  loss_prd: 0.1537  loss_mrtd: 0.1955  time: 1.2913  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 850/2559]  eta: 0:38:13  lr: 0.000001  loss_cl: 8.8814  loss_pitm: 0.0270  loss_mlm: 0.5050  loss_prd: 0.3206  loss_mrtd: 0.2260  time: 1.2982  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 900/2559]  eta: 0:37:02  lr: 0.000001  loss_cl: 9.0443  loss_pitm: 0.0060  loss_mlm: 0.1617  loss_prd: 0.0402  loss_mrtd: 0.2069  time: 1.2958  data: 0.0001  max mem: 18589
Train Epoch: [28]  [ 950/2559]  eta: 0:35:52  lr: 0.000001  loss_cl: 8.7325  loss_pitm: 0.0053  loss_mlm: 0.5036  loss_prd: 0.1987  loss_mrtd: 0.2489  time: 1.3071  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1000/2559]  eta: 0:34:42  lr: 0.000001  loss_cl: 8.4618  loss_pitm: 0.0223  loss_mlm: 0.2048  loss_prd: 0.3005  loss_mrtd: 0.1750  time: 1.2966  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1050/2559]  eta: 0:33:32  lr: 0.000001  loss_cl: 8.9100  loss_pitm: 0.0018  loss_mlm: 0.4151  loss_prd: 0.0442  loss_mrtd: 0.1719  time: 1.2871  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1100/2559]  eta: 0:32:23  lr: 0.000001  loss_cl: 9.0851  loss_pitm: 0.3473  loss_mlm: 0.3001  loss_prd: 0.2751  loss_mrtd: 0.2167  time: 1.2943  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1150/2559]  eta: 0:31:14  lr: 0.000001  loss_cl: 8.5807  loss_pitm: 0.1239  loss_mlm: 0.3258  loss_prd: 0.0371  loss_mrtd: 0.1339  time: 1.3009  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1200/2559]  eta: 0:30:05  lr: 0.000001  loss_cl: 8.6765  loss_pitm: 0.0524  loss_mlm: 0.4438  loss_prd: 0.3109  loss_mrtd: 0.2582  time: 1.3039  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1250/2559]  eta: 0:28:58  lr: 0.000001  loss_cl: 8.9115  loss_pitm: 0.0794  loss_mlm: 0.3152  loss_prd: 0.0445  loss_mrtd: 0.1167  time: 1.3050  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1300/2559]  eta: 0:27:50  lr: 0.000001  loss_cl: 8.5563  loss_pitm: 0.0080  loss_mlm: 0.5590  loss_prd: 0.0413  loss_mrtd: 0.2356  time: 1.2972  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1350/2559]  eta: 0:26:43  lr: 0.000001  loss_cl: 8.3606  loss_pitm: 0.0105  loss_mlm: 0.2687  loss_prd: 0.3440  loss_mrtd: 0.1937  time: 1.2944  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1400/2559]  eta: 0:25:35  lr: 0.000001  loss_cl: 8.4859  loss_pitm: 0.0043  loss_mlm: 0.4415  loss_prd: 0.0520  loss_mrtd: 0.2665  time: 1.3083  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1450/2559]  eta: 0:24:28  lr: 0.000001  loss_cl: 8.6754  loss_pitm: 0.0014  loss_mlm: 0.2954  loss_prd: 0.0457  loss_mrtd: 0.3244  time: 1.3019  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1500/2559]  eta: 0:23:21  lr: 0.000001  loss_cl: 8.2237  loss_pitm: 0.0447  loss_mlm: 0.2863  loss_prd: 0.5260  loss_mrtd: 0.2356  time: 1.3117  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1550/2559]  eta: 0:22:14  lr: 0.000001  loss_cl: 7.9868  loss_pitm: 0.0697  loss_mlm: 0.3463  loss_prd: 0.0452  loss_mrtd: 0.2117  time: 1.3097  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1600/2559]  eta: 0:21:07  lr: 0.000001  loss_cl: 7.9884  loss_pitm: 0.2749  loss_mlm: 0.3475  loss_prd: 0.0692  loss_mrtd: 0.2097  time: 1.2952  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1650/2559]  eta: 0:20:01  lr: 0.000001  loss_cl: 8.2721  loss_pitm: 0.0796  loss_mlm: 0.3010  loss_prd: 0.3142  loss_mrtd: 0.1587  time: 1.2999  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1700/2559]  eta: 0:18:54  lr: 0.000001  loss_cl: 8.6989  loss_pitm: 0.0087  loss_mlm: 0.2946  loss_prd: 0.0498  loss_mrtd: 0.1783  time: 1.2963  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1750/2559]  eta: 0:17:47  lr: 0.000001  loss_cl: 8.2822  loss_pitm: 0.0065  loss_mlm: 0.2269  loss_prd: 0.0833  loss_mrtd: 0.2102  time: 1.2988  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1800/2559]  eta: 0:16:41  lr: 0.000001  loss_cl: 7.9123  loss_pitm: 0.0088  loss_mlm: 0.3677  loss_prd: 0.0427  loss_mrtd: 0.2203  time: 1.3022  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1850/2559]  eta: 0:15:35  lr: 0.000001  loss_cl: 7.8788  loss_pitm: 0.0047  loss_mlm: 0.5124  loss_prd: 0.2988  loss_mrtd: 0.2450  time: 1.2936  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1900/2559]  eta: 0:14:28  lr: 0.000001  loss_cl: 8.0133  loss_pitm: 0.0625  loss_mlm: 0.3740  loss_prd: 0.0459  loss_mrtd: 0.1846  time: 1.3005  data: 0.0001  max mem: 18589
Train Epoch: [28]  [1950/2559]  eta: 0:13:22  lr: 0.000001  loss_cl: 7.6607  loss_pitm: 0.0212  loss_mlm: 0.6019  loss_prd: 0.0468  loss_mrtd: 0.2400  time: 1.3128  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2000/2559]  eta: 0:12:16  lr: 0.000001  loss_cl: 7.5189  loss_pitm: 0.0034  loss_mlm: 0.2169  loss_prd: 0.4476  loss_mrtd: 0.1980  time: 1.3052  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2050/2559]  eta: 0:11:10  lr: 0.000001  loss_cl: 7.7086  loss_pitm: 0.0209  loss_mlm: 0.6710  loss_prd: 0.0533  loss_mrtd: 0.1990  time: 1.2960  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2100/2559]  eta: 0:10:04  lr: 0.000001  loss_cl: 7.8792  loss_pitm: 0.1201  loss_mlm: 0.2791  loss_prd: 0.0536  loss_mrtd: 0.1702  time: 1.2991  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2150/2559]  eta: 0:08:58  lr: 0.000001  loss_cl: 7.6790  loss_pitm: 0.0158  loss_mlm: 0.6649  loss_prd: 0.0496  loss_mrtd: 0.1687  time: 1.2988  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2200/2559]  eta: 0:07:52  lr: 0.000001  loss_cl: 7.2640  loss_pitm: 0.0176  loss_mlm: 0.4389  loss_prd: 0.1006  loss_mrtd: 0.2195  time: 1.3031  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2250/2559]  eta: 0:06:46  lr: 0.000001  loss_cl: 7.5246  loss_pitm: 0.0021  loss_mlm: 0.4655  loss_prd: 0.2944  loss_mrtd: 0.2102  time: 1.3008  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2300/2559]  eta: 0:05:40  lr: 0.000001  loss_cl: 7.1918  loss_pitm: 0.0519  loss_mlm: 0.3727  loss_prd: 0.0436  loss_mrtd: 0.2131  time: 1.3123  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2350/2559]  eta: 0:04:34  lr: 0.000001  loss_cl: 7.3145  loss_pitm: 0.0223  loss_mlm: 0.2612  loss_prd: 0.0609  loss_mrtd: 0.1827  time: 1.2974  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2400/2559]  eta: 0:03:29  lr: 0.000001  loss_cl: 7.3284  loss_pitm: 0.0185  loss_mlm: 0.5241  loss_prd: 0.0427  loss_mrtd: 0.2629  time: 1.2952  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2450/2559]  eta: 0:02:23  lr: 0.000001  loss_cl: 7.3735  loss_pitm: 0.0469  loss_mlm: 0.6454  loss_prd: 0.0715  loss_mrtd: 0.2396  time: 1.2993  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2500/2559]  eta: 0:01:17  lr: 0.000001  loss_cl: 7.2503  loss_pitm: 0.0597  loss_mlm: 0.3213  loss_prd: 0.2853  loss_mrtd: 0.2277  time: 1.3019  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2550/2559]  eta: 0:00:11  lr: 0.000001  loss_cl: 7.1374  loss_pitm: 0.0088  loss_mlm: 0.5463  loss_prd: 0.0462  loss_mrtd: 0.1956  time: 1.3001  data: 0.0001  max mem: 18589
Train Epoch: [28]  [2558/2559]  eta: 0:00:01  lr: 0.000001  loss_cl: 7.1805  loss_pitm: 0.0057  loss_mlm: 0.2242  loss_prd: 0.4686  loss_mrtd: 0.1692  time: 1.2990  data: 0.0002  max mem: 18589
Train Epoch: [28] Total time: 0:56:01 (1.3136 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5617  loss_pitm: 0.0407  loss_mlm: 0.4114  loss_prd: 0.1880  loss_mrtd: 0.2144
Computing features for evaluation...
Evaluation:  [   0/3079]  eta: 0:19:34    time: 0.3815  data: 0.0070  max mem: 18589
Evaluation:  [  50/3079]  eta: 0:18:31    time: 0.3656  data: 0.0000  max mem: 18589
Evaluation:  [ 100/3079]  eta: 0:18:15    time: 0.3700  data: 0.0000  max mem: 18589
Evaluation:  [ 150/3079]  eta: 0:17:59    time: 0.3704  data: 0.0000  max mem: 18589
Evaluation:  [ 200/3079]  eta: 0:17:40    time: 0.3683  data: 0.0000  max mem: 18589
Evaluation:  [ 250/3079]  eta: 0:17:20    time: 0.3652  data: 0.0000  max mem: 18589
Evaluation:  [ 300/3079]  eta: 0:17:00    time: 0.3648  data: 0.0000  max mem: 18589
Evaluation:  [ 350/3079]  eta: 0:16:42    time: 0.3688  data: 0.0000  max mem: 18589
Evaluation:  [ 400/3079]  eta: 0:16:26    time: 0.3758  data: 0.0000  max mem: 18589
Evaluation:  [ 450/3079]  eta: 0:16:07    time: 0.3661  data: 0.0000  max mem: 18589
Evaluation:  [ 500/3079]  eta: 0:15:49    time: 0.3686  data: 0.0000  max mem: 18589
Evaluation:  [ 550/3079]  eta: 0:15:32    time: 0.3768  data: 0.0000  max mem: 18589
Evaluation:  [ 600/3079]  eta: 0:15:13    time: 0.3681  data: 0.0000  max mem: 18589
Evaluation:  [ 650/3079]  eta: 0:14:55    time: 0.3729  data: 0.0000  max mem: 18589
Evaluation:  [ 700/3079]  eta: 0:14:37    time: 0.3716  data: 0.0000  max mem: 18589
Evaluation:  [ 750/3079]  eta: 0:14:18    time: 0.3655  data: 0.0000  max mem: 18589
Evaluation:  [ 800/3079]  eta: 0:14:00    time: 0.3696  data: 0.0000  max mem: 18589
Evaluation:  [ 850/3079]  eta: 0:13:42    time: 0.3773  data: 0.0000  max mem: 18589
Evaluation:  [ 900/3079]  eta: 0:13:24    time: 0.3676  data: 0.0000  max mem: 18589
Evaluation:  [ 950/3079]  eta: 0:13:05    time: 0.3708  data: 0.0000  max mem: 18589
Evaluation:  [1000/3079]  eta: 0:12:47    time: 0.3685  data: 0.0000  max mem: 18589
Evaluation:  [1050/3079]  eta: 0:12:29    time: 0.3656  data: 0.0000  max mem: 18589
Evaluation:  [1100/3079]  eta: 0:12:10    time: 0.3712  data: 0.0000  max mem: 18589
Evaluation:  [1150/3079]  eta: 0:11:52    time: 0.3722  data: 0.0000  max mem: 18589
Evaluation:  [1200/3079]  eta: 0:11:33    time: 0.3669  data: 0.0000  max mem: 18589
Evaluation:  [1250/3079]  eta: 0:11:15    time: 0.3670  data: 0.0000  max mem: 18589
Evaluation:  [1300/3079]  eta: 0:10:56    time: 0.3719  data: 0.0000  max mem: 18589
Evaluation:  [1350/3079]  eta: 0:10:38    time: 0.3674  data: 0.0000  max mem: 18589
Evaluation:  [1400/3079]  eta: 0:10:20    time: 0.3706  data: 0.0000  max mem: 18589
Evaluation:  [1450/3079]  eta: 0:10:01    time: 0.3740  data: 0.0000  max mem: 18589
Evaluation:  [1500/3079]  eta: 0:09:43    time: 0.3685  data: 0.0000  max mem: 18589
Evaluation:  [1550/3079]  eta: 0:09:24    time: 0.3654  data: 0.0000  max mem: 18589
Evaluation:  [1600/3079]  eta: 0:09:06    time: 0.3708  data: 0.0000  max mem: 18589
Evaluation:  [1650/3079]  eta: 0:08:48    time: 0.3728  data: 0.0000  max mem: 18589
Evaluation:  [1700/3079]  eta: 0:08:29    time: 0.3668  data: 0.0000  max mem: 18589
Evaluation:  [1750/3079]  eta: 0:08:10    time: 0.3702  data: 0.0000  max mem: 18589
Evaluation:  [1800/3079]  eta: 0:07:52    time: 0.3732  data: 0.0000  max mem: 18589
Evaluation:  [1850/3079]  eta: 0:07:34    time: 0.3680  data: 0.0000  max mem: 18589
Evaluation:  [1900/3079]  eta: 0:07:15    time: 0.3697  data: 0.0000  max mem: 18589
Evaluation:  [1950/3079]  eta: 0:06:57    time: 0.3713  data: 0.0000  max mem: 18589
Evaluation:  [2000/3079]  eta: 0:06:38    time: 0.3684  data: 0.0000  max mem: 18589
Evaluation:  [2050/3079]  eta: 0:06:20    time: 0.3687  data: 0.0000  max mem: 18589
Evaluation:  [2100/3079]  eta: 0:06:01    time: 0.3706  data: 0.0000  max mem: 18589
Evaluation:  [2150/3079]  eta: 0:05:43    time: 0.3701  data: 0.0000  max mem: 18589
Evaluation:  [2200/3079]  eta: 0:05:24    time: 0.3689  data: 0.0000  max mem: 18589
Evaluation:  [2250/3079]  eta: 0:05:06    time: 0.3721  data: 0.0000  max mem: 18589
Evaluation:  [2300/3079]  eta: 0:04:47    time: 0.3675  data: 0.0000  max mem: 18589
Evaluation:  [2350/3079]  eta: 0:04:29    time: 0.3677  data: 0.0000  max mem: 18589
Evaluation:  [2400/3079]  eta: 0:04:10    time: 0.3719  data: 0.0000  max mem: 18589
Evaluation:  [2450/3079]  eta: 0:03:52    time: 0.3704  data: 0.0000  max mem: 18589
Evaluation:  [2500/3079]  eta: 0:03:33    time: 0.3659  data: 0.0000  max mem: 18589
Evaluation:  [2550/3079]  eta: 0:03:15    time: 0.3703  data: 0.0000  max mem: 18589
Evaluation:  [2600/3079]  eta: 0:02:57    time: 0.3750  data: 0.0000  max mem: 18589
Evaluation:  [2650/3079]  eta: 0:02:38    time: 0.3664  data: 0.0000  max mem: 18589
Evaluation:  [2700/3079]  eta: 0:02:20    time: 0.3710  data: 0.0000  max mem: 18589
Evaluation:  [2750/3079]  eta: 0:02:01    time: 0.3736  data: 0.0000  max mem: 18589
Evaluation:  [2800/3079]  eta: 0:01:43    time: 0.3658  data: 0.0000  max mem: 18589
Evaluation:  [2850/3079]  eta: 0:01:24    time: 0.3711  data: 0.0000  max mem: 18589
Evaluation:  [2900/3079]  eta: 0:01:06    time: 0.3739  data: 0.0000  max mem: 18589
Evaluation:  [2950/3079]  eta: 0:00:47    time: 0.3672  data: 0.0000  max mem: 18589
Evaluation:  [3000/3079]  eta: 0:00:29    time: 0.3717  data: 0.0000  max mem: 18589
Evaluation:  [3050/3079]  eta: 0:00:10    time: 0.3704  data: 0.0000  max mem: 18589
Evaluation:  [3078/3079]  eta: 0:00:00    time: 0.3674  data: 0.0000  max mem: 18589
Evaluation: Total time: 0:18:58 (0.3697 s / it)
Evaluation time 0:20:41
Test: {'r1': 75.53606414794922, 'r5': 89.40870666503906, 'r10': 93.1448974609375, 'r_mean': 86.02988942464192, 'mAP': 67.77669525146484} 

Train Epoch的聚类: [29]  [   0/5241]  eta: 0:50:33    time: 0.5787  data: 0.5540  max mem: 18589
Train Epoch的聚类: [29]  [5240/5241]  eta: 0:00:00    time: 0.1717  data: 0.0002  max mem: 18589
Train Epoch的聚类: [29] Total time: 0:14:58 (0.1715 s / it)
Rank 0 | 开始计算不同类之间的距离 68126
bruteForceKnn is deprecated; call bfKnn instead
bruteForceKnn is deprecated; call bfKnn instead

[Rank 0 | 聚类开始]

[Rank 0 | 聚类完成]
Dataset 总长度: 68126
聚类数（不含 -1）: 4124
-1 (未归入任何簇) 的数量: 1599

成功将伪标签写入数据集中
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Retrieval.py:219: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  idx = torch.tensor(batch_pseudo_id).to(device, non_blocking=True)
Train Epoch: [29]  [   0/2558]  eta: 20:12:43  lr: 0.000001  loss_cl: 9.5948  loss_pitm: 0.1036  loss_mlm: 0.7241  loss_prd: 0.2060  loss_mrtd: 0.2051  time: 28.4456  data: 0.5546  max mem: 18589
Train Epoch: [29]  [  50/2558]  eta: 1:15:12  lr: 0.000001  loss_cl: 9.6577  loss_pitm: 0.0068  loss_mlm: 0.3606  loss_prd: 0.2789  loss_mrtd: 0.2765  time: 1.2844  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 100/2558]  eta: 1:03:17  lr: 0.000001  loss_cl: 9.9886  loss_pitm: 0.0016  loss_mlm: 0.5767  loss_prd: 0.4893  loss_mrtd: 0.1905  time: 1.2861  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 150/2558]  eta: 0:58:45  lr: 0.000001  loss_cl: 9.7894  loss_pitm: 0.2000  loss_mlm: 0.3633  loss_prd: 0.2882  loss_mrtd: 0.2531  time: 1.3069  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 200/2558]  eta: 0:55:51  lr: 0.000001  loss_cl: 9.7950  loss_pitm: 0.0091  loss_mlm: 0.2689  loss_prd: 0.0457  loss_mrtd: 0.2506  time: 1.2956  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 250/2558]  eta: 0:53:43  lr: 0.000001  loss_cl: 9.4654  loss_pitm: 0.0172  loss_mlm: 0.5082  loss_prd: 0.2802  loss_mrtd: 0.2648  time: 1.3006  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 300/2558]  eta: 0:51:57  lr: 0.000001  loss_cl: 9.1146  loss_pitm: 0.0037  loss_mlm: 0.5035  loss_prd: 0.1516  loss_mrtd: 0.1271  time: 1.3103  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 350/2558]  eta: 0:50:23  lr: 0.000001  loss_cl: 9.4888  loss_pitm: 0.0086  loss_mlm: 0.3817  loss_prd: 0.0464  loss_mrtd: 0.2809  time: 1.2969  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 400/2558]  eta: 0:48:56  lr: 0.000001  loss_cl: 9.0956  loss_pitm: 0.0106  loss_mlm: 0.5290  loss_prd: 0.3946  loss_mrtd: 0.2562  time: 1.2984  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 450/2558]  eta: 0:47:35  lr: 0.000001  loss_cl: 8.8740  loss_pitm: 0.0065  loss_mlm: 0.3349  loss_prd: 0.0393  loss_mrtd: 0.2055  time: 1.3011  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 500/2558]  eta: 0:46:16  lr: 0.000001  loss_cl: 9.7533  loss_pitm: 0.0039  loss_mlm: 0.2252  loss_prd: 0.2667  loss_mrtd: 0.2365  time: 1.3016  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 550/2558]  eta: 0:45:00  lr: 0.000001  loss_cl: 9.4696  loss_pitm: 0.0759  loss_mlm: 0.2802  loss_prd: 0.2466  loss_mrtd: 0.1698  time: 1.3075  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 600/2558]  eta: 0:43:46  lr: 0.000001  loss_cl: 9.2014  loss_pitm: 0.0175  loss_mlm: 0.2377  loss_prd: 0.0504  loss_mrtd: 0.1327  time: 1.3076  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 650/2558]  eta: 0:42:34  lr: 0.000001  loss_cl: 8.9293  loss_pitm: 0.0060  loss_mlm: 0.5502  loss_prd: 0.0448  loss_mrtd: 0.2774  time: 1.3160  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 700/2558]  eta: 0:41:23  lr: 0.000001  loss_cl: 8.5543  loss_pitm: 0.0114  loss_mlm: 0.2653  loss_prd: 0.2802  loss_mrtd: 0.2170  time: 1.3038  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 750/2558]  eta: 0:40:12  lr: 0.000001  loss_cl: 9.6200  loss_pitm: 0.0076  loss_mlm: 0.5900  loss_prd: 0.0516  loss_mrtd: 0.1640  time: 1.3028  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 800/2558]  eta: 0:39:02  lr: 0.000001  loss_cl: 8.9126  loss_pitm: 0.0020  loss_mlm: 0.7579  loss_prd: 0.0408  loss_mrtd: 0.2315  time: 1.2958  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 850/2558]  eta: 0:37:52  lr: 0.000001  loss_cl: 8.4244  loss_pitm: 0.0140  loss_mlm: 0.4543  loss_prd: 0.6710  loss_mrtd: 0.1977  time: 1.3002  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 900/2558]  eta: 0:36:42  lr: 0.000001  loss_cl: 9.1560  loss_pitm: 0.0042  loss_mlm: 0.5514  loss_prd: 0.3151  loss_mrtd: 0.2662  time: 1.3049  data: 0.0001  max mem: 18589
Train Epoch: [29]  [ 950/2558]  eta: 0:35:34  lr: 0.000001  loss_cl: 8.3853  loss_pitm: 0.0102  loss_mlm: 0.3130  loss_prd: 0.5817  loss_mrtd: 0.1968  time: 1.3111  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1000/2558]  eta: 0:34:26  lr: 0.000001  loss_cl: 8.4579  loss_pitm: 0.0042  loss_mlm: 0.1485  loss_prd: 0.3198  loss_mrtd: 0.1476  time: 1.3015  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1050/2558]  eta: 0:33:17  lr: 0.000001  loss_cl: 8.6695  loss_pitm: 0.0464  loss_mlm: 0.3656  loss_prd: 0.9111  loss_mrtd: 0.2584  time: 1.2948  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1100/2558]  eta: 0:32:10  lr: 0.000001  loss_cl: 8.7236  loss_pitm: 0.0126  loss_mlm: 0.2988  loss_prd: 0.0639  loss_mrtd: 0.2022  time: 1.3183  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1150/2558]  eta: 0:31:02  lr: 0.000001  loss_cl: 7.9924  loss_pitm: 0.0047  loss_mlm: 0.1990  loss_prd: 0.0445  loss_mrtd: 0.2541  time: 1.3020  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1200/2558]  eta: 0:29:54  lr: 0.000001  loss_cl: 8.7756  loss_pitm: 0.0100  loss_mlm: 0.3599  loss_prd: 0.4899  loss_mrtd: 0.2662  time: 1.2917  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1250/2558]  eta: 0:28:47  lr: 0.000001  loss_cl: 8.0022  loss_pitm: 0.0143  loss_mlm: 0.4500  loss_prd: 0.2786  loss_mrtd: 0.1860  time: 1.2973  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1300/2558]  eta: 0:27:40  lr: 0.000001  loss_cl: 8.2403  loss_pitm: 0.0144  loss_mlm: 0.4110  loss_prd: 0.0897  loss_mrtd: 0.1947  time: 1.2969  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1350/2558]  eta: 0:26:33  lr: 0.000001  loss_cl: 8.4910  loss_pitm: 0.0043  loss_mlm: 0.6842  loss_prd: 0.0465  loss_mrtd: 0.2009  time: 1.3114  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1400/2558]  eta: 0:25:26  lr: 0.000001  loss_cl: 8.4498  loss_pitm: 0.0067  loss_mlm: 0.5325  loss_prd: 0.3021  loss_mrtd: 0.2254  time: 1.3042  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1450/2558]  eta: 0:24:20  lr: 0.000001  loss_cl: 8.3062  loss_pitm: 0.0088  loss_mlm: 0.2290  loss_prd: 0.0560  loss_mrtd: 0.2550  time: 1.2924  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1500/2558]  eta: 0:23:13  lr: 0.000001  loss_cl: 9.0798  loss_pitm: 0.0017  loss_mlm: 0.3215  loss_prd: 0.0417  loss_mrtd: 0.2008  time: 1.2960  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1550/2558]  eta: 0:22:07  lr: 0.000001  loss_cl: 8.2864  loss_pitm: 0.0132  loss_mlm: 0.3173  loss_prd: 0.3085  loss_mrtd: 0.1789  time: 1.3061  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1600/2558]  eta: 0:21:00  lr: 0.000001  loss_cl: 8.6061  loss_pitm: 0.0382  loss_mlm: 0.3821  loss_prd: 0.5360  loss_mrtd: 0.2703  time: 1.2862  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1650/2558]  eta: 0:19:54  lr: 0.000001  loss_cl: 8.3402  loss_pitm: 0.0037  loss_mlm: 0.4994  loss_prd: 0.5321  loss_mrtd: 0.1943  time: 1.3015  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1700/2558]  eta: 0:18:48  lr: 0.000001  loss_cl: 7.7383  loss_pitm: 0.0886  loss_mlm: 0.3393  loss_prd: 0.5453  loss_mrtd: 0.1909  time: 1.2950  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1750/2558]  eta: 0:17:42  lr: 0.000001  loss_cl: 7.9197  loss_pitm: 0.0094  loss_mlm: 0.2161  loss_prd: 0.5191  loss_mrtd: 0.2208  time: 1.3126  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1800/2558]  eta: 0:16:36  lr: 0.000001  loss_cl: 8.2870  loss_pitm: 0.0025  loss_mlm: 0.3239  loss_prd: 0.2905  loss_mrtd: 0.1740  time: 1.3014  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1850/2558]  eta: 0:15:30  lr: 0.000001  loss_cl: 8.3135  loss_pitm: 0.0131  loss_mlm: 0.4927  loss_prd: 0.0599  loss_mrtd: 0.2668  time: 1.2945  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1900/2558]  eta: 0:14:24  lr: 0.000001  loss_cl: 7.5399  loss_pitm: 0.0498  loss_mlm: 0.5885  loss_prd: 0.1028  loss_mrtd: 0.2567  time: 1.2999  data: 0.0001  max mem: 18589
Train Epoch: [29]  [1950/2558]  eta: 0:13:18  lr: 0.000001  loss_cl: 7.4255  loss_pitm: 0.0064  loss_mlm: 0.5823  loss_prd: 0.2501  loss_mrtd: 0.2400  time: 1.3017  data: 0.0001  max mem: 18589
Train Epoch: [29]  [2000/2558]  eta: 0:12:12  lr: 0.000001  loss_cl: 8.2560  loss_pitm: 0.0558  loss_mlm: 0.1923  loss_prd: 0.4377  loss_mrtd: 0.2144  time: 1.3109  data: 0.0001  max mem: 18589
Train Epoch: [29]  [2050/2558]  eta: 0:11:06  lr: 0.000001  loss_cl: 7.7021  loss_pitm: 0.1497  loss_mlm: 0.0989  loss_prd: 0.0484  loss_mrtd: 0.1439  time: 1.3003  data: 0.0001  max mem: 18589
Train Epoch: [29]  [2100/2558]  eta: 0:10:01  lr: 0.000001  loss_cl: 7.6361  loss_pitm: 0.0114  loss_mlm: 0.3958  loss_prd: 0.0747  loss_mrtd: 0.1969  time: 1.2884  data: 0.0001  max mem: 18589
Train Epoch: [29]  [2150/2558]  eta: 0:08:55  lr: 0.000001  loss_cl: 7.3822  loss_pitm: 0.0059  loss_mlm: 0.2236  loss_prd: 0.0463  loss_mrtd: 0.2075  time: 1.2875  data: 0.0001  max mem: 18589
Train Epoch: [29]  [2200/2558]  eta: 0:07:49  lr: 0.000001  loss_cl: 7.6831  loss_pitm: 0.0032  loss_mlm: 0.2736  loss_prd: 0.0471  loss_mrtd: 0.1829  time: 1.3079  data: 0.0001  max mem: 18589
Train Epoch: [29]  [2250/2558]  eta: 0:06:43  lr: 0.000001  loss_cl: 7.6613  loss_pitm: 0.0025  loss_mlm: 0.4964  loss_prd: 0.0438  loss_mrtd: 0.2204  time: 1.3036  data: 0.0001  max mem: 18589
Train Epoch: [29]  [2300/2558]  eta: 0:05:38  lr: 0.000001  loss_cl: 7.4587  loss_pitm: 0.0077  loss_mlm: 0.3672  loss_prd: 0.0511  loss_mrtd: 0.2072  time: 1.3036  data: 0.0001  max mem: 18589
Train Epoch: [29]  [2350/2558]  eta: 0:04:32  lr: 0.000001  loss_cl: 7.4581  loss_pitm: 0.0257  loss_mlm: 0.1892  loss_prd: 0.0825  loss_mrtd: 0.2493  time: 1.2928  data: 0.0001  max mem: 18589
Train Epoch: [29]  [2400/2558]  eta: 0:03:27  lr: 0.000001  loss_cl: 7.4268  loss_pitm: 0.2679  loss_mlm: 0.4503  loss_prd: 0.2530  loss_mrtd: 0.1208  time: 1.3080  data: 0.0001  max mem: 18589
Train Epoch: [29]  [2450/2558]  eta: 0:02:21  lr: 0.000001  loss_cl: 7.2046  loss_pitm: 0.0555  loss_mlm: 0.8096  loss_prd: 0.0554  loss_mrtd: 0.2014  time: 1.3005  data: 0.0001  max mem: 18589
Train Epoch: [29]  [2500/2558]  eta: 0:01:16  lr: 0.000001  loss_cl: 7.2495  loss_pitm: 0.0064  loss_mlm: 0.5474  loss_prd: 0.0482  loss_mrtd: 0.2285  time: 1.3000  data: 0.0001  max mem: 18589
Train Epoch: [29]  [2550/2558]  eta: 0:00:10  lr: 0.000001  loss_cl: 7.2284  loss_pitm: 0.0120  loss_mlm: 0.2478  loss_prd: 0.2269  loss_mrtd: 0.1721  time: 1.2979  data: 0.0002  max mem: 18589
Train Epoch: [29]  [2557/2558]  eta: 0:00:01  lr: 0.000001  loss_cl: 7.6730  loss_pitm: 0.0645  loss_mlm: 0.6369  loss_prd: 0.2829  loss_mrtd: 0.1670  time: 1.2939  data: 0.0002  max mem: 18589
Train Epoch: [29] Total time: 0:55:51 (1.3103 s / it)
Averaged stats: lr: 0.0000  loss_cl: 8.5183  loss_pitm: 0.0428  loss_mlm: 0.4091  loss_prd: 0.1843  loss_mrtd: 0.2132
Computing features for evaluation...
Evaluation:  [   0/3079]  eta: 0:18:55    time: 0.3688  data: 0.0063  max mem: 18589
Evaluation:  [  50/3079]  eta: 0:18:23    time: 0.3636  data: 0.0000  max mem: 18589
Evaluation:  [ 100/3079]  eta: 0:18:03    time: 0.3632  data: 0.0000  max mem: 18589
Evaluation:  [ 150/3079]  eta: 0:17:45    time: 0.3666  data: 0.0000  max mem: 18589
Evaluation:  [ 200/3079]  eta: 0:17:32    time: 0.3711  data: 0.0000  max mem: 18589
Evaluation:  [ 250/3079]  eta: 0:17:17    time: 0.3701  data: 0.0000  max mem: 18589
Evaluation:  [ 300/3079]  eta: 0:16:58    time: 0.3665  data: 0.0000  max mem: 18589
Evaluation:  [ 350/3079]  eta: 0:16:41    time: 0.3708  data: 0.0000  max mem: 18589
Evaluation:  [ 400/3079]  eta: 0:16:24    time: 0.3730  data: 0.0000  max mem: 18589
Evaluation:  [ 450/3079]  eta: 0:16:06    time: 0.3663  data: 0.0000  max mem: 18589
Evaluation:  [ 500/3079]  eta: 0:15:47    time: 0.3695  data: 0.0000  max mem: 18589
Evaluation:  [ 550/3079]  eta: 0:15:30    time: 0.3730  data: 0.0000  max mem: 18589
Evaluation:  [ 600/3079]  eta: 0:15:11    time: 0.3648  data: 0.0000  max mem: 18589
Evaluation:  [ 650/3079]  eta: 0:14:53    time: 0.3719  data: 0.0000  max mem: 18589
Evaluation:  [ 700/3079]  eta: 0:14:35    time: 0.3706  data: 0.0000  max mem: 18589
Evaluation:  [ 750/3079]  eta: 0:14:17    time: 0.3652  data: 0.0000  max mem: 18589
Evaluation:  [ 800/3079]  eta: 0:13:58    time: 0.3694  data: 0.0000  max mem: 18589
Evaluation:  [ 850/3079]  eta: 0:13:41    time: 0.3751  data: 0.0000  max mem: 18589
Evaluation:  [ 900/3079]  eta: 0:13:22    time: 0.3666  data: 0.0000  max mem: 18589
Evaluation:  [ 950/3079]  eta: 0:13:03    time: 0.3697  data: 0.0000  max mem: 18589
Evaluation:  [1000/3079]  eta: 0:12:45    time: 0.3678  data: 0.0000  max mem: 18589
Evaluation:  [1050/3079]  eta: 0:12:27    time: 0.3649  data: 0.0000  max mem: 18589
Evaluation:  [1100/3079]  eta: 0:12:08    time: 0.3688  data: 0.0000  max mem: 18589
Evaluation:  [1150/3079]  eta: 0:11:50    time: 0.3715  data: 0.0000  max mem: 18589
Evaluation:  [1200/3079]  eta: 0:11:32    time: 0.3667  data: 0.0000  max mem: 18589
Evaluation:  [1250/3079]  eta: 0:11:13    time: 0.3654  data: 0.0000  max mem: 18589
Evaluation:  [1300/3079]  eta: 0:10:55    time: 0.3701  data: 0.0000  max mem: 18589
Evaluation:  [1350/3079]  eta: 0:10:37    time: 0.3680  data: 0.0000  max mem: 18589
Evaluation:  [1400/3079]  eta: 0:10:18    time: 0.3678  data: 0.0000  max mem: 18589
Evaluation:  [1450/3079]  eta: 0:10:00    time: 0.3723  data: 0.0000  max mem: 18589
Evaluation:  [1500/3079]  eta: 0:09:42    time: 0.3711  data: 0.0000  max mem: 18589
Evaluation:  [1550/3079]  eta: 0:09:23    time: 0.3641  data: 0.0000  max mem: 18589
Evaluation:  [1600/3079]  eta: 0:09:05    time: 0.3697  data: 0.0000  max mem: 18589
Evaluation:  [1650/3079]  eta: 0:08:46    time: 0.3718  data: 0.0000  max mem: 18589
Evaluation:  [1700/3079]  eta: 0:08:28    time: 0.3665  data: 0.0000  max mem: 18589
Evaluation:  [1750/3079]  eta: 0:08:09    time: 0.3693  data: 0.0000  max mem: 18589
Evaluation:  [1800/3079]  eta: 0:07:51    time: 0.3743  data: 0.0000  max mem: 18589
Evaluation:  [1850/3079]  eta: 0:07:32    time: 0.3672  data: 0.0000  max mem: 18589
Evaluation:  [1900/3079]  eta: 0:07:14    time: 0.3693  data: 0.0000  max mem: 18589
Evaluation:  [1950/3079]  eta: 0:06:56    time: 0.3699  data: 0.0000  max mem: 18589
Evaluation:  [2000/3079]  eta: 0:06:37    time: 0.3669  data: 0.0000  max mem: 18589
Evaluation:  [2050/3079]  eta: 0:06:19    time: 0.3702  data: 0.0000  max mem: 18589
Evaluation:  [2100/3079]  eta: 0:06:00    time: 0.3722  data: 0.0000  max mem: 18589
Evaluation:  [2150/3079]  eta: 0:05:42    time: 0.3677  data: 0.0000  max mem: 18589
Evaluation:  [2200/3079]  eta: 0:05:24    time: 0.3687  data: 0.0000  max mem: 18589
Evaluation:  [2250/3079]  eta: 0:05:05    time: 0.3727  data: 0.0000  max mem: 18589
Evaluation:  [2300/3079]  eta: 0:04:47    time: 0.3660  data: 0.0000  max mem: 18589
Evaluation:  [2350/3079]  eta: 0:04:28    time: 0.3684  data: 0.0000  max mem: 18589
Evaluation:  [2400/3079]  eta: 0:04:10    time: 0.3711  data: 0.0000  max mem: 18589
Evaluation:  [2450/3079]  eta: 0:03:51    time: 0.3676  data: 0.0000  max mem: 18589
Evaluation:  [2500/3079]  eta: 0:03:33    time: 0.3676  data: 0.0000  max mem: 18589
Evaluation:  [2550/3079]  eta: 0:03:15    time: 0.3717  data: 0.0000  max mem: 18589
Evaluation:  [2600/3079]  eta: 0:02:56    time: 0.3711  data: 0.0000  max mem: 18589
Evaluation:  [2650/3079]  eta: 0:02:38    time: 0.3670  data: 0.0000  max mem: 18589
Evaluation:  [2700/3079]  eta: 0:02:19    time: 0.3718  data: 0.0000  max mem: 18589
Evaluation:  [2750/3079]  eta: 0:02:01    time: 0.3681  data: 0.0000  max mem: 18589
Evaluation:  [2800/3079]  eta: 0:01:42    time: 0.3685  data: 0.0000  max mem: 18589
Evaluation:  [2850/3079]  eta: 0:01:24    time: 0.3689  data: 0.0000  max mem: 18589
Evaluation:  [2900/3079]  eta: 0:01:06    time: 0.3703  data: 0.0000  max mem: 18589
Evaluation:  [2950/3079]  eta: 0:00:47    time: 0.3670  data: 0.0000  max mem: 18589
Evaluation:  [3000/3079]  eta: 0:00:29    time: 0.3736  data: 0.0000  max mem: 18589
Evaluation:  [3050/3079]  eta: 0:00:10    time: 0.3684  data: 0.0000  max mem: 18589
Evaluation:  [3078/3079]  eta: 0:00:00    time: 0.3668  data: 0.0000  max mem: 18589
Evaluation: Total time: 0:18:55 (0.3689 s / it)
Evaluation time 0:20:45
Test: {'r1': 75.47108459472656, 'r5': 89.376220703125, 'r10': 93.12865447998047, 'r_mean': 85.99198659261067, 'mAP': 67.794921875} 

Training time 1 day, 14:36:24
